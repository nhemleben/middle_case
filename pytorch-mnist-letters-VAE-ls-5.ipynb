{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "batch_size = 10**3\n",
    "Z_dim = 4\n",
    "# Extended MNIST Dataset\n",
    "train_dataset = datasets.EMNIST(root='./emnist_data/', split= 'byclass', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.EMNIST(root='./emnist_data/', split= 'byclass', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "# build model\n",
    "vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=Z_dim)\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()\n",
    "    model = vae.to('cuda')\n",
    "    print('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc31): Linear(in_features=256, out_features=4, bias=True)\n",
       "  (fc32): Linear(in_features=256, out_features=4, bias=True)\n",
       "  (fc4): Linear(in_features=4, out_features=256, bias=True)\n",
       "  (fc5): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (fc6): Linear(in_features=512, out_features=784, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    vae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.cuda()\n",
    "            recon, mu, log_var = vae(data)\n",
    "            \n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/697932 (0%)]\tLoss: 175.709219\n",
      "Train Epoch: 0 [100000/697932 (14%)]\tLoss: 173.346578\n",
      "Train Epoch: 0 [200000/697932 (29%)]\tLoss: 171.978766\n",
      "Train Epoch: 0 [300000/697932 (43%)]\tLoss: 174.228453\n",
      "Train Epoch: 0 [400000/697932 (57%)]\tLoss: 176.259250\n",
      "Train Epoch: 0 [500000/697932 (72%)]\tLoss: 173.619656\n",
      "Train Epoch: 0 [600000/697932 (86%)]\tLoss: 171.914203\n",
      "====> Epoch: 0 Average loss: 173.2145\n",
      "====> Test set loss: 173.4767\n",
      "Train Epoch: 1 [0/697932 (0%)]\tLoss: 170.557437\n",
      "Train Epoch: 1 [100000/697932 (14%)]\tLoss: 172.521047\n",
      "Train Epoch: 1 [200000/697932 (29%)]\tLoss: 171.979422\n",
      "Train Epoch: 1 [300000/697932 (43%)]\tLoss: 173.653656\n",
      "Train Epoch: 1 [400000/697932 (57%)]\tLoss: 172.159125\n",
      "Train Epoch: 1 [500000/697932 (72%)]\tLoss: 174.619766\n",
      "Train Epoch: 1 [600000/697932 (86%)]\tLoss: 168.874609\n",
      "====> Epoch: 1 Average loss: 172.6799\n",
      "====> Test set loss: 172.9760\n",
      "Train Epoch: 2 [0/697932 (0%)]\tLoss: 171.379297\n",
      "Train Epoch: 2 [100000/697932 (14%)]\tLoss: 173.215797\n",
      "Train Epoch: 2 [200000/697932 (29%)]\tLoss: 170.040750\n",
      "Train Epoch: 2 [300000/697932 (43%)]\tLoss: 170.255266\n",
      "Train Epoch: 2 [400000/697932 (57%)]\tLoss: 172.408813\n",
      "Train Epoch: 2 [500000/697932 (72%)]\tLoss: 167.750203\n",
      "Train Epoch: 2 [600000/697932 (86%)]\tLoss: 173.040641\n",
      "====> Epoch: 2 Average loss: 172.2521\n",
      "====> Test set loss: 172.5803\n",
      "Train Epoch: 3 [0/697932 (0%)]\tLoss: 170.460469\n",
      "Train Epoch: 3 [100000/697932 (14%)]\tLoss: 169.343516\n",
      "Train Epoch: 3 [200000/697932 (29%)]\tLoss: 170.140250\n",
      "Train Epoch: 3 [300000/697932 (43%)]\tLoss: 172.070562\n",
      "Train Epoch: 3 [400000/697932 (57%)]\tLoss: 172.508875\n",
      "Train Epoch: 3 [500000/697932 (72%)]\tLoss: 172.133547\n",
      "Train Epoch: 3 [600000/697932 (86%)]\tLoss: 172.743594\n",
      "====> Epoch: 3 Average loss: 171.8412\n",
      "====> Test set loss: 172.2971\n",
      "Train Epoch: 4 [0/697932 (0%)]\tLoss: 171.413891\n",
      "Train Epoch: 4 [100000/697932 (14%)]\tLoss: 171.331266\n",
      "Train Epoch: 4 [200000/697932 (29%)]\tLoss: 169.272000\n",
      "Train Epoch: 4 [300000/697932 (43%)]\tLoss: 170.017828\n",
      "Train Epoch: 4 [400000/697932 (57%)]\tLoss: 175.541781\n",
      "Train Epoch: 4 [500000/697932 (72%)]\tLoss: 173.082531\n",
      "Train Epoch: 4 [600000/697932 (86%)]\tLoss: 171.329641\n",
      "====> Epoch: 4 Average loss: 171.4983\n",
      "====> Test set loss: 171.8627\n",
      "Train Epoch: 5 [0/697932 (0%)]\tLoss: 167.830766\n",
      "Train Epoch: 5 [100000/697932 (14%)]\tLoss: 170.354703\n",
      "Train Epoch: 5 [200000/697932 (29%)]\tLoss: 169.814328\n",
      "Train Epoch: 5 [300000/697932 (43%)]\tLoss: 170.961047\n",
      "Train Epoch: 5 [400000/697932 (57%)]\tLoss: 172.724031\n",
      "Train Epoch: 5 [500000/697932 (72%)]\tLoss: 171.338516\n",
      "Train Epoch: 5 [600000/697932 (86%)]\tLoss: 172.756922\n",
      "====> Epoch: 5 Average loss: 171.1469\n",
      "====> Test set loss: 171.6353\n",
      "Train Epoch: 6 [0/697932 (0%)]\tLoss: 173.145109\n",
      "Train Epoch: 6 [100000/697932 (14%)]\tLoss: 173.356562\n",
      "Train Epoch: 6 [200000/697932 (29%)]\tLoss: 172.509969\n",
      "Train Epoch: 6 [300000/697932 (43%)]\tLoss: 175.374953\n",
      "Train Epoch: 6 [400000/697932 (57%)]\tLoss: 168.599844\n",
      "Train Epoch: 6 [500000/697932 (72%)]\tLoss: 171.205203\n",
      "Train Epoch: 6 [600000/697932 (86%)]\tLoss: 168.463969\n",
      "====> Epoch: 6 Average loss: 170.8303\n",
      "====> Test set loss: 171.3003\n",
      "Train Epoch: 7 [0/697932 (0%)]\tLoss: 171.201641\n",
      "Train Epoch: 7 [100000/697932 (14%)]\tLoss: 172.243172\n",
      "Train Epoch: 7 [200000/697932 (29%)]\tLoss: 173.317578\n",
      "Train Epoch: 7 [300000/697932 (43%)]\tLoss: 167.430766\n",
      "Train Epoch: 7 [400000/697932 (57%)]\tLoss: 166.024141\n",
      "Train Epoch: 7 [500000/697932 (72%)]\tLoss: 169.914109\n",
      "Train Epoch: 7 [600000/697932 (86%)]\tLoss: 169.596719\n",
      "====> Epoch: 7 Average loss: 170.5470\n",
      "====> Test set loss: 171.1046\n",
      "Train Epoch: 8 [0/697932 (0%)]\tLoss: 169.384437\n",
      "Train Epoch: 8 [100000/697932 (14%)]\tLoss: 169.367719\n",
      "Train Epoch: 8 [200000/697932 (29%)]\tLoss: 168.646250\n",
      "Train Epoch: 8 [300000/697932 (43%)]\tLoss: 168.348797\n",
      "Train Epoch: 8 [400000/697932 (57%)]\tLoss: 170.026109\n",
      "Train Epoch: 8 [500000/697932 (72%)]\tLoss: 169.592891\n",
      "Train Epoch: 8 [600000/697932 (86%)]\tLoss: 169.143984\n",
      "====> Epoch: 8 Average loss: 170.2619\n",
      "====> Test set loss: 170.7624\n",
      "Train Epoch: 9 [0/697932 (0%)]\tLoss: 170.503281\n",
      "Train Epoch: 9 [100000/697932 (14%)]\tLoss: 172.756109\n",
      "Train Epoch: 9 [200000/697932 (29%)]\tLoss: 167.922406\n",
      "Train Epoch: 9 [300000/697932 (43%)]\tLoss: 170.518313\n",
      "Train Epoch: 9 [400000/697932 (57%)]\tLoss: 168.136625\n",
      "Train Epoch: 9 [500000/697932 (72%)]\tLoss: 171.271453\n",
      "Train Epoch: 9 [600000/697932 (86%)]\tLoss: 172.725125\n",
      "====> Epoch: 9 Average loss: 170.0310\n",
      "====> Test set loss: 170.6826\n",
      "Train Epoch: 10 [0/697932 (0%)]\tLoss: 168.255063\n",
      "Train Epoch: 10 [100000/697932 (14%)]\tLoss: 168.312719\n",
      "Train Epoch: 10 [200000/697932 (29%)]\tLoss: 170.546391\n",
      "Train Epoch: 10 [300000/697932 (43%)]\tLoss: 167.767812\n",
      "Train Epoch: 10 [400000/697932 (57%)]\tLoss: 167.404234\n",
      "Train Epoch: 10 [500000/697932 (72%)]\tLoss: 172.365797\n",
      "Train Epoch: 10 [600000/697932 (86%)]\tLoss: 169.728250\n",
      "====> Epoch: 10 Average loss: 169.8099\n",
      "====> Test set loss: 170.4900\n",
      "Train Epoch: 11 [0/697932 (0%)]\tLoss: 170.572812\n",
      "Train Epoch: 11 [100000/697932 (14%)]\tLoss: 168.707250\n",
      "Train Epoch: 11 [200000/697932 (29%)]\tLoss: 170.667484\n",
      "Train Epoch: 11 [300000/697932 (43%)]\tLoss: 169.659344\n",
      "Train Epoch: 11 [400000/697932 (57%)]\tLoss: 169.328609\n",
      "Train Epoch: 11 [500000/697932 (72%)]\tLoss: 169.918391\n",
      "Train Epoch: 11 [600000/697932 (86%)]\tLoss: 169.318219\n",
      "====> Epoch: 11 Average loss: 169.6029\n",
      "====> Test set loss: 170.2912\n",
      "Train Epoch: 12 [0/697932 (0%)]\tLoss: 172.194094\n",
      "Train Epoch: 12 [100000/697932 (14%)]\tLoss: 168.618828\n",
      "Train Epoch: 12 [200000/697932 (29%)]\tLoss: 168.582047\n",
      "Train Epoch: 12 [300000/697932 (43%)]\tLoss: 168.413484\n",
      "Train Epoch: 12 [400000/697932 (57%)]\tLoss: 168.224266\n",
      "Train Epoch: 12 [500000/697932 (72%)]\tLoss: 170.625359\n",
      "Train Epoch: 12 [600000/697932 (86%)]\tLoss: 172.121969\n",
      "====> Epoch: 12 Average loss: 169.4244\n",
      "====> Test set loss: 170.1220\n",
      "Train Epoch: 13 [0/697932 (0%)]\tLoss: 166.968000\n",
      "Train Epoch: 13 [100000/697932 (14%)]\tLoss: 169.361750\n",
      "Train Epoch: 13 [200000/697932 (29%)]\tLoss: 169.328656\n",
      "Train Epoch: 13 [300000/697932 (43%)]\tLoss: 170.570078\n",
      "Train Epoch: 13 [400000/697932 (57%)]\tLoss: 169.555375\n",
      "Train Epoch: 13 [500000/697932 (72%)]\tLoss: 167.401641\n",
      "Train Epoch: 13 [600000/697932 (86%)]\tLoss: 167.372859\n",
      "====> Epoch: 13 Average loss: 169.1860\n",
      "====> Test set loss: 169.9774\n",
      "Train Epoch: 14 [0/697932 (0%)]\tLoss: 169.807281\n",
      "Train Epoch: 14 [100000/697932 (14%)]\tLoss: 167.572266\n",
      "Train Epoch: 14 [200000/697932 (29%)]\tLoss: 169.699203\n",
      "Train Epoch: 14 [300000/697932 (43%)]\tLoss: 169.294375\n",
      "Train Epoch: 14 [400000/697932 (57%)]\tLoss: 167.995187\n",
      "Train Epoch: 14 [500000/697932 (72%)]\tLoss: 166.576516\n",
      "Train Epoch: 14 [600000/697932 (86%)]\tLoss: 170.993266\n",
      "====> Epoch: 14 Average loss: 169.0124\n",
      "====> Test set loss: 169.7635\n",
      "Train Epoch: 15 [0/697932 (0%)]\tLoss: 167.438813\n",
      "Train Epoch: 15 [100000/697932 (14%)]\tLoss: 169.817969\n",
      "Train Epoch: 15 [200000/697932 (29%)]\tLoss: 166.947734\n",
      "Train Epoch: 15 [300000/697932 (43%)]\tLoss: 166.478219\n",
      "Train Epoch: 15 [400000/697932 (57%)]\tLoss: 170.972406\n",
      "Train Epoch: 15 [500000/697932 (72%)]\tLoss: 168.895250\n",
      "Train Epoch: 15 [600000/697932 (86%)]\tLoss: 166.251812\n",
      "====> Epoch: 15 Average loss: 168.8442\n",
      "====> Test set loss: 169.7747\n",
      "Train Epoch: 16 [0/697932 (0%)]\tLoss: 169.447906\n",
      "Train Epoch: 16 [100000/697932 (14%)]\tLoss: 171.316922\n",
      "Train Epoch: 16 [200000/697932 (29%)]\tLoss: 171.453203\n",
      "Train Epoch: 16 [300000/697932 (43%)]\tLoss: 170.457859\n",
      "Train Epoch: 16 [400000/697932 (57%)]\tLoss: 168.241437\n",
      "Train Epoch: 16 [500000/697932 (72%)]\tLoss: 171.284828\n",
      "Train Epoch: 16 [600000/697932 (86%)]\tLoss: 167.249156\n",
      "====> Epoch: 16 Average loss: 168.7062\n",
      "====> Test set loss: 169.6553\n",
      "Train Epoch: 17 [0/697932 (0%)]\tLoss: 167.570687\n",
      "Train Epoch: 17 [100000/697932 (14%)]\tLoss: 168.166703\n",
      "Train Epoch: 17 [200000/697932 (29%)]\tLoss: 167.987859\n",
      "Train Epoch: 17 [300000/697932 (43%)]\tLoss: 170.154875\n",
      "Train Epoch: 17 [400000/697932 (57%)]\tLoss: 170.671156\n",
      "Train Epoch: 17 [500000/697932 (72%)]\tLoss: 168.939906\n",
      "Train Epoch: 17 [600000/697932 (86%)]\tLoss: 164.742344\n",
      "====> Epoch: 17 Average loss: 168.5794\n",
      "====> Test set loss: 169.5206\n",
      "Train Epoch: 18 [0/697932 (0%)]\tLoss: 169.051359\n",
      "Train Epoch: 18 [100000/697932 (14%)]\tLoss: 168.886969\n",
      "Train Epoch: 18 [200000/697932 (29%)]\tLoss: 165.497531\n",
      "Train Epoch: 18 [300000/697932 (43%)]\tLoss: 170.733484\n",
      "Train Epoch: 18 [400000/697932 (57%)]\tLoss: 169.431594\n",
      "Train Epoch: 18 [500000/697932 (72%)]\tLoss: 167.599469\n",
      "Train Epoch: 18 [600000/697932 (86%)]\tLoss: 166.126688\n",
      "====> Epoch: 18 Average loss: 168.4690\n",
      "====> Test set loss: 169.1649\n",
      "Train Epoch: 19 [0/697932 (0%)]\tLoss: 165.735609\n",
      "Train Epoch: 19 [100000/697932 (14%)]\tLoss: 168.190625\n",
      "Train Epoch: 19 [200000/697932 (29%)]\tLoss: 168.938344\n",
      "Train Epoch: 19 [300000/697932 (43%)]\tLoss: 166.522344\n",
      "Train Epoch: 19 [400000/697932 (57%)]\tLoss: 166.713328\n",
      "Train Epoch: 19 [500000/697932 (72%)]\tLoss: 170.604109\n",
      "Train Epoch: 19 [600000/697932 (86%)]\tLoss: 170.996328\n",
      "====> Epoch: 19 Average loss: 168.2900\n",
      "====> Test set loss: 169.1955\n",
      "Train Epoch: 20 [0/697932 (0%)]\tLoss: 165.037516\n",
      "Train Epoch: 20 [100000/697932 (14%)]\tLoss: 173.202141\n",
      "Train Epoch: 20 [200000/697932 (29%)]\tLoss: 171.421031\n",
      "Train Epoch: 20 [300000/697932 (43%)]\tLoss: 165.011875\n",
      "Train Epoch: 20 [400000/697932 (57%)]\tLoss: 167.807266\n",
      "Train Epoch: 20 [500000/697932 (72%)]\tLoss: 165.748562\n",
      "Train Epoch: 20 [600000/697932 (86%)]\tLoss: 167.052156\n",
      "====> Epoch: 20 Average loss: 168.1308\n",
      "====> Test set loss: 169.0487\n",
      "Train Epoch: 21 [0/697932 (0%)]\tLoss: 168.814281\n",
      "Train Epoch: 21 [100000/697932 (14%)]\tLoss: 166.121125\n",
      "Train Epoch: 21 [200000/697932 (29%)]\tLoss: 167.808844\n",
      "Train Epoch: 21 [300000/697932 (43%)]\tLoss: 166.389703\n",
      "Train Epoch: 21 [400000/697932 (57%)]\tLoss: 167.799094\n",
      "Train Epoch: 21 [500000/697932 (72%)]\tLoss: 165.833281\n",
      "Train Epoch: 21 [600000/697932 (86%)]\tLoss: 166.630609\n",
      "====> Epoch: 21 Average loss: 168.0111\n",
      "====> Test set loss: 169.2586\n",
      "Train Epoch: 22 [0/697932 (0%)]\tLoss: 168.300094\n",
      "Train Epoch: 22 [100000/697932 (14%)]\tLoss: 167.966547\n",
      "Train Epoch: 22 [200000/697932 (29%)]\tLoss: 168.830297\n",
      "Train Epoch: 22 [300000/697932 (43%)]\tLoss: 166.679031\n",
      "Train Epoch: 22 [400000/697932 (57%)]\tLoss: 167.719906\n",
      "Train Epoch: 22 [500000/697932 (72%)]\tLoss: 167.532187\n",
      "Train Epoch: 22 [600000/697932 (86%)]\tLoss: 168.939734\n",
      "====> Epoch: 22 Average loss: 167.8853\n",
      "====> Test set loss: 169.1071\n",
      "Train Epoch: 23 [0/697932 (0%)]\tLoss: 167.023156\n",
      "Train Epoch: 23 [100000/697932 (14%)]\tLoss: 166.544750\n",
      "Train Epoch: 23 [200000/697932 (29%)]\tLoss: 167.961781\n",
      "Train Epoch: 23 [300000/697932 (43%)]\tLoss: 168.836813\n",
      "Train Epoch: 23 [400000/697932 (57%)]\tLoss: 170.015391\n",
      "Train Epoch: 23 [500000/697932 (72%)]\tLoss: 169.585813\n",
      "Train Epoch: 23 [600000/697932 (86%)]\tLoss: 167.786422\n",
      "====> Epoch: 23 Average loss: 167.8048\n",
      "====> Test set loss: 168.8804\n",
      "Train Epoch: 24 [0/697932 (0%)]\tLoss: 165.880500\n",
      "Train Epoch: 24 [100000/697932 (14%)]\tLoss: 167.360031\n",
      "Train Epoch: 24 [200000/697932 (29%)]\tLoss: 167.542078\n",
      "Train Epoch: 24 [300000/697932 (43%)]\tLoss: 167.483922\n",
      "Train Epoch: 24 [400000/697932 (57%)]\tLoss: 167.523109\n",
      "Train Epoch: 24 [500000/697932 (72%)]\tLoss: 165.555438\n",
      "Train Epoch: 24 [600000/697932 (86%)]\tLoss: 167.791906\n",
      "====> Epoch: 24 Average loss: 167.6679\n",
      "====> Test set loss: 168.9570\n",
      "Train Epoch: 25 [0/697932 (0%)]\tLoss: 168.707797\n",
      "Train Epoch: 25 [100000/697932 (14%)]\tLoss: 168.330094\n",
      "Train Epoch: 25 [200000/697932 (29%)]\tLoss: 165.946375\n",
      "Train Epoch: 25 [300000/697932 (43%)]\tLoss: 166.635219\n",
      "Train Epoch: 25 [400000/697932 (57%)]\tLoss: 165.563109\n",
      "Train Epoch: 25 [500000/697932 (72%)]\tLoss: 168.892703\n",
      "Train Epoch: 25 [600000/697932 (86%)]\tLoss: 165.446703\n",
      "====> Epoch: 25 Average loss: 167.5177\n",
      "====> Test set loss: 168.5750\n",
      "Train Epoch: 26 [0/697932 (0%)]\tLoss: 169.237203\n",
      "Train Epoch: 26 [100000/697932 (14%)]\tLoss: 166.137125\n",
      "Train Epoch: 26 [200000/697932 (29%)]\tLoss: 166.825844\n",
      "Train Epoch: 26 [300000/697932 (43%)]\tLoss: 165.247437\n",
      "Train Epoch: 26 [400000/697932 (57%)]\tLoss: 167.675516\n",
      "Train Epoch: 26 [500000/697932 (72%)]\tLoss: 166.679000\n",
      "Train Epoch: 26 [600000/697932 (86%)]\tLoss: 167.898937\n",
      "====> Epoch: 26 Average loss: 167.4496\n",
      "====> Test set loss: 168.7484\n",
      "Train Epoch: 27 [0/697932 (0%)]\tLoss: 166.563469\n",
      "Train Epoch: 27 [100000/697932 (14%)]\tLoss: 166.823797\n",
      "Train Epoch: 27 [200000/697932 (29%)]\tLoss: 169.136656\n",
      "Train Epoch: 27 [300000/697932 (43%)]\tLoss: 167.981281\n",
      "Train Epoch: 27 [400000/697932 (57%)]\tLoss: 168.304031\n",
      "Train Epoch: 27 [500000/697932 (72%)]\tLoss: 168.993422\n",
      "Train Epoch: 27 [600000/697932 (86%)]\tLoss: 163.073641\n",
      "====> Epoch: 27 Average loss: 167.3632\n",
      "====> Test set loss: 168.4066\n",
      "Train Epoch: 28 [0/697932 (0%)]\tLoss: 167.166453\n",
      "Train Epoch: 28 [100000/697932 (14%)]\tLoss: 167.063297\n",
      "Train Epoch: 28 [200000/697932 (29%)]\tLoss: 167.569344\n",
      "Train Epoch: 28 [300000/697932 (43%)]\tLoss: 169.648969\n",
      "Train Epoch: 28 [400000/697932 (57%)]\tLoss: 168.339656\n",
      "Train Epoch: 28 [500000/697932 (72%)]\tLoss: 170.435859\n",
      "Train Epoch: 28 [600000/697932 (86%)]\tLoss: 168.849219\n",
      "====> Epoch: 28 Average loss: 167.2765\n",
      "====> Test set loss: 168.4070\n",
      "Train Epoch: 29 [0/697932 (0%)]\tLoss: 169.431781\n",
      "Train Epoch: 29 [100000/697932 (14%)]\tLoss: 167.617437\n",
      "Train Epoch: 29 [200000/697932 (29%)]\tLoss: 168.990641\n",
      "Train Epoch: 29 [300000/697932 (43%)]\tLoss: 167.236578\n",
      "Train Epoch: 29 [400000/697932 (57%)]\tLoss: 168.072937\n",
      "Train Epoch: 29 [500000/697932 (72%)]\tLoss: 164.619031\n",
      "Train Epoch: 29 [600000/697932 (86%)]\tLoss: 170.238969\n",
      "====> Epoch: 29 Average loss: 167.2275\n",
      "====> Test set loss: 168.3671\n",
      "Train Epoch: 30 [0/697932 (0%)]\tLoss: 166.472469\n",
      "Train Epoch: 30 [100000/697932 (14%)]\tLoss: 168.926031\n",
      "Train Epoch: 30 [200000/697932 (29%)]\tLoss: 165.352062\n",
      "Train Epoch: 30 [300000/697932 (43%)]\tLoss: 165.960484\n",
      "Train Epoch: 30 [400000/697932 (57%)]\tLoss: 166.854656\n",
      "Train Epoch: 30 [500000/697932 (72%)]\tLoss: 166.926844\n",
      "Train Epoch: 30 [600000/697932 (86%)]\tLoss: 169.688750\n",
      "====> Epoch: 30 Average loss: 167.0930\n",
      "====> Test set loss: 168.3351\n",
      "Train Epoch: 31 [0/697932 (0%)]\tLoss: 167.962531\n",
      "Train Epoch: 31 [100000/697932 (14%)]\tLoss: 167.250891\n",
      "Train Epoch: 31 [200000/697932 (29%)]\tLoss: 166.895453\n",
      "Train Epoch: 31 [300000/697932 (43%)]\tLoss: 169.523937\n",
      "Train Epoch: 31 [400000/697932 (57%)]\tLoss: 167.600422\n",
      "Train Epoch: 31 [500000/697932 (72%)]\tLoss: 165.707969\n",
      "Train Epoch: 31 [600000/697932 (86%)]\tLoss: 169.178094\n",
      "====> Epoch: 31 Average loss: 166.9884\n",
      "====> Test set loss: 168.1982\n",
      "Train Epoch: 32 [0/697932 (0%)]\tLoss: 167.970047\n",
      "Train Epoch: 32 [100000/697932 (14%)]\tLoss: 164.056984\n",
      "Train Epoch: 32 [200000/697932 (29%)]\tLoss: 167.311563\n",
      "Train Epoch: 32 [300000/697932 (43%)]\tLoss: 166.889547\n",
      "Train Epoch: 32 [400000/697932 (57%)]\tLoss: 167.539156\n",
      "Train Epoch: 32 [500000/697932 (72%)]\tLoss: 167.345500\n",
      "Train Epoch: 32 [600000/697932 (86%)]\tLoss: 162.834469\n",
      "====> Epoch: 32 Average loss: 166.9143\n",
      "====> Test set loss: 168.1191\n",
      "Train Epoch: 33 [0/697932 (0%)]\tLoss: 167.045578\n",
      "Train Epoch: 33 [100000/697932 (14%)]\tLoss: 169.315656\n",
      "Train Epoch: 33 [200000/697932 (29%)]\tLoss: 170.140203\n",
      "Train Epoch: 33 [300000/697932 (43%)]\tLoss: 167.365859\n",
      "Train Epoch: 33 [400000/697932 (57%)]\tLoss: 164.746297\n",
      "Train Epoch: 33 [500000/697932 (72%)]\tLoss: 165.842984\n",
      "Train Epoch: 33 [600000/697932 (86%)]\tLoss: 169.798188\n",
      "====> Epoch: 33 Average loss: 166.8570\n",
      "====> Test set loss: 168.2796\n",
      "Train Epoch: 34 [0/697932 (0%)]\tLoss: 165.915281\n",
      "Train Epoch: 34 [100000/697932 (14%)]\tLoss: 167.149937\n",
      "Train Epoch: 34 [200000/697932 (29%)]\tLoss: 163.862125\n",
      "Train Epoch: 34 [300000/697932 (43%)]\tLoss: 169.720984\n",
      "Train Epoch: 34 [400000/697932 (57%)]\tLoss: 168.777359\n",
      "Train Epoch: 34 [500000/697932 (72%)]\tLoss: 167.270438\n",
      "Train Epoch: 34 [600000/697932 (86%)]\tLoss: 166.912016\n",
      "====> Epoch: 34 Average loss: 166.8016\n",
      "====> Test set loss: 168.3283\n",
      "Train Epoch: 35 [0/697932 (0%)]\tLoss: 167.926234\n",
      "Train Epoch: 35 [100000/697932 (14%)]\tLoss: 166.951266\n",
      "Train Epoch: 35 [200000/697932 (29%)]\tLoss: 166.101609\n",
      "Train Epoch: 35 [300000/697932 (43%)]\tLoss: 167.959547\n",
      "Train Epoch: 35 [400000/697932 (57%)]\tLoss: 169.452547\n",
      "Train Epoch: 35 [500000/697932 (72%)]\tLoss: 163.522812\n",
      "Train Epoch: 35 [600000/697932 (86%)]\tLoss: 165.438406\n",
      "====> Epoch: 35 Average loss: 166.8091\n",
      "====> Test set loss: 167.8953\n",
      "Train Epoch: 36 [0/697932 (0%)]\tLoss: 163.368297\n",
      "Train Epoch: 36 [100000/697932 (14%)]\tLoss: 163.142531\n",
      "Train Epoch: 36 [200000/697932 (29%)]\tLoss: 166.095422\n",
      "Train Epoch: 36 [300000/697932 (43%)]\tLoss: 165.299281\n",
      "Train Epoch: 36 [400000/697932 (57%)]\tLoss: 166.161188\n",
      "Train Epoch: 36 [500000/697932 (72%)]\tLoss: 167.429141\n",
      "Train Epoch: 36 [600000/697932 (86%)]\tLoss: 164.893906\n",
      "====> Epoch: 36 Average loss: 166.6159\n",
      "====> Test set loss: 167.8571\n",
      "Train Epoch: 37 [0/697932 (0%)]\tLoss: 166.013625\n",
      "Train Epoch: 37 [100000/697932 (14%)]\tLoss: 168.474984\n",
      "Train Epoch: 37 [200000/697932 (29%)]\tLoss: 165.870812\n",
      "Train Epoch: 37 [300000/697932 (43%)]\tLoss: 166.165031\n",
      "Train Epoch: 37 [400000/697932 (57%)]\tLoss: 166.324406\n",
      "Train Epoch: 37 [500000/697932 (72%)]\tLoss: 163.023781\n",
      "Train Epoch: 37 [600000/697932 (86%)]\tLoss: 168.452281\n",
      "====> Epoch: 37 Average loss: 166.5976\n",
      "====> Test set loss: 167.9844\n",
      "Train Epoch: 38 [0/697932 (0%)]\tLoss: 165.926078\n",
      "Train Epoch: 38 [100000/697932 (14%)]\tLoss: 167.041000\n",
      "Train Epoch: 38 [200000/697932 (29%)]\tLoss: 164.261094\n",
      "Train Epoch: 38 [300000/697932 (43%)]\tLoss: 166.680672\n",
      "Train Epoch: 38 [400000/697932 (57%)]\tLoss: 168.225844\n",
      "Train Epoch: 38 [500000/697932 (72%)]\tLoss: 168.419062\n",
      "Train Epoch: 38 [600000/697932 (86%)]\tLoss: 169.281891\n",
      "====> Epoch: 38 Average loss: 166.5333\n",
      "====> Test set loss: 168.0776\n",
      "Train Epoch: 39 [0/697932 (0%)]\tLoss: 167.417641\n",
      "Train Epoch: 39 [100000/697932 (14%)]\tLoss: 166.534297\n",
      "Train Epoch: 39 [200000/697932 (29%)]\tLoss: 167.041828\n",
      "Train Epoch: 39 [300000/697932 (43%)]\tLoss: 165.482844\n",
      "Train Epoch: 39 [400000/697932 (57%)]\tLoss: 167.131031\n",
      "Train Epoch: 39 [500000/697932 (72%)]\tLoss: 162.806203\n",
      "Train Epoch: 39 [600000/697932 (86%)]\tLoss: 164.868969\n",
      "====> Epoch: 39 Average loss: 166.4052\n",
      "====> Test set loss: 167.8256\n",
      "Train Epoch: 40 [0/697932 (0%)]\tLoss: 166.550047\n",
      "Train Epoch: 40 [100000/697932 (14%)]\tLoss: 163.874625\n",
      "Train Epoch: 40 [200000/697932 (29%)]\tLoss: 164.832000\n",
      "Train Epoch: 40 [300000/697932 (43%)]\tLoss: 164.971812\n",
      "Train Epoch: 40 [400000/697932 (57%)]\tLoss: 165.634844\n",
      "Train Epoch: 40 [500000/697932 (72%)]\tLoss: 168.824516\n",
      "Train Epoch: 40 [600000/697932 (86%)]\tLoss: 166.037906\n",
      "====> Epoch: 40 Average loss: 166.3742\n",
      "====> Test set loss: 167.8419\n",
      "Train Epoch: 41 [0/697932 (0%)]\tLoss: 165.496094\n",
      "Train Epoch: 41 [100000/697932 (14%)]\tLoss: 166.751328\n",
      "Train Epoch: 41 [200000/697932 (29%)]\tLoss: 164.843500\n",
      "Train Epoch: 41 [300000/697932 (43%)]\tLoss: 163.328000\n",
      "Train Epoch: 41 [400000/697932 (57%)]\tLoss: 166.226812\n",
      "Train Epoch: 41 [500000/697932 (72%)]\tLoss: 166.805266\n",
      "Train Epoch: 41 [600000/697932 (86%)]\tLoss: 167.940672\n",
      "====> Epoch: 41 Average loss: 166.2813\n",
      "====> Test set loss: 168.0098\n",
      "Train Epoch: 42 [0/697932 (0%)]\tLoss: 166.121953\n",
      "Train Epoch: 42 [100000/697932 (14%)]\tLoss: 166.889312\n",
      "Train Epoch: 42 [200000/697932 (29%)]\tLoss: 167.636469\n",
      "Train Epoch: 42 [300000/697932 (43%)]\tLoss: 167.175312\n",
      "Train Epoch: 42 [400000/697932 (57%)]\tLoss: 165.891797\n",
      "Train Epoch: 42 [500000/697932 (72%)]\tLoss: 166.351219\n",
      "Train Epoch: 42 [600000/697932 (86%)]\tLoss: 166.814578\n",
      "====> Epoch: 42 Average loss: 166.2415\n",
      "====> Test set loss: 167.6863\n",
      "Train Epoch: 43 [0/697932 (0%)]\tLoss: 167.025563\n",
      "Train Epoch: 43 [100000/697932 (14%)]\tLoss: 165.279609\n",
      "Train Epoch: 43 [200000/697932 (29%)]\tLoss: 165.314344\n",
      "Train Epoch: 43 [300000/697932 (43%)]\tLoss: 166.871672\n",
      "Train Epoch: 43 [400000/697932 (57%)]\tLoss: 164.763688\n",
      "Train Epoch: 43 [500000/697932 (72%)]\tLoss: 165.761422\n",
      "Train Epoch: 43 [600000/697932 (86%)]\tLoss: 166.066547\n",
      "====> Epoch: 43 Average loss: 166.1711\n",
      "====> Test set loss: 167.4775\n",
      "Train Epoch: 44 [0/697932 (0%)]\tLoss: 167.030156\n",
      "Train Epoch: 44 [100000/697932 (14%)]\tLoss: 166.432219\n",
      "Train Epoch: 44 [200000/697932 (29%)]\tLoss: 165.307375\n",
      "Train Epoch: 44 [300000/697932 (43%)]\tLoss: 164.208641\n",
      "Train Epoch: 44 [400000/697932 (57%)]\tLoss: 168.069047\n",
      "Train Epoch: 44 [500000/697932 (72%)]\tLoss: 166.136906\n",
      "Train Epoch: 44 [600000/697932 (86%)]\tLoss: 165.937531\n",
      "====> Epoch: 44 Average loss: 166.1718\n",
      "====> Test set loss: 167.8562\n",
      "Train Epoch: 45 [0/697932 (0%)]\tLoss: 168.699844\n",
      "Train Epoch: 45 [100000/697932 (14%)]\tLoss: 163.848031\n",
      "Train Epoch: 45 [200000/697932 (29%)]\tLoss: 167.181547\n",
      "Train Epoch: 45 [300000/697932 (43%)]\tLoss: 165.872906\n",
      "Train Epoch: 45 [400000/697932 (57%)]\tLoss: 166.945000\n",
      "Train Epoch: 45 [500000/697932 (72%)]\tLoss: 161.588844\n",
      "Train Epoch: 45 [600000/697932 (86%)]\tLoss: 163.769625\n",
      "====> Epoch: 45 Average loss: 166.0943\n",
      "====> Test set loss: 167.5263\n",
      "Train Epoch: 46 [0/697932 (0%)]\tLoss: 163.036031\n",
      "Train Epoch: 46 [100000/697932 (14%)]\tLoss: 164.061016\n",
      "Train Epoch: 46 [200000/697932 (29%)]\tLoss: 166.415266\n",
      "Train Epoch: 46 [300000/697932 (43%)]\tLoss: 167.022125\n",
      "Train Epoch: 46 [400000/697932 (57%)]\tLoss: 165.938516\n",
      "Train Epoch: 46 [500000/697932 (72%)]\tLoss: 165.508000\n",
      "Train Epoch: 46 [600000/697932 (86%)]\tLoss: 166.841234\n",
      "====> Epoch: 46 Average loss: 166.0859\n",
      "====> Test set loss: 167.6360\n",
      "Train Epoch: 47 [0/697932 (0%)]\tLoss: 165.429188\n",
      "Train Epoch: 47 [100000/697932 (14%)]\tLoss: 163.931125\n",
      "Train Epoch: 47 [200000/697932 (29%)]\tLoss: 163.684500\n",
      "Train Epoch: 47 [300000/697932 (43%)]\tLoss: 164.935719\n",
      "Train Epoch: 47 [400000/697932 (57%)]\tLoss: 166.571188\n",
      "Train Epoch: 47 [500000/697932 (72%)]\tLoss: 162.858250\n",
      "Train Epoch: 47 [600000/697932 (86%)]\tLoss: 165.748391\n",
      "====> Epoch: 47 Average loss: 166.0067\n",
      "====> Test set loss: 167.4753\n",
      "Train Epoch: 48 [0/697932 (0%)]\tLoss: 163.859766\n",
      "Train Epoch: 48 [100000/697932 (14%)]\tLoss: 164.013469\n",
      "Train Epoch: 48 [200000/697932 (29%)]\tLoss: 165.873609\n",
      "Train Epoch: 48 [300000/697932 (43%)]\tLoss: 167.217000\n",
      "Train Epoch: 48 [400000/697932 (57%)]\tLoss: 167.110953\n",
      "Train Epoch: 48 [500000/697932 (72%)]\tLoss: 164.219094\n",
      "Train Epoch: 48 [600000/697932 (86%)]\tLoss: 166.831406\n",
      "====> Epoch: 48 Average loss: 165.9045\n",
      "====> Test set loss: 167.2962\n",
      "Train Epoch: 49 [0/697932 (0%)]\tLoss: 167.623203\n",
      "Train Epoch: 49 [100000/697932 (14%)]\tLoss: 166.277984\n",
      "Train Epoch: 49 [200000/697932 (29%)]\tLoss: 165.860109\n",
      "Train Epoch: 49 [300000/697932 (43%)]\tLoss: 166.492125\n",
      "Train Epoch: 49 [400000/697932 (57%)]\tLoss: 163.583469\n",
      "Train Epoch: 49 [500000/697932 (72%)]\tLoss: 164.264500\n",
      "Train Epoch: 49 [600000/697932 (86%)]\tLoss: 164.283969\n",
      "====> Epoch: 49 Average loss: 165.9092\n",
      "====> Test set loss: 167.3680\n",
      "Train Epoch: 50 [0/697932 (0%)]\tLoss: 166.951500\n",
      "Train Epoch: 50 [100000/697932 (14%)]\tLoss: 165.084672\n",
      "Train Epoch: 50 [200000/697932 (29%)]\tLoss: 162.607063\n",
      "Train Epoch: 50 [300000/697932 (43%)]\tLoss: 164.121469\n",
      "Train Epoch: 50 [400000/697932 (57%)]\tLoss: 164.136594\n",
      "Train Epoch: 50 [500000/697932 (72%)]\tLoss: 167.992281\n",
      "Train Epoch: 50 [600000/697932 (86%)]\tLoss: 166.591234\n",
      "====> Epoch: 50 Average loss: 165.7964\n",
      "====> Test set loss: 167.4375\n",
      "Train Epoch: 51 [0/697932 (0%)]\tLoss: 165.807437\n",
      "Train Epoch: 51 [100000/697932 (14%)]\tLoss: 167.892344\n",
      "Train Epoch: 51 [200000/697932 (29%)]\tLoss: 164.067703\n",
      "Train Epoch: 51 [300000/697932 (43%)]\tLoss: 166.264125\n",
      "Train Epoch: 51 [400000/697932 (57%)]\tLoss: 165.313219\n",
      "Train Epoch: 51 [500000/697932 (72%)]\tLoss: 163.945094\n",
      "Train Epoch: 51 [600000/697932 (86%)]\tLoss: 164.236531\n",
      "====> Epoch: 51 Average loss: 165.8124\n",
      "====> Test set loss: 167.2816\n",
      "Train Epoch: 52 [0/697932 (0%)]\tLoss: 163.789453\n",
      "Train Epoch: 52 [100000/697932 (14%)]\tLoss: 167.408859\n",
      "Train Epoch: 52 [200000/697932 (29%)]\tLoss: 165.971844\n",
      "Train Epoch: 52 [300000/697932 (43%)]\tLoss: 168.694766\n",
      "Train Epoch: 52 [400000/697932 (57%)]\tLoss: 165.894359\n",
      "Train Epoch: 52 [500000/697932 (72%)]\tLoss: 164.200672\n",
      "Train Epoch: 52 [600000/697932 (86%)]\tLoss: 167.515453\n",
      "====> Epoch: 52 Average loss: 165.7128\n",
      "====> Test set loss: 167.2139\n",
      "Train Epoch: 53 [0/697932 (0%)]\tLoss: 164.640500\n",
      "Train Epoch: 53 [100000/697932 (14%)]\tLoss: 166.315156\n",
      "Train Epoch: 53 [200000/697932 (29%)]\tLoss: 167.949391\n",
      "Train Epoch: 53 [300000/697932 (43%)]\tLoss: 168.640531\n",
      "Train Epoch: 53 [400000/697932 (57%)]\tLoss: 166.237672\n",
      "Train Epoch: 53 [500000/697932 (72%)]\tLoss: 164.300156\n",
      "Train Epoch: 53 [600000/697932 (86%)]\tLoss: 166.911594\n",
      "====> Epoch: 53 Average loss: 165.6868\n",
      "====> Test set loss: 167.1787\n",
      "Train Epoch: 54 [0/697932 (0%)]\tLoss: 164.342250\n",
      "Train Epoch: 54 [100000/697932 (14%)]\tLoss: 166.195422\n",
      "Train Epoch: 54 [200000/697932 (29%)]\tLoss: 168.029156\n",
      "Train Epoch: 54 [300000/697932 (43%)]\tLoss: 160.534234\n",
      "Train Epoch: 54 [400000/697932 (57%)]\tLoss: 169.477031\n",
      "Train Epoch: 54 [500000/697932 (72%)]\tLoss: 168.218000\n",
      "Train Epoch: 54 [600000/697932 (86%)]\tLoss: 163.936531\n",
      "====> Epoch: 54 Average loss: 165.6057\n",
      "====> Test set loss: 167.3885\n",
      "Train Epoch: 55 [0/697932 (0%)]\tLoss: 166.936969\n",
      "Train Epoch: 55 [100000/697932 (14%)]\tLoss: 165.574453\n",
      "Train Epoch: 55 [200000/697932 (29%)]\tLoss: 160.959750\n",
      "Train Epoch: 55 [300000/697932 (43%)]\tLoss: 166.947391\n",
      "Train Epoch: 55 [400000/697932 (57%)]\tLoss: 164.340016\n",
      "Train Epoch: 55 [500000/697932 (72%)]\tLoss: 164.271203\n",
      "Train Epoch: 55 [600000/697932 (86%)]\tLoss: 164.011391\n",
      "====> Epoch: 55 Average loss: 165.5806\n",
      "====> Test set loss: 167.0748\n",
      "Train Epoch: 56 [0/697932 (0%)]\tLoss: 166.558781\n",
      "Train Epoch: 56 [100000/697932 (14%)]\tLoss: 163.811641\n",
      "Train Epoch: 56 [200000/697932 (29%)]\tLoss: 163.987063\n",
      "Train Epoch: 56 [300000/697932 (43%)]\tLoss: 165.312953\n",
      "Train Epoch: 56 [400000/697932 (57%)]\tLoss: 166.190328\n",
      "Train Epoch: 56 [500000/697932 (72%)]\tLoss: 164.363375\n",
      "Train Epoch: 56 [600000/697932 (86%)]\tLoss: 166.089281\n",
      "====> Epoch: 56 Average loss: 165.6213\n",
      "====> Test set loss: 167.3663\n",
      "Train Epoch: 57 [0/697932 (0%)]\tLoss: 165.605562\n",
      "Train Epoch: 57 [100000/697932 (14%)]\tLoss: 166.258844\n",
      "Train Epoch: 57 [200000/697932 (29%)]\tLoss: 168.408281\n",
      "Train Epoch: 57 [300000/697932 (43%)]\tLoss: 162.849063\n",
      "Train Epoch: 57 [400000/697932 (57%)]\tLoss: 167.963562\n",
      "Train Epoch: 57 [500000/697932 (72%)]\tLoss: 168.206156\n",
      "Train Epoch: 57 [600000/697932 (86%)]\tLoss: 166.261750\n",
      "====> Epoch: 57 Average loss: 165.4828\n",
      "====> Test set loss: 167.0342\n",
      "Train Epoch: 58 [0/697932 (0%)]\tLoss: 164.816766\n",
      "Train Epoch: 58 [100000/697932 (14%)]\tLoss: 166.115531\n",
      "Train Epoch: 58 [200000/697932 (29%)]\tLoss: 163.775703\n",
      "Train Epoch: 58 [300000/697932 (43%)]\tLoss: 162.445562\n",
      "Train Epoch: 58 [400000/697932 (57%)]\tLoss: 163.209672\n",
      "Train Epoch: 58 [500000/697932 (72%)]\tLoss: 164.500484\n",
      "Train Epoch: 58 [600000/697932 (86%)]\tLoss: 166.368078\n",
      "====> Epoch: 58 Average loss: 165.5006\n",
      "====> Test set loss: 167.1834\n",
      "Train Epoch: 59 [0/697932 (0%)]\tLoss: 164.448281\n",
      "Train Epoch: 59 [100000/697932 (14%)]\tLoss: 167.088219\n",
      "Train Epoch: 59 [200000/697932 (29%)]\tLoss: 165.697141\n",
      "Train Epoch: 59 [300000/697932 (43%)]\tLoss: 163.349875\n",
      "Train Epoch: 59 [400000/697932 (57%)]\tLoss: 164.725984\n",
      "Train Epoch: 59 [500000/697932 (72%)]\tLoss: 168.795266\n",
      "Train Epoch: 59 [600000/697932 (86%)]\tLoss: 166.228312\n",
      "====> Epoch: 59 Average loss: 165.4222\n",
      "====> Test set loss: 167.0683\n",
      "Train Epoch: 60 [0/697932 (0%)]\tLoss: 166.968266\n",
      "Train Epoch: 60 [100000/697932 (14%)]\tLoss: 165.807078\n",
      "Train Epoch: 60 [200000/697932 (29%)]\tLoss: 166.136797\n",
      "Train Epoch: 60 [300000/697932 (43%)]\tLoss: 164.541438\n",
      "Train Epoch: 60 [400000/697932 (57%)]\tLoss: 164.860453\n",
      "Train Epoch: 60 [500000/697932 (72%)]\tLoss: 165.417078\n",
      "Train Epoch: 60 [600000/697932 (86%)]\tLoss: 160.963797\n",
      "====> Epoch: 60 Average loss: 165.4195\n",
      "====> Test set loss: 167.1171\n",
      "Train Epoch: 61 [0/697932 (0%)]\tLoss: 164.203031\n",
      "Train Epoch: 61 [100000/697932 (14%)]\tLoss: 165.308594\n",
      "Train Epoch: 61 [200000/697932 (29%)]\tLoss: 162.829125\n",
      "Train Epoch: 61 [300000/697932 (43%)]\tLoss: 168.937000\n",
      "Train Epoch: 61 [400000/697932 (57%)]\tLoss: 167.307750\n",
      "Train Epoch: 61 [500000/697932 (72%)]\tLoss: 164.958359\n",
      "Train Epoch: 61 [600000/697932 (86%)]\tLoss: 167.461531\n",
      "====> Epoch: 61 Average loss: 165.4282\n",
      "====> Test set loss: 166.8475\n",
      "Train Epoch: 62 [0/697932 (0%)]\tLoss: 166.472875\n",
      "Train Epoch: 62 [100000/697932 (14%)]\tLoss: 165.915016\n",
      "Train Epoch: 62 [200000/697932 (29%)]\tLoss: 164.924875\n",
      "Train Epoch: 62 [300000/697932 (43%)]\tLoss: 164.397734\n",
      "Train Epoch: 62 [400000/697932 (57%)]\tLoss: 165.505953\n",
      "Train Epoch: 62 [500000/697932 (72%)]\tLoss: 162.889312\n",
      "Train Epoch: 62 [600000/697932 (86%)]\tLoss: 162.619484\n",
      "====> Epoch: 62 Average loss: 165.3071\n",
      "====> Test set loss: 166.9387\n",
      "Train Epoch: 63 [0/697932 (0%)]\tLoss: 164.953344\n",
      "Train Epoch: 63 [100000/697932 (14%)]\tLoss: 165.806938\n",
      "Train Epoch: 63 [200000/697932 (29%)]\tLoss: 166.938516\n",
      "Train Epoch: 63 [300000/697932 (43%)]\tLoss: 165.431484\n",
      "Train Epoch: 63 [400000/697932 (57%)]\tLoss: 166.061297\n",
      "Train Epoch: 63 [500000/697932 (72%)]\tLoss: 163.680641\n",
      "Train Epoch: 63 [600000/697932 (86%)]\tLoss: 169.725891\n",
      "====> Epoch: 63 Average loss: 165.3261\n",
      "====> Test set loss: 166.8146\n",
      "Train Epoch: 64 [0/697932 (0%)]\tLoss: 165.372203\n",
      "Train Epoch: 64 [100000/697932 (14%)]\tLoss: 166.784844\n",
      "Train Epoch: 64 [200000/697932 (29%)]\tLoss: 164.857719\n",
      "Train Epoch: 64 [300000/697932 (43%)]\tLoss: 167.238594\n",
      "Train Epoch: 64 [400000/697932 (57%)]\tLoss: 164.225422\n",
      "Train Epoch: 64 [500000/697932 (72%)]\tLoss: 164.628281\n",
      "Train Epoch: 64 [600000/697932 (86%)]\tLoss: 164.055391\n",
      "====> Epoch: 64 Average loss: 165.2292\n",
      "====> Test set loss: 166.7906\n",
      "Train Epoch: 65 [0/697932 (0%)]\tLoss: 166.749687\n",
      "Train Epoch: 65 [100000/697932 (14%)]\tLoss: 166.179469\n",
      "Train Epoch: 65 [200000/697932 (29%)]\tLoss: 168.154125\n",
      "Train Epoch: 65 [300000/697932 (43%)]\tLoss: 162.854656\n",
      "Train Epoch: 65 [400000/697932 (57%)]\tLoss: 166.500406\n",
      "Train Epoch: 65 [500000/697932 (72%)]\tLoss: 167.220656\n",
      "Train Epoch: 65 [600000/697932 (86%)]\tLoss: 165.994594\n",
      "====> Epoch: 65 Average loss: 165.1978\n",
      "====> Test set loss: 166.8567\n",
      "Train Epoch: 66 [0/697932 (0%)]\tLoss: 162.399641\n",
      "Train Epoch: 66 [100000/697932 (14%)]\tLoss: 165.463000\n",
      "Train Epoch: 66 [200000/697932 (29%)]\tLoss: 165.267844\n",
      "Train Epoch: 66 [300000/697932 (43%)]\tLoss: 165.230984\n",
      "Train Epoch: 66 [400000/697932 (57%)]\tLoss: 165.585625\n",
      "Train Epoch: 66 [500000/697932 (72%)]\tLoss: 164.149125\n",
      "Train Epoch: 66 [600000/697932 (86%)]\tLoss: 165.995031\n",
      "====> Epoch: 66 Average loss: 165.1594\n",
      "====> Test set loss: 166.9444\n",
      "Train Epoch: 67 [0/697932 (0%)]\tLoss: 164.866250\n",
      "Train Epoch: 67 [100000/697932 (14%)]\tLoss: 165.709781\n",
      "Train Epoch: 67 [200000/697932 (29%)]\tLoss: 162.713078\n",
      "Train Epoch: 67 [300000/697932 (43%)]\tLoss: 165.527266\n",
      "Train Epoch: 67 [400000/697932 (57%)]\tLoss: 166.633938\n",
      "Train Epoch: 67 [500000/697932 (72%)]\tLoss: 163.648922\n",
      "Train Epoch: 67 [600000/697932 (86%)]\tLoss: 163.074187\n",
      "====> Epoch: 67 Average loss: 165.1374\n",
      "====> Test set loss: 166.7725\n",
      "Train Epoch: 68 [0/697932 (0%)]\tLoss: 166.225422\n",
      "Train Epoch: 68 [100000/697932 (14%)]\tLoss: 166.201594\n",
      "Train Epoch: 68 [200000/697932 (29%)]\tLoss: 166.405734\n",
      "Train Epoch: 68 [300000/697932 (43%)]\tLoss: 163.861562\n",
      "Train Epoch: 68 [400000/697932 (57%)]\tLoss: 165.972203\n",
      "Train Epoch: 68 [500000/697932 (72%)]\tLoss: 165.987969\n",
      "Train Epoch: 68 [600000/697932 (86%)]\tLoss: 164.909188\n",
      "====> Epoch: 68 Average loss: 165.0357\n",
      "====> Test set loss: 166.7484\n",
      "Train Epoch: 69 [0/697932 (0%)]\tLoss: 164.396469\n",
      "Train Epoch: 69 [100000/697932 (14%)]\tLoss: 167.160875\n",
      "Train Epoch: 69 [200000/697932 (29%)]\tLoss: 164.676297\n",
      "Train Epoch: 69 [300000/697932 (43%)]\tLoss: 165.909203\n",
      "Train Epoch: 69 [400000/697932 (57%)]\tLoss: 165.036734\n",
      "Train Epoch: 69 [500000/697932 (72%)]\tLoss: 163.738547\n",
      "Train Epoch: 69 [600000/697932 (86%)]\tLoss: 167.576141\n",
      "====> Epoch: 69 Average loss: 165.0618\n",
      "====> Test set loss: 166.6263\n",
      "Train Epoch: 70 [0/697932 (0%)]\tLoss: 165.567031\n",
      "Train Epoch: 70 [100000/697932 (14%)]\tLoss: 167.086219\n",
      "Train Epoch: 70 [200000/697932 (29%)]\tLoss: 162.883359\n",
      "Train Epoch: 70 [300000/697932 (43%)]\tLoss: 160.918156\n",
      "Train Epoch: 70 [400000/697932 (57%)]\tLoss: 162.452344\n",
      "Train Epoch: 70 [500000/697932 (72%)]\tLoss: 164.773734\n",
      "Train Epoch: 70 [600000/697932 (86%)]\tLoss: 167.142063\n",
      "====> Epoch: 70 Average loss: 165.0161\n",
      "====> Test set loss: 166.6129\n",
      "Train Epoch: 71 [0/697932 (0%)]\tLoss: 166.282187\n",
      "Train Epoch: 71 [100000/697932 (14%)]\tLoss: 165.460266\n",
      "Train Epoch: 71 [200000/697932 (29%)]\tLoss: 165.072516\n",
      "Train Epoch: 71 [300000/697932 (43%)]\tLoss: 167.628516\n",
      "Train Epoch: 71 [400000/697932 (57%)]\tLoss: 163.501313\n",
      "Train Epoch: 71 [500000/697932 (72%)]\tLoss: 163.839750\n",
      "Train Epoch: 71 [600000/697932 (86%)]\tLoss: 167.272922\n",
      "====> Epoch: 71 Average loss: 164.9963\n",
      "====> Test set loss: 166.5591\n",
      "Train Epoch: 72 [0/697932 (0%)]\tLoss: 164.153484\n",
      "Train Epoch: 72 [100000/697932 (14%)]\tLoss: 167.848516\n",
      "Train Epoch: 72 [200000/697932 (29%)]\tLoss: 166.772562\n",
      "Train Epoch: 72 [300000/697932 (43%)]\tLoss: 166.550703\n",
      "Train Epoch: 72 [400000/697932 (57%)]\tLoss: 162.290078\n",
      "Train Epoch: 72 [500000/697932 (72%)]\tLoss: 169.968375\n",
      "Train Epoch: 72 [600000/697932 (86%)]\tLoss: 166.287047\n",
      "====> Epoch: 72 Average loss: 164.9649\n",
      "====> Test set loss: 166.7195\n",
      "Train Epoch: 73 [0/697932 (0%)]\tLoss: 165.374422\n",
      "Train Epoch: 73 [100000/697932 (14%)]\tLoss: 166.458734\n",
      "Train Epoch: 73 [200000/697932 (29%)]\tLoss: 162.610156\n",
      "Train Epoch: 73 [300000/697932 (43%)]\tLoss: 165.346297\n",
      "Train Epoch: 73 [400000/697932 (57%)]\tLoss: 163.568422\n",
      "Train Epoch: 73 [500000/697932 (72%)]\tLoss: 165.583594\n",
      "Train Epoch: 73 [600000/697932 (86%)]\tLoss: 165.141672\n",
      "====> Epoch: 73 Average loss: 164.9865\n",
      "====> Test set loss: 166.6423\n",
      "Train Epoch: 74 [0/697932 (0%)]\tLoss: 165.729531\n",
      "Train Epoch: 74 [100000/697932 (14%)]\tLoss: 164.117406\n",
      "Train Epoch: 74 [200000/697932 (29%)]\tLoss: 166.036875\n",
      "Train Epoch: 74 [300000/697932 (43%)]\tLoss: 164.730687\n",
      "Train Epoch: 74 [400000/697932 (57%)]\tLoss: 166.541250\n",
      "Train Epoch: 74 [500000/697932 (72%)]\tLoss: 164.804531\n",
      "Train Epoch: 74 [600000/697932 (86%)]\tLoss: 165.681156\n",
      "====> Epoch: 74 Average loss: 164.9161\n",
      "====> Test set loss: 166.5456\n",
      "Train Epoch: 75 [0/697932 (0%)]\tLoss: 162.819094\n",
      "Train Epoch: 75 [100000/697932 (14%)]\tLoss: 162.113313\n",
      "Train Epoch: 75 [200000/697932 (29%)]\tLoss: 164.764406\n",
      "Train Epoch: 75 [300000/697932 (43%)]\tLoss: 165.514781\n",
      "Train Epoch: 75 [400000/697932 (57%)]\tLoss: 171.161812\n",
      "Train Epoch: 75 [500000/697932 (72%)]\tLoss: 162.444437\n",
      "Train Epoch: 75 [600000/697932 (86%)]\tLoss: 164.712312\n",
      "====> Epoch: 75 Average loss: 164.8890\n",
      "====> Test set loss: 166.5421\n",
      "Train Epoch: 76 [0/697932 (0%)]\tLoss: 165.471406\n",
      "Train Epoch: 76 [100000/697932 (14%)]\tLoss: 165.624469\n",
      "Train Epoch: 76 [200000/697932 (29%)]\tLoss: 164.939875\n",
      "Train Epoch: 76 [300000/697932 (43%)]\tLoss: 164.127938\n",
      "Train Epoch: 76 [400000/697932 (57%)]\tLoss: 168.656797\n",
      "Train Epoch: 76 [500000/697932 (72%)]\tLoss: 167.676703\n",
      "Train Epoch: 76 [600000/697932 (86%)]\tLoss: 167.281547\n",
      "====> Epoch: 76 Average loss: 164.8454\n",
      "====> Test set loss: 166.6813\n",
      "Train Epoch: 77 [0/697932 (0%)]\tLoss: 166.244672\n",
      "Train Epoch: 77 [100000/697932 (14%)]\tLoss: 165.020813\n",
      "Train Epoch: 77 [200000/697932 (29%)]\tLoss: 164.681750\n",
      "Train Epoch: 77 [300000/697932 (43%)]\tLoss: 165.917188\n",
      "Train Epoch: 77 [400000/697932 (57%)]\tLoss: 162.208313\n",
      "Train Epoch: 77 [500000/697932 (72%)]\tLoss: 166.596688\n",
      "Train Epoch: 77 [600000/697932 (86%)]\tLoss: 165.108750\n",
      "====> Epoch: 77 Average loss: 164.7864\n",
      "====> Test set loss: 166.4169\n",
      "Train Epoch: 78 [0/697932 (0%)]\tLoss: 164.936813\n",
      "Train Epoch: 78 [100000/697932 (14%)]\tLoss: 166.093813\n",
      "Train Epoch: 78 [200000/697932 (29%)]\tLoss: 165.242094\n",
      "Train Epoch: 78 [300000/697932 (43%)]\tLoss: 166.451250\n",
      "Train Epoch: 78 [400000/697932 (57%)]\tLoss: 165.747781\n",
      "Train Epoch: 78 [500000/697932 (72%)]\tLoss: 163.630641\n",
      "Train Epoch: 78 [600000/697932 (86%)]\tLoss: 164.855656\n",
      "====> Epoch: 78 Average loss: 164.7747\n",
      "====> Test set loss: 166.4978\n",
      "Train Epoch: 79 [0/697932 (0%)]\tLoss: 167.865953\n",
      "Train Epoch: 79 [100000/697932 (14%)]\tLoss: 162.756625\n",
      "Train Epoch: 79 [200000/697932 (29%)]\tLoss: 165.242891\n",
      "Train Epoch: 79 [300000/697932 (43%)]\tLoss: 167.939000\n",
      "Train Epoch: 79 [400000/697932 (57%)]\tLoss: 167.395000\n",
      "Train Epoch: 79 [500000/697932 (72%)]\tLoss: 165.243297\n",
      "Train Epoch: 79 [600000/697932 (86%)]\tLoss: 164.420656\n",
      "====> Epoch: 79 Average loss: 164.7634\n",
      "====> Test set loss: 166.6637\n",
      "Train Epoch: 80 [0/697932 (0%)]\tLoss: 162.109266\n",
      "Train Epoch: 80 [100000/697932 (14%)]\tLoss: 163.805156\n",
      "Train Epoch: 80 [200000/697932 (29%)]\tLoss: 169.049609\n",
      "Train Epoch: 80 [300000/697932 (43%)]\tLoss: 164.346828\n",
      "Train Epoch: 80 [400000/697932 (57%)]\tLoss: 166.121875\n",
      "Train Epoch: 80 [500000/697932 (72%)]\tLoss: 164.174937\n",
      "Train Epoch: 80 [600000/697932 (86%)]\tLoss: 164.597234\n",
      "====> Epoch: 80 Average loss: 164.7950\n",
      "====> Test set loss: 166.5755\n",
      "Train Epoch: 81 [0/697932 (0%)]\tLoss: 164.859578\n",
      "Train Epoch: 81 [100000/697932 (14%)]\tLoss: 168.667469\n",
      "Train Epoch: 81 [200000/697932 (29%)]\tLoss: 164.213687\n",
      "Train Epoch: 81 [300000/697932 (43%)]\tLoss: 165.053219\n",
      "Train Epoch: 81 [400000/697932 (57%)]\tLoss: 166.718797\n",
      "Train Epoch: 81 [500000/697932 (72%)]\tLoss: 162.924313\n",
      "Train Epoch: 81 [600000/697932 (86%)]\tLoss: 164.275469\n",
      "====> Epoch: 81 Average loss: 164.6790\n",
      "====> Test set loss: 166.4485\n",
      "Train Epoch: 82 [0/697932 (0%)]\tLoss: 163.762406\n",
      "Train Epoch: 82 [100000/697932 (14%)]\tLoss: 163.695172\n",
      "Train Epoch: 82 [200000/697932 (29%)]\tLoss: 163.033688\n",
      "Train Epoch: 82 [300000/697932 (43%)]\tLoss: 164.054234\n",
      "Train Epoch: 82 [400000/697932 (57%)]\tLoss: 163.497656\n",
      "Train Epoch: 82 [500000/697932 (72%)]\tLoss: 164.099359\n",
      "Train Epoch: 82 [600000/697932 (86%)]\tLoss: 166.573625\n",
      "====> Epoch: 82 Average loss: 164.6729\n",
      "====> Test set loss: 166.3167\n",
      "Train Epoch: 83 [0/697932 (0%)]\tLoss: 163.985359\n",
      "Train Epoch: 83 [100000/697932 (14%)]\tLoss: 167.528469\n",
      "Train Epoch: 83 [200000/697932 (29%)]\tLoss: 164.068219\n",
      "Train Epoch: 83 [300000/697932 (43%)]\tLoss: 165.104813\n",
      "Train Epoch: 83 [400000/697932 (57%)]\tLoss: 162.918281\n",
      "Train Epoch: 83 [500000/697932 (72%)]\tLoss: 165.244313\n",
      "Train Epoch: 83 [600000/697932 (86%)]\tLoss: 164.673141\n",
      "====> Epoch: 83 Average loss: 164.6637\n",
      "====> Test set loss: 166.3602\n",
      "Train Epoch: 84 [0/697932 (0%)]\tLoss: 164.227906\n",
      "Train Epoch: 84 [100000/697932 (14%)]\tLoss: 164.161734\n",
      "Train Epoch: 84 [200000/697932 (29%)]\tLoss: 164.982937\n",
      "Train Epoch: 84 [300000/697932 (43%)]\tLoss: 162.353000\n",
      "Train Epoch: 84 [400000/697932 (57%)]\tLoss: 165.799375\n",
      "Train Epoch: 84 [500000/697932 (72%)]\tLoss: 164.575297\n",
      "Train Epoch: 84 [600000/697932 (86%)]\tLoss: 164.268297\n",
      "====> Epoch: 84 Average loss: 164.5944\n",
      "====> Test set loss: 166.4280\n",
      "Train Epoch: 85 [0/697932 (0%)]\tLoss: 165.383203\n",
      "Train Epoch: 85 [100000/697932 (14%)]\tLoss: 167.779172\n",
      "Train Epoch: 85 [200000/697932 (29%)]\tLoss: 165.668359\n",
      "Train Epoch: 85 [300000/697932 (43%)]\tLoss: 165.138781\n",
      "Train Epoch: 85 [400000/697932 (57%)]\tLoss: 166.268094\n",
      "Train Epoch: 85 [500000/697932 (72%)]\tLoss: 164.311094\n",
      "Train Epoch: 85 [600000/697932 (86%)]\tLoss: 165.349469\n",
      "====> Epoch: 85 Average loss: 164.6472\n",
      "====> Test set loss: 166.4282\n",
      "Train Epoch: 86 [0/697932 (0%)]\tLoss: 163.492922\n",
      "Train Epoch: 86 [100000/697932 (14%)]\tLoss: 166.307437\n",
      "Train Epoch: 86 [200000/697932 (29%)]\tLoss: 165.725750\n",
      "Train Epoch: 86 [300000/697932 (43%)]\tLoss: 163.610266\n",
      "Train Epoch: 86 [400000/697932 (57%)]\tLoss: 167.486750\n",
      "Train Epoch: 86 [500000/697932 (72%)]\tLoss: 166.869891\n",
      "Train Epoch: 86 [600000/697932 (86%)]\tLoss: 166.259187\n",
      "====> Epoch: 86 Average loss: 164.5862\n",
      "====> Test set loss: 166.2734\n",
      "Train Epoch: 87 [0/697932 (0%)]\tLoss: 165.274156\n",
      "Train Epoch: 87 [100000/697932 (14%)]\tLoss: 163.378078\n",
      "Train Epoch: 87 [200000/697932 (29%)]\tLoss: 164.132484\n",
      "Train Epoch: 87 [300000/697932 (43%)]\tLoss: 164.253187\n",
      "Train Epoch: 87 [400000/697932 (57%)]\tLoss: 164.072719\n",
      "Train Epoch: 87 [500000/697932 (72%)]\tLoss: 165.569031\n",
      "Train Epoch: 87 [600000/697932 (86%)]\tLoss: 165.931938\n",
      "====> Epoch: 87 Average loss: 164.5273\n",
      "====> Test set loss: 166.1241\n",
      "Train Epoch: 88 [0/697932 (0%)]\tLoss: 164.968641\n",
      "Train Epoch: 88 [100000/697932 (14%)]\tLoss: 163.364953\n",
      "Train Epoch: 88 [200000/697932 (29%)]\tLoss: 164.418531\n",
      "Train Epoch: 88 [300000/697932 (43%)]\tLoss: 164.466906\n",
      "Train Epoch: 88 [400000/697932 (57%)]\tLoss: 164.839766\n",
      "Train Epoch: 88 [500000/697932 (72%)]\tLoss: 163.395109\n",
      "Train Epoch: 88 [600000/697932 (86%)]\tLoss: 164.450125\n",
      "====> Epoch: 88 Average loss: 164.5446\n",
      "====> Test set loss: 166.3692\n",
      "Train Epoch: 89 [0/697932 (0%)]\tLoss: 162.373031\n",
      "Train Epoch: 89 [100000/697932 (14%)]\tLoss: 164.563578\n",
      "Train Epoch: 89 [200000/697932 (29%)]\tLoss: 162.682187\n",
      "Train Epoch: 89 [300000/697932 (43%)]\tLoss: 162.639797\n",
      "Train Epoch: 89 [400000/697932 (57%)]\tLoss: 166.924906\n",
      "Train Epoch: 89 [500000/697932 (72%)]\tLoss: 164.180844\n",
      "Train Epoch: 89 [600000/697932 (86%)]\tLoss: 163.008406\n",
      "====> Epoch: 89 Average loss: 164.5344\n",
      "====> Test set loss: 166.4289\n",
      "Train Epoch: 90 [0/697932 (0%)]\tLoss: 163.887063\n",
      "Train Epoch: 90 [100000/697932 (14%)]\tLoss: 166.613109\n",
      "Train Epoch: 90 [200000/697932 (29%)]\tLoss: 166.628344\n",
      "Train Epoch: 90 [300000/697932 (43%)]\tLoss: 162.578203\n",
      "Train Epoch: 90 [400000/697932 (57%)]\tLoss: 163.619828\n",
      "Train Epoch: 90 [500000/697932 (72%)]\tLoss: 165.079641\n",
      "Train Epoch: 90 [600000/697932 (86%)]\tLoss: 163.083094\n",
      "====> Epoch: 90 Average loss: 164.5864\n",
      "====> Test set loss: 166.4942\n",
      "Train Epoch: 91 [0/697932 (0%)]\tLoss: 165.288016\n",
      "Train Epoch: 91 [100000/697932 (14%)]\tLoss: 161.749266\n",
      "Train Epoch: 91 [200000/697932 (29%)]\tLoss: 162.448781\n",
      "Train Epoch: 91 [300000/697932 (43%)]\tLoss: 169.815422\n",
      "Train Epoch: 91 [400000/697932 (57%)]\tLoss: 164.321281\n",
      "Train Epoch: 91 [500000/697932 (72%)]\tLoss: 166.987609\n",
      "Train Epoch: 91 [600000/697932 (86%)]\tLoss: 164.337172\n",
      "====> Epoch: 91 Average loss: 164.4587\n",
      "====> Test set loss: 166.2237\n",
      "Train Epoch: 92 [0/697932 (0%)]\tLoss: 165.065500\n",
      "Train Epoch: 92 [100000/697932 (14%)]\tLoss: 162.952562\n",
      "Train Epoch: 92 [200000/697932 (29%)]\tLoss: 164.795547\n",
      "Train Epoch: 92 [300000/697932 (43%)]\tLoss: 164.009062\n",
      "Train Epoch: 92 [400000/697932 (57%)]\tLoss: 163.243063\n",
      "Train Epoch: 92 [500000/697932 (72%)]\tLoss: 164.123469\n",
      "Train Epoch: 92 [600000/697932 (86%)]\tLoss: 163.715000\n",
      "====> Epoch: 92 Average loss: 164.4223\n",
      "====> Test set loss: 166.2055\n",
      "Train Epoch: 93 [0/697932 (0%)]\tLoss: 165.492156\n",
      "Train Epoch: 93 [100000/697932 (14%)]\tLoss: 163.183781\n",
      "Train Epoch: 93 [200000/697932 (29%)]\tLoss: 164.547687\n",
      "Train Epoch: 93 [300000/697932 (43%)]\tLoss: 165.888125\n",
      "Train Epoch: 93 [400000/697932 (57%)]\tLoss: 164.218594\n",
      "Train Epoch: 93 [500000/697932 (72%)]\tLoss: 164.985406\n",
      "Train Epoch: 93 [600000/697932 (86%)]\tLoss: 161.181938\n",
      "====> Epoch: 93 Average loss: 164.4414\n",
      "====> Test set loss: 166.1452\n",
      "Train Epoch: 94 [0/697932 (0%)]\tLoss: 162.953969\n",
      "Train Epoch: 94 [100000/697932 (14%)]\tLoss: 166.518859\n",
      "Train Epoch: 94 [200000/697932 (29%)]\tLoss: 164.196781\n",
      "Train Epoch: 94 [300000/697932 (43%)]\tLoss: 166.918203\n",
      "Train Epoch: 94 [400000/697932 (57%)]\tLoss: 164.669406\n",
      "Train Epoch: 94 [500000/697932 (72%)]\tLoss: 164.166328\n",
      "Train Epoch: 94 [600000/697932 (86%)]\tLoss: 165.124219\n",
      "====> Epoch: 94 Average loss: 164.3888\n",
      "====> Test set loss: 166.1585\n",
      "Train Epoch: 95 [0/697932 (0%)]\tLoss: 163.891516\n",
      "Train Epoch: 95 [100000/697932 (14%)]\tLoss: 160.331984\n",
      "Train Epoch: 95 [200000/697932 (29%)]\tLoss: 164.723531\n",
      "Train Epoch: 95 [300000/697932 (43%)]\tLoss: 163.061172\n",
      "Train Epoch: 95 [400000/697932 (57%)]\tLoss: 167.519656\n",
      "Train Epoch: 95 [500000/697932 (72%)]\tLoss: 164.145109\n",
      "Train Epoch: 95 [600000/697932 (86%)]\tLoss: 165.648156\n",
      "====> Epoch: 95 Average loss: 164.3255\n",
      "====> Test set loss: 166.4253\n",
      "Train Epoch: 96 [0/697932 (0%)]\tLoss: 167.043937\n",
      "Train Epoch: 96 [100000/697932 (14%)]\tLoss: 163.135562\n",
      "Train Epoch: 96 [200000/697932 (29%)]\tLoss: 162.931328\n",
      "Train Epoch: 96 [300000/697932 (43%)]\tLoss: 162.672031\n",
      "Train Epoch: 96 [400000/697932 (57%)]\tLoss: 164.119578\n",
      "Train Epoch: 96 [500000/697932 (72%)]\tLoss: 164.506625\n",
      "Train Epoch: 96 [600000/697932 (86%)]\tLoss: 162.674313\n",
      "====> Epoch: 96 Average loss: 164.3414\n",
      "====> Test set loss: 166.2566\n",
      "Train Epoch: 97 [0/697932 (0%)]\tLoss: 165.414094\n",
      "Train Epoch: 97 [100000/697932 (14%)]\tLoss: 162.997406\n",
      "Train Epoch: 97 [200000/697932 (29%)]\tLoss: 163.761922\n",
      "Train Epoch: 97 [300000/697932 (43%)]\tLoss: 166.960750\n",
      "Train Epoch: 97 [400000/697932 (57%)]\tLoss: 160.271187\n",
      "Train Epoch: 97 [500000/697932 (72%)]\tLoss: 162.623875\n",
      "Train Epoch: 97 [600000/697932 (86%)]\tLoss: 164.516219\n",
      "====> Epoch: 97 Average loss: 164.3097\n",
      "====> Test set loss: 166.1980\n",
      "Train Epoch: 98 [0/697932 (0%)]\tLoss: 163.769922\n",
      "Train Epoch: 98 [100000/697932 (14%)]\tLoss: 166.859672\n",
      "Train Epoch: 98 [200000/697932 (29%)]\tLoss: 164.346391\n",
      "Train Epoch: 98 [300000/697932 (43%)]\tLoss: 165.340609\n",
      "Train Epoch: 98 [400000/697932 (57%)]\tLoss: 164.782312\n",
      "Train Epoch: 98 [500000/697932 (72%)]\tLoss: 163.855734\n",
      "Train Epoch: 98 [600000/697932 (86%)]\tLoss: 163.877813\n",
      "====> Epoch: 98 Average loss: 164.3225\n",
      "====> Test set loss: 166.2190\n",
      "Train Epoch: 99 [0/697932 (0%)]\tLoss: 164.491219\n",
      "Train Epoch: 99 [100000/697932 (14%)]\tLoss: 162.520078\n",
      "Train Epoch: 99 [200000/697932 (29%)]\tLoss: 163.270734\n",
      "Train Epoch: 99 [300000/697932 (43%)]\tLoss: 164.050187\n",
      "Train Epoch: 99 [400000/697932 (57%)]\tLoss: 164.303937\n",
      "Train Epoch: 99 [500000/697932 (72%)]\tLoss: 165.815812\n",
      "Train Epoch: 99 [600000/697932 (86%)]\tLoss: 165.196375\n",
      "====> Epoch: 99 Average loss: 164.2641\n",
      "====> Test set loss: 166.2198\n",
      "Train Epoch: 100 [0/697932 (0%)]\tLoss: 163.728781\n",
      "Train Epoch: 100 [100000/697932 (14%)]\tLoss: 161.142141\n",
      "Train Epoch: 100 [200000/697932 (29%)]\tLoss: 163.358234\n",
      "Train Epoch: 100 [300000/697932 (43%)]\tLoss: 164.630078\n",
      "Train Epoch: 100 [400000/697932 (57%)]\tLoss: 163.152359\n",
      "Train Epoch: 100 [500000/697932 (72%)]\tLoss: 162.423766\n",
      "Train Epoch: 100 [600000/697932 (86%)]\tLoss: 164.764469\n",
      "====> Epoch: 100 Average loss: 164.3014\n",
      "====> Test set loss: 166.1419\n",
      "Train Epoch: 101 [0/697932 (0%)]\tLoss: 164.307281\n",
      "Train Epoch: 101 [100000/697932 (14%)]\tLoss: 163.853906\n",
      "Train Epoch: 101 [200000/697932 (29%)]\tLoss: 161.713188\n",
      "Train Epoch: 101 [300000/697932 (43%)]\tLoss: 161.879188\n",
      "Train Epoch: 101 [400000/697932 (57%)]\tLoss: 165.047750\n",
      "Train Epoch: 101 [500000/697932 (72%)]\tLoss: 164.654500\n",
      "Train Epoch: 101 [600000/697932 (86%)]\tLoss: 162.471641\n",
      "====> Epoch: 101 Average loss: 164.2839\n",
      "====> Test set loss: 166.2272\n",
      "Train Epoch: 102 [0/697932 (0%)]\tLoss: 163.841000\n",
      "Train Epoch: 102 [100000/697932 (14%)]\tLoss: 162.365172\n",
      "Train Epoch: 102 [200000/697932 (29%)]\tLoss: 164.670734\n",
      "Train Epoch: 102 [300000/697932 (43%)]\tLoss: 163.026906\n",
      "Train Epoch: 102 [400000/697932 (57%)]\tLoss: 163.530984\n",
      "Train Epoch: 102 [500000/697932 (72%)]\tLoss: 163.660313\n",
      "Train Epoch: 102 [600000/697932 (86%)]\tLoss: 163.934313\n",
      "====> Epoch: 102 Average loss: 164.2150\n",
      "====> Test set loss: 165.9734\n",
      "Train Epoch: 103 [0/697932 (0%)]\tLoss: 163.389312\n",
      "Train Epoch: 103 [100000/697932 (14%)]\tLoss: 167.148781\n",
      "Train Epoch: 103 [200000/697932 (29%)]\tLoss: 165.844859\n",
      "Train Epoch: 103 [300000/697932 (43%)]\tLoss: 163.968109\n",
      "Train Epoch: 103 [400000/697932 (57%)]\tLoss: 166.254641\n",
      "Train Epoch: 103 [500000/697932 (72%)]\tLoss: 164.356234\n",
      "Train Epoch: 103 [600000/697932 (86%)]\tLoss: 162.836828\n",
      "====> Epoch: 103 Average loss: 164.2648\n",
      "====> Test set loss: 165.9464\n",
      "Train Epoch: 104 [0/697932 (0%)]\tLoss: 161.643547\n",
      "Train Epoch: 104 [100000/697932 (14%)]\tLoss: 166.229578\n",
      "Train Epoch: 104 [200000/697932 (29%)]\tLoss: 164.001734\n",
      "Train Epoch: 104 [300000/697932 (43%)]\tLoss: 164.826938\n",
      "Train Epoch: 104 [400000/697932 (57%)]\tLoss: 162.910359\n",
      "Train Epoch: 104 [500000/697932 (72%)]\tLoss: 164.617359\n",
      "Train Epoch: 104 [600000/697932 (86%)]\tLoss: 164.992844\n",
      "====> Epoch: 104 Average loss: 164.1955\n",
      "====> Test set loss: 166.0094\n",
      "Train Epoch: 105 [0/697932 (0%)]\tLoss: 164.567672\n",
      "Train Epoch: 105 [100000/697932 (14%)]\tLoss: 165.710969\n",
      "Train Epoch: 105 [200000/697932 (29%)]\tLoss: 164.863109\n",
      "Train Epoch: 105 [300000/697932 (43%)]\tLoss: 163.971688\n",
      "Train Epoch: 105 [400000/697932 (57%)]\tLoss: 162.390437\n",
      "Train Epoch: 105 [500000/697932 (72%)]\tLoss: 163.965906\n",
      "Train Epoch: 105 [600000/697932 (86%)]\tLoss: 164.682297\n",
      "====> Epoch: 105 Average loss: 164.2082\n",
      "====> Test set loss: 166.0108\n",
      "Train Epoch: 106 [0/697932 (0%)]\tLoss: 162.537781\n",
      "Train Epoch: 106 [100000/697932 (14%)]\tLoss: 163.452516\n",
      "Train Epoch: 106 [200000/697932 (29%)]\tLoss: 161.621187\n",
      "Train Epoch: 106 [300000/697932 (43%)]\tLoss: 166.678281\n",
      "Train Epoch: 106 [400000/697932 (57%)]\tLoss: 162.662234\n",
      "Train Epoch: 106 [500000/697932 (72%)]\tLoss: 163.878422\n",
      "Train Epoch: 106 [600000/697932 (86%)]\tLoss: 162.092500\n",
      "====> Epoch: 106 Average loss: 164.0923\n",
      "====> Test set loss: 165.9933\n",
      "Train Epoch: 107 [0/697932 (0%)]\tLoss: 164.575500\n",
      "Train Epoch: 107 [100000/697932 (14%)]\tLoss: 164.678859\n",
      "Train Epoch: 107 [200000/697932 (29%)]\tLoss: 165.181984\n",
      "Train Epoch: 107 [300000/697932 (43%)]\tLoss: 160.715812\n",
      "Train Epoch: 107 [400000/697932 (57%)]\tLoss: 163.492625\n",
      "Train Epoch: 107 [500000/697932 (72%)]\tLoss: 162.345781\n",
      "Train Epoch: 107 [600000/697932 (86%)]\tLoss: 163.239688\n",
      "====> Epoch: 107 Average loss: 164.1345\n",
      "====> Test set loss: 165.9892\n",
      "Train Epoch: 108 [0/697932 (0%)]\tLoss: 165.271125\n",
      "Train Epoch: 108 [100000/697932 (14%)]\tLoss: 164.818266\n",
      "Train Epoch: 108 [200000/697932 (29%)]\tLoss: 166.811797\n",
      "Train Epoch: 108 [300000/697932 (43%)]\tLoss: 163.263062\n",
      "Train Epoch: 108 [400000/697932 (57%)]\tLoss: 164.667438\n",
      "Train Epoch: 108 [500000/697932 (72%)]\tLoss: 164.517969\n",
      "Train Epoch: 108 [600000/697932 (86%)]\tLoss: 166.527500\n",
      "====> Epoch: 108 Average loss: 164.1082\n",
      "====> Test set loss: 166.1007\n",
      "Train Epoch: 109 [0/697932 (0%)]\tLoss: 164.124172\n",
      "Train Epoch: 109 [100000/697932 (14%)]\tLoss: 162.204078\n",
      "Train Epoch: 109 [200000/697932 (29%)]\tLoss: 164.243812\n",
      "Train Epoch: 109 [300000/697932 (43%)]\tLoss: 165.125031\n",
      "Train Epoch: 109 [400000/697932 (57%)]\tLoss: 163.122953\n",
      "Train Epoch: 109 [500000/697932 (72%)]\tLoss: 166.794859\n",
      "Train Epoch: 109 [600000/697932 (86%)]\tLoss: 162.932063\n",
      "====> Epoch: 109 Average loss: 164.0436\n",
      "====> Test set loss: 165.8955\n",
      "Train Epoch: 110 [0/697932 (0%)]\tLoss: 162.152906\n",
      "Train Epoch: 110 [100000/697932 (14%)]\tLoss: 166.601438\n",
      "Train Epoch: 110 [200000/697932 (29%)]\tLoss: 163.442156\n",
      "Train Epoch: 110 [300000/697932 (43%)]\tLoss: 163.872203\n",
      "Train Epoch: 110 [400000/697932 (57%)]\tLoss: 163.593781\n",
      "Train Epoch: 110 [500000/697932 (72%)]\tLoss: 166.214906\n",
      "Train Epoch: 110 [600000/697932 (86%)]\tLoss: 165.689031\n",
      "====> Epoch: 110 Average loss: 164.0159\n",
      "====> Test set loss: 165.7756\n",
      "Train Epoch: 111 [0/697932 (0%)]\tLoss: 165.827047\n",
      "Train Epoch: 111 [100000/697932 (14%)]\tLoss: 166.716438\n",
      "Train Epoch: 111 [200000/697932 (29%)]\tLoss: 162.649406\n",
      "Train Epoch: 111 [300000/697932 (43%)]\tLoss: 163.026625\n",
      "Train Epoch: 111 [400000/697932 (57%)]\tLoss: 164.007797\n",
      "Train Epoch: 111 [500000/697932 (72%)]\tLoss: 163.133484\n",
      "Train Epoch: 111 [600000/697932 (86%)]\tLoss: 164.737250\n",
      "====> Epoch: 111 Average loss: 164.1369\n",
      "====> Test set loss: 166.0517\n",
      "Train Epoch: 112 [0/697932 (0%)]\tLoss: 162.552984\n",
      "Train Epoch: 112 [100000/697932 (14%)]\tLoss: 164.681844\n",
      "Train Epoch: 112 [200000/697932 (29%)]\tLoss: 163.814281\n",
      "Train Epoch: 112 [300000/697932 (43%)]\tLoss: 163.490766\n",
      "Train Epoch: 112 [400000/697932 (57%)]\tLoss: 163.935156\n",
      "Train Epoch: 112 [500000/697932 (72%)]\tLoss: 162.959859\n",
      "Train Epoch: 112 [600000/697932 (86%)]\tLoss: 164.272906\n",
      "====> Epoch: 112 Average loss: 164.0346\n",
      "====> Test set loss: 165.9842\n",
      "Train Epoch: 113 [0/697932 (0%)]\tLoss: 162.566500\n",
      "Train Epoch: 113 [100000/697932 (14%)]\tLoss: 164.167719\n",
      "Train Epoch: 113 [200000/697932 (29%)]\tLoss: 164.488344\n",
      "Train Epoch: 113 [300000/697932 (43%)]\tLoss: 164.221344\n",
      "Train Epoch: 113 [400000/697932 (57%)]\tLoss: 163.519062\n",
      "Train Epoch: 113 [500000/697932 (72%)]\tLoss: 163.569781\n",
      "Train Epoch: 113 [600000/697932 (86%)]\tLoss: 163.764312\n",
      "====> Epoch: 113 Average loss: 164.0400\n",
      "====> Test set loss: 166.0413\n",
      "Train Epoch: 114 [0/697932 (0%)]\tLoss: 166.072734\n",
      "Train Epoch: 114 [100000/697932 (14%)]\tLoss: 162.872500\n",
      "Train Epoch: 114 [200000/697932 (29%)]\tLoss: 160.893797\n",
      "Train Epoch: 114 [300000/697932 (43%)]\tLoss: 164.085453\n",
      "Train Epoch: 114 [400000/697932 (57%)]\tLoss: 165.012000\n",
      "Train Epoch: 114 [500000/697932 (72%)]\tLoss: 161.078172\n",
      "Train Epoch: 114 [600000/697932 (86%)]\tLoss: 161.868266\n",
      "====> Epoch: 114 Average loss: 163.9741\n",
      "====> Test set loss: 166.0563\n",
      "Train Epoch: 115 [0/697932 (0%)]\tLoss: 163.161797\n",
      "Train Epoch: 115 [100000/697932 (14%)]\tLoss: 162.929953\n",
      "Train Epoch: 115 [200000/697932 (29%)]\tLoss: 161.442000\n",
      "Train Epoch: 115 [300000/697932 (43%)]\tLoss: 163.584781\n",
      "Train Epoch: 115 [400000/697932 (57%)]\tLoss: 163.607719\n",
      "Train Epoch: 115 [500000/697932 (72%)]\tLoss: 167.000625\n",
      "Train Epoch: 115 [600000/697932 (86%)]\tLoss: 164.191469\n",
      "====> Epoch: 115 Average loss: 163.9847\n",
      "====> Test set loss: 165.8369\n",
      "Train Epoch: 116 [0/697932 (0%)]\tLoss: 162.022656\n",
      "Train Epoch: 116 [100000/697932 (14%)]\tLoss: 162.811375\n",
      "Train Epoch: 116 [200000/697932 (29%)]\tLoss: 165.251531\n",
      "Train Epoch: 116 [300000/697932 (43%)]\tLoss: 162.937156\n",
      "Train Epoch: 116 [400000/697932 (57%)]\tLoss: 164.262281\n",
      "Train Epoch: 116 [500000/697932 (72%)]\tLoss: 164.536063\n",
      "Train Epoch: 116 [600000/697932 (86%)]\tLoss: 164.062297\n",
      "====> Epoch: 116 Average loss: 163.9995\n",
      "====> Test set loss: 165.8727\n",
      "Train Epoch: 117 [0/697932 (0%)]\tLoss: 162.058328\n",
      "Train Epoch: 117 [100000/697932 (14%)]\tLoss: 162.638891\n",
      "Train Epoch: 117 [200000/697932 (29%)]\tLoss: 161.261547\n",
      "Train Epoch: 117 [300000/697932 (43%)]\tLoss: 164.540703\n",
      "Train Epoch: 117 [400000/697932 (57%)]\tLoss: 165.118000\n",
      "Train Epoch: 117 [500000/697932 (72%)]\tLoss: 166.766594\n",
      "Train Epoch: 117 [600000/697932 (86%)]\tLoss: 163.817828\n",
      "====> Epoch: 117 Average loss: 163.9327\n",
      "====> Test set loss: 165.9230\n",
      "Train Epoch: 118 [0/697932 (0%)]\tLoss: 161.567641\n",
      "Train Epoch: 118 [100000/697932 (14%)]\tLoss: 162.019609\n",
      "Train Epoch: 118 [200000/697932 (29%)]\tLoss: 166.811172\n",
      "Train Epoch: 118 [300000/697932 (43%)]\tLoss: 165.131547\n",
      "Train Epoch: 118 [400000/697932 (57%)]\tLoss: 163.106812\n",
      "Train Epoch: 118 [500000/697932 (72%)]\tLoss: 168.042969\n",
      "Train Epoch: 118 [600000/697932 (86%)]\tLoss: 162.634844\n",
      "====> Epoch: 118 Average loss: 164.0728\n",
      "====> Test set loss: 165.7192\n",
      "Train Epoch: 119 [0/697932 (0%)]\tLoss: 163.296781\n",
      "Train Epoch: 119 [100000/697932 (14%)]\tLoss: 164.025469\n",
      "Train Epoch: 119 [200000/697932 (29%)]\tLoss: 162.064703\n",
      "Train Epoch: 119 [300000/697932 (43%)]\tLoss: 164.824938\n",
      "Train Epoch: 119 [400000/697932 (57%)]\tLoss: 162.190141\n",
      "Train Epoch: 119 [500000/697932 (72%)]\tLoss: 161.874750\n",
      "Train Epoch: 119 [600000/697932 (86%)]\tLoss: 163.249047\n",
      "====> Epoch: 119 Average loss: 163.9477\n",
      "====> Test set loss: 165.9247\n",
      "Train Epoch: 120 [0/697932 (0%)]\tLoss: 169.643578\n",
      "Train Epoch: 120 [100000/697932 (14%)]\tLoss: 164.064875\n",
      "Train Epoch: 120 [200000/697932 (29%)]\tLoss: 164.357703\n",
      "Train Epoch: 120 [300000/697932 (43%)]\tLoss: 161.203859\n",
      "Train Epoch: 120 [400000/697932 (57%)]\tLoss: 164.518375\n",
      "Train Epoch: 120 [500000/697932 (72%)]\tLoss: 163.408969\n",
      "Train Epoch: 120 [600000/697932 (86%)]\tLoss: 164.204000\n",
      "====> Epoch: 120 Average loss: 163.9139\n",
      "====> Test set loss: 166.1450\n",
      "Train Epoch: 121 [0/697932 (0%)]\tLoss: 164.181453\n",
      "Train Epoch: 121 [100000/697932 (14%)]\tLoss: 165.062891\n",
      "Train Epoch: 121 [200000/697932 (29%)]\tLoss: 161.336813\n",
      "Train Epoch: 121 [300000/697932 (43%)]\tLoss: 163.774828\n",
      "Train Epoch: 121 [400000/697932 (57%)]\tLoss: 162.697469\n",
      "Train Epoch: 121 [500000/697932 (72%)]\tLoss: 165.249500\n",
      "Train Epoch: 121 [600000/697932 (86%)]\tLoss: 161.961922\n",
      "====> Epoch: 121 Average loss: 163.8934\n",
      "====> Test set loss: 165.8120\n",
      "Train Epoch: 122 [0/697932 (0%)]\tLoss: 160.833078\n",
      "Train Epoch: 122 [100000/697932 (14%)]\tLoss: 159.526281\n",
      "Train Epoch: 122 [200000/697932 (29%)]\tLoss: 163.236422\n",
      "Train Epoch: 122 [300000/697932 (43%)]\tLoss: 164.864609\n",
      "Train Epoch: 122 [400000/697932 (57%)]\tLoss: 161.162094\n",
      "Train Epoch: 122 [500000/697932 (72%)]\tLoss: 165.255031\n",
      "Train Epoch: 122 [600000/697932 (86%)]\tLoss: 165.818734\n",
      "====> Epoch: 122 Average loss: 163.8922\n",
      "====> Test set loss: 165.9319\n",
      "Train Epoch: 123 [0/697932 (0%)]\tLoss: 164.764438\n",
      "Train Epoch: 123 [100000/697932 (14%)]\tLoss: 162.618234\n",
      "Train Epoch: 123 [200000/697932 (29%)]\tLoss: 164.142734\n",
      "Train Epoch: 123 [300000/697932 (43%)]\tLoss: 161.087234\n",
      "Train Epoch: 123 [400000/697932 (57%)]\tLoss: 164.559297\n",
      "Train Epoch: 123 [500000/697932 (72%)]\tLoss: 162.771672\n",
      "Train Epoch: 123 [600000/697932 (86%)]\tLoss: 163.164078\n",
      "====> Epoch: 123 Average loss: 163.8285\n",
      "====> Test set loss: 165.6949\n",
      "Train Epoch: 124 [0/697932 (0%)]\tLoss: 164.707141\n",
      "Train Epoch: 124 [100000/697932 (14%)]\tLoss: 163.869844\n",
      "Train Epoch: 124 [200000/697932 (29%)]\tLoss: 163.991172\n",
      "Train Epoch: 124 [300000/697932 (43%)]\tLoss: 161.530250\n",
      "Train Epoch: 124 [400000/697932 (57%)]\tLoss: 162.392766\n",
      "Train Epoch: 124 [500000/697932 (72%)]\tLoss: 164.850406\n",
      "Train Epoch: 124 [600000/697932 (86%)]\tLoss: 164.999516\n",
      "====> Epoch: 124 Average loss: 163.7754\n",
      "====> Test set loss: 165.7837\n",
      "Train Epoch: 125 [0/697932 (0%)]\tLoss: 164.986984\n",
      "Train Epoch: 125 [100000/697932 (14%)]\tLoss: 163.044828\n",
      "Train Epoch: 125 [200000/697932 (29%)]\tLoss: 161.649641\n",
      "Train Epoch: 125 [300000/697932 (43%)]\tLoss: 165.490781\n",
      "Train Epoch: 125 [400000/697932 (57%)]\tLoss: 163.829922\n",
      "Train Epoch: 125 [500000/697932 (72%)]\tLoss: 164.695953\n",
      "Train Epoch: 125 [600000/697932 (86%)]\tLoss: 164.349437\n",
      "====> Epoch: 125 Average loss: 163.7756\n",
      "====> Test set loss: 165.8298\n",
      "Train Epoch: 126 [0/697932 (0%)]\tLoss: 166.135797\n",
      "Train Epoch: 126 [100000/697932 (14%)]\tLoss: 163.829719\n",
      "Train Epoch: 126 [200000/697932 (29%)]\tLoss: 161.320516\n",
      "Train Epoch: 126 [300000/697932 (43%)]\tLoss: 162.164031\n",
      "Train Epoch: 126 [400000/697932 (57%)]\tLoss: 163.321188\n",
      "Train Epoch: 126 [500000/697932 (72%)]\tLoss: 162.529953\n",
      "Train Epoch: 126 [600000/697932 (86%)]\tLoss: 161.209641\n",
      "====> Epoch: 126 Average loss: 163.7966\n",
      "====> Test set loss: 165.8759\n",
      "Train Epoch: 127 [0/697932 (0%)]\tLoss: 162.235625\n",
      "Train Epoch: 127 [100000/697932 (14%)]\tLoss: 165.372547\n",
      "Train Epoch: 127 [200000/697932 (29%)]\tLoss: 163.115187\n",
      "Train Epoch: 127 [300000/697932 (43%)]\tLoss: 164.357047\n",
      "Train Epoch: 127 [400000/697932 (57%)]\tLoss: 163.294547\n",
      "Train Epoch: 127 [500000/697932 (72%)]\tLoss: 162.822734\n",
      "Train Epoch: 127 [600000/697932 (86%)]\tLoss: 168.374562\n",
      "====> Epoch: 127 Average loss: 163.8008\n",
      "====> Test set loss: 165.9893\n",
      "Train Epoch: 128 [0/697932 (0%)]\tLoss: 164.159484\n",
      "Train Epoch: 128 [100000/697932 (14%)]\tLoss: 165.532813\n",
      "Train Epoch: 128 [200000/697932 (29%)]\tLoss: 162.567547\n",
      "Train Epoch: 128 [300000/697932 (43%)]\tLoss: 163.899141\n",
      "Train Epoch: 128 [400000/697932 (57%)]\tLoss: 164.886812\n",
      "Train Epoch: 128 [500000/697932 (72%)]\tLoss: 164.438703\n",
      "Train Epoch: 128 [600000/697932 (86%)]\tLoss: 167.222609\n",
      "====> Epoch: 128 Average loss: 163.8514\n",
      "====> Test set loss: 165.9091\n",
      "Train Epoch: 129 [0/697932 (0%)]\tLoss: 162.832953\n",
      "Train Epoch: 129 [100000/697932 (14%)]\tLoss: 162.683188\n",
      "Train Epoch: 129 [200000/697932 (29%)]\tLoss: 164.848953\n",
      "Train Epoch: 129 [300000/697932 (43%)]\tLoss: 163.617437\n",
      "Train Epoch: 129 [400000/697932 (57%)]\tLoss: 162.933469\n",
      "Train Epoch: 129 [500000/697932 (72%)]\tLoss: 160.834547\n",
      "Train Epoch: 129 [600000/697932 (86%)]\tLoss: 164.574953\n",
      "====> Epoch: 129 Average loss: 163.7864\n",
      "====> Test set loss: 165.8112\n",
      "Train Epoch: 130 [0/697932 (0%)]\tLoss: 163.911031\n",
      "Train Epoch: 130 [100000/697932 (14%)]\tLoss: 164.439547\n",
      "Train Epoch: 130 [200000/697932 (29%)]\tLoss: 165.360312\n",
      "Train Epoch: 130 [300000/697932 (43%)]\tLoss: 162.469531\n",
      "Train Epoch: 130 [400000/697932 (57%)]\tLoss: 164.671125\n",
      "Train Epoch: 130 [500000/697932 (72%)]\tLoss: 163.457250\n",
      "Train Epoch: 130 [600000/697932 (86%)]\tLoss: 164.032219\n",
      "====> Epoch: 130 Average loss: 163.7176\n",
      "====> Test set loss: 165.8829\n",
      "Train Epoch: 131 [0/697932 (0%)]\tLoss: 167.707641\n",
      "Train Epoch: 131 [100000/697932 (14%)]\tLoss: 164.925062\n",
      "Train Epoch: 131 [200000/697932 (29%)]\tLoss: 161.225703\n",
      "Train Epoch: 131 [300000/697932 (43%)]\tLoss: 168.120359\n",
      "Train Epoch: 131 [400000/697932 (57%)]\tLoss: 163.651578\n",
      "Train Epoch: 131 [500000/697932 (72%)]\tLoss: 163.063359\n",
      "Train Epoch: 131 [600000/697932 (86%)]\tLoss: 161.867562\n",
      "====> Epoch: 131 Average loss: 163.7256\n",
      "====> Test set loss: 165.7550\n",
      "Train Epoch: 132 [0/697932 (0%)]\tLoss: 163.711422\n",
      "Train Epoch: 132 [100000/697932 (14%)]\tLoss: 162.110672\n",
      "Train Epoch: 132 [200000/697932 (29%)]\tLoss: 164.754156\n",
      "Train Epoch: 132 [300000/697932 (43%)]\tLoss: 161.904125\n",
      "Train Epoch: 132 [400000/697932 (57%)]\tLoss: 164.944828\n",
      "Train Epoch: 132 [500000/697932 (72%)]\tLoss: 164.153531\n",
      "Train Epoch: 132 [600000/697932 (86%)]\tLoss: 164.807594\n",
      "====> Epoch: 132 Average loss: 163.6907\n",
      "====> Test set loss: 165.8577\n",
      "Train Epoch: 133 [0/697932 (0%)]\tLoss: 163.447844\n",
      "Train Epoch: 133 [100000/697932 (14%)]\tLoss: 161.953656\n",
      "Train Epoch: 133 [200000/697932 (29%)]\tLoss: 162.721484\n",
      "Train Epoch: 133 [300000/697932 (43%)]\tLoss: 165.083703\n",
      "Train Epoch: 133 [400000/697932 (57%)]\tLoss: 162.840281\n",
      "Train Epoch: 133 [500000/697932 (72%)]\tLoss: 161.806641\n",
      "Train Epoch: 133 [600000/697932 (86%)]\tLoss: 166.547281\n",
      "====> Epoch: 133 Average loss: 163.7203\n",
      "====> Test set loss: 165.5543\n",
      "Train Epoch: 134 [0/697932 (0%)]\tLoss: 160.415859\n",
      "Train Epoch: 134 [100000/697932 (14%)]\tLoss: 160.755156\n",
      "Train Epoch: 134 [200000/697932 (29%)]\tLoss: 164.255953\n",
      "Train Epoch: 134 [300000/697932 (43%)]\tLoss: 165.916578\n",
      "Train Epoch: 134 [400000/697932 (57%)]\tLoss: 161.977094\n",
      "Train Epoch: 134 [500000/697932 (72%)]\tLoss: 164.830563\n",
      "Train Epoch: 134 [600000/697932 (86%)]\tLoss: 164.631109\n",
      "====> Epoch: 134 Average loss: 163.6478\n",
      "====> Test set loss: 165.6170\n",
      "Train Epoch: 135 [0/697932 (0%)]\tLoss: 164.934063\n",
      "Train Epoch: 135 [100000/697932 (14%)]\tLoss: 166.561781\n",
      "Train Epoch: 135 [200000/697932 (29%)]\tLoss: 163.151953\n",
      "Train Epoch: 135 [300000/697932 (43%)]\tLoss: 164.140094\n",
      "Train Epoch: 135 [400000/697932 (57%)]\tLoss: 162.377859\n",
      "Train Epoch: 135 [500000/697932 (72%)]\tLoss: 165.556609\n",
      "Train Epoch: 135 [600000/697932 (86%)]\tLoss: 166.646922\n",
      "====> Epoch: 135 Average loss: 163.6342\n",
      "====> Test set loss: 165.6538\n",
      "Train Epoch: 136 [0/697932 (0%)]\tLoss: 163.697094\n",
      "Train Epoch: 136 [100000/697932 (14%)]\tLoss: 162.409047\n",
      "Train Epoch: 136 [200000/697932 (29%)]\tLoss: 167.477109\n",
      "Train Epoch: 136 [300000/697932 (43%)]\tLoss: 162.540594\n",
      "Train Epoch: 136 [400000/697932 (57%)]\tLoss: 163.954125\n",
      "Train Epoch: 136 [500000/697932 (72%)]\tLoss: 164.890594\n",
      "Train Epoch: 136 [600000/697932 (86%)]\tLoss: 163.286016\n",
      "====> Epoch: 136 Average loss: 163.6368\n",
      "====> Test set loss: 165.6702\n",
      "Train Epoch: 137 [0/697932 (0%)]\tLoss: 162.556359\n",
      "Train Epoch: 137 [100000/697932 (14%)]\tLoss: 166.758828\n",
      "Train Epoch: 137 [200000/697932 (29%)]\tLoss: 164.352844\n",
      "Train Epoch: 137 [300000/697932 (43%)]\tLoss: 165.911844\n",
      "Train Epoch: 137 [400000/697932 (57%)]\tLoss: 163.568656\n",
      "Train Epoch: 137 [500000/697932 (72%)]\tLoss: 164.917937\n",
      "Train Epoch: 137 [600000/697932 (86%)]\tLoss: 165.722359\n",
      "====> Epoch: 137 Average loss: 163.6715\n",
      "====> Test set loss: 165.7519\n",
      "Train Epoch: 138 [0/697932 (0%)]\tLoss: 162.796469\n",
      "Train Epoch: 138 [100000/697932 (14%)]\tLoss: 163.036047\n",
      "Train Epoch: 138 [200000/697932 (29%)]\tLoss: 162.312359\n",
      "Train Epoch: 138 [300000/697932 (43%)]\tLoss: 166.899875\n",
      "Train Epoch: 138 [400000/697932 (57%)]\tLoss: 164.990906\n",
      "Train Epoch: 138 [500000/697932 (72%)]\tLoss: 165.402516\n",
      "Train Epoch: 138 [600000/697932 (86%)]\tLoss: 163.333750\n",
      "====> Epoch: 138 Average loss: 163.5972\n",
      "====> Test set loss: 165.8731\n",
      "Train Epoch: 139 [0/697932 (0%)]\tLoss: 162.516328\n",
      "Train Epoch: 139 [100000/697932 (14%)]\tLoss: 162.330719\n",
      "Train Epoch: 139 [200000/697932 (29%)]\tLoss: 164.083469\n",
      "Train Epoch: 139 [300000/697932 (43%)]\tLoss: 163.649094\n",
      "Train Epoch: 139 [400000/697932 (57%)]\tLoss: 165.729562\n",
      "Train Epoch: 139 [500000/697932 (72%)]\tLoss: 165.447547\n",
      "Train Epoch: 139 [600000/697932 (86%)]\tLoss: 163.824234\n",
      "====> Epoch: 139 Average loss: 163.7043\n",
      "====> Test set loss: 165.5418\n",
      "Train Epoch: 140 [0/697932 (0%)]\tLoss: 166.630016\n",
      "Train Epoch: 140 [100000/697932 (14%)]\tLoss: 165.097187\n",
      "Train Epoch: 140 [200000/697932 (29%)]\tLoss: 164.561625\n",
      "Train Epoch: 140 [300000/697932 (43%)]\tLoss: 163.753187\n",
      "Train Epoch: 140 [400000/697932 (57%)]\tLoss: 162.202859\n",
      "Train Epoch: 140 [500000/697932 (72%)]\tLoss: 164.784547\n",
      "Train Epoch: 140 [600000/697932 (86%)]\tLoss: 163.807125\n",
      "====> Epoch: 140 Average loss: 163.5902\n",
      "====> Test set loss: 165.4804\n",
      "Train Epoch: 141 [0/697932 (0%)]\tLoss: 164.157187\n",
      "Train Epoch: 141 [100000/697932 (14%)]\tLoss: 164.044781\n",
      "Train Epoch: 141 [200000/697932 (29%)]\tLoss: 163.254484\n",
      "Train Epoch: 141 [300000/697932 (43%)]\tLoss: 166.057312\n",
      "Train Epoch: 141 [400000/697932 (57%)]\tLoss: 162.134437\n",
      "Train Epoch: 141 [500000/697932 (72%)]\tLoss: 163.472859\n",
      "Train Epoch: 141 [600000/697932 (86%)]\tLoss: 162.544781\n",
      "====> Epoch: 141 Average loss: 163.5833\n",
      "====> Test set loss: 165.7396\n",
      "Train Epoch: 142 [0/697932 (0%)]\tLoss: 162.313312\n",
      "Train Epoch: 142 [100000/697932 (14%)]\tLoss: 167.504094\n",
      "Train Epoch: 142 [200000/697932 (29%)]\tLoss: 164.814812\n",
      "Train Epoch: 142 [300000/697932 (43%)]\tLoss: 163.243422\n",
      "Train Epoch: 142 [400000/697932 (57%)]\tLoss: 161.040625\n",
      "Train Epoch: 142 [500000/697932 (72%)]\tLoss: 165.223203\n",
      "Train Epoch: 142 [600000/697932 (86%)]\tLoss: 162.100281\n",
      "====> Epoch: 142 Average loss: 163.5073\n",
      "====> Test set loss: 165.7019\n",
      "Train Epoch: 143 [0/697932 (0%)]\tLoss: 164.080875\n",
      "Train Epoch: 143 [100000/697932 (14%)]\tLoss: 165.096438\n",
      "Train Epoch: 143 [200000/697932 (29%)]\tLoss: 159.107500\n",
      "Train Epoch: 143 [300000/697932 (43%)]\tLoss: 164.002516\n",
      "Train Epoch: 143 [400000/697932 (57%)]\tLoss: 164.688406\n",
      "Train Epoch: 143 [500000/697932 (72%)]\tLoss: 163.496719\n",
      "Train Epoch: 143 [600000/697932 (86%)]\tLoss: 164.571937\n",
      "====> Epoch: 143 Average loss: 163.5168\n",
      "====> Test set loss: 165.6757\n",
      "Train Epoch: 144 [0/697932 (0%)]\tLoss: 162.164016\n",
      "Train Epoch: 144 [100000/697932 (14%)]\tLoss: 163.787281\n",
      "Train Epoch: 144 [200000/697932 (29%)]\tLoss: 162.159797\n",
      "Train Epoch: 144 [300000/697932 (43%)]\tLoss: 161.346641\n",
      "Train Epoch: 144 [400000/697932 (57%)]\tLoss: 164.312703\n",
      "Train Epoch: 144 [500000/697932 (72%)]\tLoss: 160.593859\n",
      "Train Epoch: 144 [600000/697932 (86%)]\tLoss: 166.578094\n",
      "====> Epoch: 144 Average loss: 163.5162\n",
      "====> Test set loss: 165.4334\n",
      "Train Epoch: 145 [0/697932 (0%)]\tLoss: 163.717844\n",
      "Train Epoch: 145 [100000/697932 (14%)]\tLoss: 166.304797\n",
      "Train Epoch: 145 [200000/697932 (29%)]\tLoss: 166.082297\n",
      "Train Epoch: 145 [300000/697932 (43%)]\tLoss: 159.114937\n",
      "Train Epoch: 145 [400000/697932 (57%)]\tLoss: 160.011625\n",
      "Train Epoch: 145 [500000/697932 (72%)]\tLoss: 162.109516\n",
      "Train Epoch: 145 [600000/697932 (86%)]\tLoss: 161.870000\n",
      "====> Epoch: 145 Average loss: 163.4870\n",
      "====> Test set loss: 165.4702\n",
      "Train Epoch: 146 [0/697932 (0%)]\tLoss: 166.146016\n",
      "Train Epoch: 146 [100000/697932 (14%)]\tLoss: 162.038750\n",
      "Train Epoch: 146 [200000/697932 (29%)]\tLoss: 163.811375\n",
      "Train Epoch: 146 [300000/697932 (43%)]\tLoss: 165.609938\n",
      "Train Epoch: 146 [400000/697932 (57%)]\tLoss: 163.421703\n",
      "Train Epoch: 146 [500000/697932 (72%)]\tLoss: 163.773047\n",
      "Train Epoch: 146 [600000/697932 (86%)]\tLoss: 163.228312\n",
      "====> Epoch: 146 Average loss: 163.5604\n",
      "====> Test set loss: 165.5312\n",
      "Train Epoch: 147 [0/697932 (0%)]\tLoss: 164.784234\n",
      "Train Epoch: 147 [100000/697932 (14%)]\tLoss: 164.012109\n",
      "Train Epoch: 147 [200000/697932 (29%)]\tLoss: 159.721172\n",
      "Train Epoch: 147 [300000/697932 (43%)]\tLoss: 163.158062\n",
      "Train Epoch: 147 [400000/697932 (57%)]\tLoss: 166.497422\n",
      "Train Epoch: 147 [500000/697932 (72%)]\tLoss: 164.258859\n",
      "Train Epoch: 147 [600000/697932 (86%)]\tLoss: 162.610500\n",
      "====> Epoch: 147 Average loss: 163.4940\n",
      "====> Test set loss: 165.5277\n",
      "Train Epoch: 148 [0/697932 (0%)]\tLoss: 164.039813\n",
      "Train Epoch: 148 [100000/697932 (14%)]\tLoss: 161.476016\n",
      "Train Epoch: 148 [200000/697932 (29%)]\tLoss: 160.118172\n",
      "Train Epoch: 148 [300000/697932 (43%)]\tLoss: 167.701156\n",
      "Train Epoch: 148 [400000/697932 (57%)]\tLoss: 164.218781\n",
      "Train Epoch: 148 [500000/697932 (72%)]\tLoss: 164.678219\n",
      "Train Epoch: 148 [600000/697932 (86%)]\tLoss: 162.226484\n",
      "====> Epoch: 148 Average loss: 163.4393\n",
      "====> Test set loss: 165.7490\n",
      "Train Epoch: 149 [0/697932 (0%)]\tLoss: 164.061297\n",
      "Train Epoch: 149 [100000/697932 (14%)]\tLoss: 164.118328\n",
      "Train Epoch: 149 [200000/697932 (29%)]\tLoss: 165.646938\n",
      "Train Epoch: 149 [300000/697932 (43%)]\tLoss: 162.243453\n",
      "Train Epoch: 149 [400000/697932 (57%)]\tLoss: 162.078219\n",
      "Train Epoch: 149 [500000/697932 (72%)]\tLoss: 162.670016\n",
      "Train Epoch: 149 [600000/697932 (86%)]\tLoss: 164.799859\n",
      "====> Epoch: 149 Average loss: 163.4627\n",
      "====> Test set loss: 165.5656\n",
      "Train Epoch: 150 [0/697932 (0%)]\tLoss: 163.580422\n",
      "Train Epoch: 150 [100000/697932 (14%)]\tLoss: 162.924313\n",
      "Train Epoch: 150 [200000/697932 (29%)]\tLoss: 167.328125\n",
      "Train Epoch: 150 [300000/697932 (43%)]\tLoss: 163.086156\n",
      "Train Epoch: 150 [400000/697932 (57%)]\tLoss: 164.317953\n",
      "Train Epoch: 150 [500000/697932 (72%)]\tLoss: 164.324609\n",
      "Train Epoch: 150 [600000/697932 (86%)]\tLoss: 161.431156\n",
      "====> Epoch: 150 Average loss: 163.4762\n",
      "====> Test set loss: 165.4998\n",
      "Train Epoch: 151 [0/697932 (0%)]\tLoss: 164.727812\n",
      "Train Epoch: 151 [100000/697932 (14%)]\tLoss: 161.447375\n",
      "Train Epoch: 151 [200000/697932 (29%)]\tLoss: 162.027125\n",
      "Train Epoch: 151 [300000/697932 (43%)]\tLoss: 162.630672\n",
      "Train Epoch: 151 [400000/697932 (57%)]\tLoss: 167.100281\n",
      "Train Epoch: 151 [500000/697932 (72%)]\tLoss: 162.620563\n",
      "Train Epoch: 151 [600000/697932 (86%)]\tLoss: 167.157641\n",
      "====> Epoch: 151 Average loss: 163.4282\n",
      "====> Test set loss: 165.5231\n",
      "Train Epoch: 152 [0/697932 (0%)]\tLoss: 162.114797\n",
      "Train Epoch: 152 [100000/697932 (14%)]\tLoss: 164.801000\n",
      "Train Epoch: 152 [200000/697932 (29%)]\tLoss: 164.115000\n",
      "Train Epoch: 152 [300000/697932 (43%)]\tLoss: 165.199312\n",
      "Train Epoch: 152 [400000/697932 (57%)]\tLoss: 165.626922\n",
      "Train Epoch: 152 [500000/697932 (72%)]\tLoss: 162.484703\n",
      "Train Epoch: 152 [600000/697932 (86%)]\tLoss: 165.049875\n",
      "====> Epoch: 152 Average loss: 163.4264\n",
      "====> Test set loss: 165.4664\n",
      "Train Epoch: 153 [0/697932 (0%)]\tLoss: 162.446234\n",
      "Train Epoch: 153 [100000/697932 (14%)]\tLoss: 164.110437\n",
      "Train Epoch: 153 [200000/697932 (29%)]\tLoss: 164.537297\n",
      "Train Epoch: 153 [300000/697932 (43%)]\tLoss: 163.809469\n",
      "Train Epoch: 153 [400000/697932 (57%)]\tLoss: 162.040750\n",
      "Train Epoch: 153 [500000/697932 (72%)]\tLoss: 159.971047\n",
      "Train Epoch: 153 [600000/697932 (86%)]\tLoss: 165.397109\n",
      "====> Epoch: 153 Average loss: 163.4206\n",
      "====> Test set loss: 165.6202\n",
      "Train Epoch: 154 [0/697932 (0%)]\tLoss: 163.873469\n",
      "Train Epoch: 154 [100000/697932 (14%)]\tLoss: 165.649594\n",
      "Train Epoch: 154 [200000/697932 (29%)]\tLoss: 161.551625\n",
      "Train Epoch: 154 [300000/697932 (43%)]\tLoss: 165.451109\n",
      "Train Epoch: 154 [400000/697932 (57%)]\tLoss: 161.871250\n",
      "Train Epoch: 154 [500000/697932 (72%)]\tLoss: 165.644937\n",
      "Train Epoch: 154 [600000/697932 (86%)]\tLoss: 162.815469\n",
      "====> Epoch: 154 Average loss: 163.3973\n",
      "====> Test set loss: 165.5275\n",
      "Train Epoch: 155 [0/697932 (0%)]\tLoss: 161.229625\n",
      "Train Epoch: 155 [100000/697932 (14%)]\tLoss: 161.698703\n",
      "Train Epoch: 155 [200000/697932 (29%)]\tLoss: 160.655828\n",
      "Train Epoch: 155 [300000/697932 (43%)]\tLoss: 162.300172\n",
      "Train Epoch: 155 [400000/697932 (57%)]\tLoss: 164.167109\n",
      "Train Epoch: 155 [500000/697932 (72%)]\tLoss: 161.849641\n",
      "Train Epoch: 155 [600000/697932 (86%)]\tLoss: 163.357125\n",
      "====> Epoch: 155 Average loss: 163.4285\n",
      "====> Test set loss: 165.4280\n",
      "Train Epoch: 156 [0/697932 (0%)]\tLoss: 162.694016\n",
      "Train Epoch: 156 [100000/697932 (14%)]\tLoss: 161.639828\n",
      "Train Epoch: 156 [200000/697932 (29%)]\tLoss: 163.443109\n",
      "Train Epoch: 156 [300000/697932 (43%)]\tLoss: 161.880453\n",
      "Train Epoch: 156 [400000/697932 (57%)]\tLoss: 165.360234\n",
      "Train Epoch: 156 [500000/697932 (72%)]\tLoss: 162.230063\n",
      "Train Epoch: 156 [600000/697932 (86%)]\tLoss: 162.208063\n",
      "====> Epoch: 156 Average loss: 163.4690\n",
      "====> Test set loss: 165.5723\n",
      "Train Epoch: 157 [0/697932 (0%)]\tLoss: 163.031578\n",
      "Train Epoch: 157 [100000/697932 (14%)]\tLoss: 164.741562\n",
      "Train Epoch: 157 [200000/697932 (29%)]\tLoss: 163.720266\n",
      "Train Epoch: 157 [300000/697932 (43%)]\tLoss: 161.709313\n",
      "Train Epoch: 157 [400000/697932 (57%)]\tLoss: 166.470687\n",
      "Train Epoch: 157 [500000/697932 (72%)]\tLoss: 160.519594\n",
      "Train Epoch: 157 [600000/697932 (86%)]\tLoss: 162.621203\n",
      "====> Epoch: 157 Average loss: 163.4347\n",
      "====> Test set loss: 165.4712\n",
      "Train Epoch: 158 [0/697932 (0%)]\tLoss: 161.077000\n",
      "Train Epoch: 158 [100000/697932 (14%)]\tLoss: 163.949813\n",
      "Train Epoch: 158 [200000/697932 (29%)]\tLoss: 164.995844\n",
      "Train Epoch: 158 [300000/697932 (43%)]\tLoss: 164.197891\n",
      "Train Epoch: 158 [400000/697932 (57%)]\tLoss: 163.092953\n",
      "Train Epoch: 158 [500000/697932 (72%)]\tLoss: 162.983906\n",
      "Train Epoch: 158 [600000/697932 (86%)]\tLoss: 162.145625\n",
      "====> Epoch: 158 Average loss: 163.3873\n",
      "====> Test set loss: 165.5788\n",
      "Train Epoch: 159 [0/697932 (0%)]\tLoss: 163.157078\n",
      "Train Epoch: 159 [100000/697932 (14%)]\tLoss: 163.834937\n",
      "Train Epoch: 159 [200000/697932 (29%)]\tLoss: 162.152562\n",
      "Train Epoch: 159 [300000/697932 (43%)]\tLoss: 165.035906\n",
      "Train Epoch: 159 [400000/697932 (57%)]\tLoss: 163.902750\n",
      "Train Epoch: 159 [500000/697932 (72%)]\tLoss: 166.268328\n",
      "Train Epoch: 159 [600000/697932 (86%)]\tLoss: 161.591563\n",
      "====> Epoch: 159 Average loss: 163.3221\n",
      "====> Test set loss: 165.4192\n",
      "Train Epoch: 160 [0/697932 (0%)]\tLoss: 163.979344\n",
      "Train Epoch: 160 [100000/697932 (14%)]\tLoss: 160.540266\n",
      "Train Epoch: 160 [200000/697932 (29%)]\tLoss: 163.615641\n",
      "Train Epoch: 160 [300000/697932 (43%)]\tLoss: 163.527656\n",
      "Train Epoch: 160 [400000/697932 (57%)]\tLoss: 162.514578\n",
      "Train Epoch: 160 [500000/697932 (72%)]\tLoss: 163.325562\n",
      "Train Epoch: 160 [600000/697932 (86%)]\tLoss: 164.678078\n",
      "====> Epoch: 160 Average loss: 163.3521\n",
      "====> Test set loss: 165.4216\n",
      "Train Epoch: 161 [0/697932 (0%)]\tLoss: 163.240922\n",
      "Train Epoch: 161 [100000/697932 (14%)]\tLoss: 161.498984\n",
      "Train Epoch: 161 [200000/697932 (29%)]\tLoss: 164.036547\n",
      "Train Epoch: 161 [300000/697932 (43%)]\tLoss: 162.631313\n",
      "Train Epoch: 161 [400000/697932 (57%)]\tLoss: 162.102281\n",
      "Train Epoch: 161 [500000/697932 (72%)]\tLoss: 161.788984\n",
      "Train Epoch: 161 [600000/697932 (86%)]\tLoss: 164.380984\n",
      "====> Epoch: 161 Average loss: 163.3922\n",
      "====> Test set loss: 165.3411\n",
      "Train Epoch: 162 [0/697932 (0%)]\tLoss: 163.313031\n",
      "Train Epoch: 162 [100000/697932 (14%)]\tLoss: 163.526797\n",
      "Train Epoch: 162 [200000/697932 (29%)]\tLoss: 166.016547\n",
      "Train Epoch: 162 [300000/697932 (43%)]\tLoss: 165.001141\n",
      "Train Epoch: 162 [400000/697932 (57%)]\tLoss: 160.479875\n",
      "Train Epoch: 162 [500000/697932 (72%)]\tLoss: 163.169688\n",
      "Train Epoch: 162 [600000/697932 (86%)]\tLoss: 166.065016\n",
      "====> Epoch: 162 Average loss: 163.4057\n",
      "====> Test set loss: 165.4475\n",
      "Train Epoch: 163 [0/697932 (0%)]\tLoss: 164.012844\n",
      "Train Epoch: 163 [100000/697932 (14%)]\tLoss: 163.433656\n",
      "Train Epoch: 163 [200000/697932 (29%)]\tLoss: 165.315250\n",
      "Train Epoch: 163 [300000/697932 (43%)]\tLoss: 160.543047\n",
      "Train Epoch: 163 [400000/697932 (57%)]\tLoss: 165.511453\n",
      "Train Epoch: 163 [500000/697932 (72%)]\tLoss: 164.245281\n",
      "Train Epoch: 163 [600000/697932 (86%)]\tLoss: 165.339719\n",
      "====> Epoch: 163 Average loss: 163.2942\n",
      "====> Test set loss: 165.3308\n",
      "Train Epoch: 164 [0/697932 (0%)]\tLoss: 161.550453\n",
      "Train Epoch: 164 [100000/697932 (14%)]\tLoss: 163.065078\n",
      "Train Epoch: 164 [200000/697932 (29%)]\tLoss: 163.708281\n",
      "Train Epoch: 164 [300000/697932 (43%)]\tLoss: 164.266984\n",
      "Train Epoch: 164 [400000/697932 (57%)]\tLoss: 162.000094\n",
      "Train Epoch: 164 [500000/697932 (72%)]\tLoss: 162.776563\n",
      "Train Epoch: 164 [600000/697932 (86%)]\tLoss: 166.129281\n",
      "====> Epoch: 164 Average loss: 163.2990\n",
      "====> Test set loss: 165.5736\n",
      "Train Epoch: 165 [0/697932 (0%)]\tLoss: 161.409000\n",
      "Train Epoch: 165 [100000/697932 (14%)]\tLoss: 162.710984\n",
      "Train Epoch: 165 [200000/697932 (29%)]\tLoss: 165.428453\n",
      "Train Epoch: 165 [300000/697932 (43%)]\tLoss: 164.897969\n",
      "Train Epoch: 165 [400000/697932 (57%)]\tLoss: 163.645984\n",
      "Train Epoch: 165 [500000/697932 (72%)]\tLoss: 163.384156\n",
      "Train Epoch: 165 [600000/697932 (86%)]\tLoss: 162.522141\n",
      "====> Epoch: 165 Average loss: 163.3650\n",
      "====> Test set loss: 165.3382\n",
      "Train Epoch: 166 [0/697932 (0%)]\tLoss: 161.839469\n",
      "Train Epoch: 166 [100000/697932 (14%)]\tLoss: 162.625984\n",
      "Train Epoch: 166 [200000/697932 (29%)]\tLoss: 159.192625\n",
      "Train Epoch: 166 [300000/697932 (43%)]\tLoss: 162.294828\n",
      "Train Epoch: 166 [400000/697932 (57%)]\tLoss: 163.394781\n",
      "Train Epoch: 166 [500000/697932 (72%)]\tLoss: 161.891141\n",
      "Train Epoch: 166 [600000/697932 (86%)]\tLoss: 164.684687\n",
      "====> Epoch: 166 Average loss: 163.3022\n",
      "====> Test set loss: 165.3699\n",
      "Train Epoch: 167 [0/697932 (0%)]\tLoss: 163.093625\n",
      "Train Epoch: 167 [100000/697932 (14%)]\tLoss: 163.026141\n",
      "Train Epoch: 167 [200000/697932 (29%)]\tLoss: 165.090937\n",
      "Train Epoch: 167 [300000/697932 (43%)]\tLoss: 162.645813\n",
      "Train Epoch: 167 [400000/697932 (57%)]\tLoss: 164.860703\n",
      "Train Epoch: 167 [500000/697932 (72%)]\tLoss: 165.021062\n",
      "Train Epoch: 167 [600000/697932 (86%)]\tLoss: 161.861016\n",
      "====> Epoch: 167 Average loss: 163.2303\n",
      "====> Test set loss: 165.2362\n",
      "Train Epoch: 168 [0/697932 (0%)]\tLoss: 164.867047\n",
      "Train Epoch: 168 [100000/697932 (14%)]\tLoss: 161.780438\n",
      "Train Epoch: 168 [200000/697932 (29%)]\tLoss: 164.305953\n",
      "Train Epoch: 168 [300000/697932 (43%)]\tLoss: 165.473500\n",
      "Train Epoch: 168 [400000/697932 (57%)]\tLoss: 163.525516\n",
      "Train Epoch: 168 [500000/697932 (72%)]\tLoss: 162.531078\n",
      "Train Epoch: 168 [600000/697932 (86%)]\tLoss: 161.934859\n",
      "====> Epoch: 168 Average loss: 163.1824\n",
      "====> Test set loss: 165.4832\n",
      "Train Epoch: 169 [0/697932 (0%)]\tLoss: 160.955437\n",
      "Train Epoch: 169 [100000/697932 (14%)]\tLoss: 162.781516\n",
      "Train Epoch: 169 [200000/697932 (29%)]\tLoss: 160.981203\n",
      "Train Epoch: 169 [300000/697932 (43%)]\tLoss: 162.219094\n",
      "Train Epoch: 169 [400000/697932 (57%)]\tLoss: 168.057469\n",
      "Train Epoch: 169 [500000/697932 (72%)]\tLoss: 163.058828\n",
      "Train Epoch: 169 [600000/697932 (86%)]\tLoss: 161.265250\n",
      "====> Epoch: 169 Average loss: 163.2415\n",
      "====> Test set loss: 165.3339\n",
      "Train Epoch: 170 [0/697932 (0%)]\tLoss: 160.617234\n",
      "Train Epoch: 170 [100000/697932 (14%)]\tLoss: 165.809594\n",
      "Train Epoch: 170 [200000/697932 (29%)]\tLoss: 162.461109\n",
      "Train Epoch: 170 [300000/697932 (43%)]\tLoss: 159.923438\n",
      "Train Epoch: 170 [400000/697932 (57%)]\tLoss: 163.099906\n",
      "Train Epoch: 170 [500000/697932 (72%)]\tLoss: 160.048625\n",
      "Train Epoch: 170 [600000/697932 (86%)]\tLoss: 163.267297\n",
      "====> Epoch: 170 Average loss: 163.2182\n",
      "====> Test set loss: 165.5192\n",
      "Train Epoch: 171 [0/697932 (0%)]\tLoss: 163.913250\n",
      "Train Epoch: 171 [100000/697932 (14%)]\tLoss: 166.755531\n",
      "Train Epoch: 171 [200000/697932 (29%)]\tLoss: 164.596688\n",
      "Train Epoch: 171 [300000/697932 (43%)]\tLoss: 162.884062\n",
      "Train Epoch: 171 [400000/697932 (57%)]\tLoss: 165.299156\n",
      "Train Epoch: 171 [500000/697932 (72%)]\tLoss: 161.395312\n",
      "Train Epoch: 171 [600000/697932 (86%)]\tLoss: 164.634391\n",
      "====> Epoch: 171 Average loss: 163.2499\n",
      "====> Test set loss: 165.5258\n",
      "Train Epoch: 172 [0/697932 (0%)]\tLoss: 164.821734\n",
      "Train Epoch: 172 [100000/697932 (14%)]\tLoss: 163.638953\n",
      "Train Epoch: 172 [200000/697932 (29%)]\tLoss: 160.823203\n",
      "Train Epoch: 172 [300000/697932 (43%)]\tLoss: 162.326547\n",
      "Train Epoch: 172 [400000/697932 (57%)]\tLoss: 164.567422\n",
      "Train Epoch: 172 [500000/697932 (72%)]\tLoss: 163.359500\n",
      "Train Epoch: 172 [600000/697932 (86%)]\tLoss: 165.685344\n",
      "====> Epoch: 172 Average loss: 163.2254\n",
      "====> Test set loss: 165.4154\n",
      "Train Epoch: 173 [0/697932 (0%)]\tLoss: 162.884609\n",
      "Train Epoch: 173 [100000/697932 (14%)]\tLoss: 161.234906\n",
      "Train Epoch: 173 [200000/697932 (29%)]\tLoss: 165.187484\n",
      "Train Epoch: 173 [300000/697932 (43%)]\tLoss: 164.422594\n",
      "Train Epoch: 173 [400000/697932 (57%)]\tLoss: 163.040203\n",
      "Train Epoch: 173 [500000/697932 (72%)]\tLoss: 162.799922\n",
      "Train Epoch: 173 [600000/697932 (86%)]\tLoss: 165.026500\n",
      "====> Epoch: 173 Average loss: 163.2375\n",
      "====> Test set loss: 165.4591\n",
      "Train Epoch: 174 [0/697932 (0%)]\tLoss: 163.755422\n",
      "Train Epoch: 174 [100000/697932 (14%)]\tLoss: 163.037672\n",
      "Train Epoch: 174 [200000/697932 (29%)]\tLoss: 163.567266\n",
      "Train Epoch: 174 [300000/697932 (43%)]\tLoss: 162.543984\n",
      "Train Epoch: 174 [400000/697932 (57%)]\tLoss: 163.789422\n",
      "Train Epoch: 174 [500000/697932 (72%)]\tLoss: 163.075562\n",
      "Train Epoch: 174 [600000/697932 (86%)]\tLoss: 160.483141\n",
      "====> Epoch: 174 Average loss: 163.1974\n",
      "====> Test set loss: 165.4053\n",
      "Train Epoch: 175 [0/697932 (0%)]\tLoss: 162.819406\n",
      "Train Epoch: 175 [100000/697932 (14%)]\tLoss: 163.550578\n",
      "Train Epoch: 175 [200000/697932 (29%)]\tLoss: 163.266141\n",
      "Train Epoch: 175 [300000/697932 (43%)]\tLoss: 164.573656\n",
      "Train Epoch: 175 [400000/697932 (57%)]\tLoss: 163.660937\n",
      "Train Epoch: 175 [500000/697932 (72%)]\tLoss: 162.386141\n",
      "Train Epoch: 175 [600000/697932 (86%)]\tLoss: 165.822609\n",
      "====> Epoch: 175 Average loss: 163.1509\n",
      "====> Test set loss: 165.2296\n",
      "Train Epoch: 176 [0/697932 (0%)]\tLoss: 164.787703\n",
      "Train Epoch: 176 [100000/697932 (14%)]\tLoss: 163.029687\n",
      "Train Epoch: 176 [200000/697932 (29%)]\tLoss: 163.518703\n",
      "Train Epoch: 176 [300000/697932 (43%)]\tLoss: 161.155812\n",
      "Train Epoch: 176 [400000/697932 (57%)]\tLoss: 163.774531\n",
      "Train Epoch: 176 [500000/697932 (72%)]\tLoss: 161.179531\n",
      "Train Epoch: 176 [600000/697932 (86%)]\tLoss: 160.726906\n",
      "====> Epoch: 176 Average loss: 163.1990\n",
      "====> Test set loss: 165.3567\n",
      "Train Epoch: 177 [0/697932 (0%)]\tLoss: 162.162359\n",
      "Train Epoch: 177 [100000/697932 (14%)]\tLoss: 162.961031\n",
      "Train Epoch: 177 [200000/697932 (29%)]\tLoss: 162.653016\n",
      "Train Epoch: 177 [300000/697932 (43%)]\tLoss: 164.017906\n",
      "Train Epoch: 177 [400000/697932 (57%)]\tLoss: 163.009688\n",
      "Train Epoch: 177 [500000/697932 (72%)]\tLoss: 161.449281\n",
      "Train Epoch: 177 [600000/697932 (86%)]\tLoss: 164.741031\n",
      "====> Epoch: 177 Average loss: 163.1366\n",
      "====> Test set loss: 165.4568\n",
      "Train Epoch: 178 [0/697932 (0%)]\tLoss: 162.493094\n",
      "Train Epoch: 178 [100000/697932 (14%)]\tLoss: 163.683391\n",
      "Train Epoch: 178 [200000/697932 (29%)]\tLoss: 164.787672\n",
      "Train Epoch: 178 [300000/697932 (43%)]\tLoss: 161.870031\n",
      "Train Epoch: 178 [400000/697932 (57%)]\tLoss: 161.937547\n",
      "Train Epoch: 178 [500000/697932 (72%)]\tLoss: 158.974453\n",
      "Train Epoch: 178 [600000/697932 (86%)]\tLoss: 164.063547\n",
      "====> Epoch: 178 Average loss: 163.1756\n",
      "====> Test set loss: 165.2110\n",
      "Train Epoch: 179 [0/697932 (0%)]\tLoss: 163.938938\n",
      "Train Epoch: 179 [100000/697932 (14%)]\tLoss: 162.739328\n",
      "Train Epoch: 179 [200000/697932 (29%)]\tLoss: 159.572047\n",
      "Train Epoch: 179 [300000/697932 (43%)]\tLoss: 164.019516\n",
      "Train Epoch: 179 [400000/697932 (57%)]\tLoss: 164.868047\n",
      "Train Epoch: 179 [500000/697932 (72%)]\tLoss: 164.081453\n",
      "Train Epoch: 179 [600000/697932 (86%)]\tLoss: 162.013969\n",
      "====> Epoch: 179 Average loss: 163.0871\n",
      "====> Test set loss: 165.3691\n",
      "Train Epoch: 180 [0/697932 (0%)]\tLoss: 163.719328\n",
      "Train Epoch: 180 [100000/697932 (14%)]\tLoss: 162.284094\n",
      "Train Epoch: 180 [200000/697932 (29%)]\tLoss: 163.351641\n",
      "Train Epoch: 180 [300000/697932 (43%)]\tLoss: 161.510891\n",
      "Train Epoch: 180 [400000/697932 (57%)]\tLoss: 166.145609\n",
      "Train Epoch: 180 [500000/697932 (72%)]\tLoss: 165.041172\n",
      "Train Epoch: 180 [600000/697932 (86%)]\tLoss: 164.654609\n",
      "====> Epoch: 180 Average loss: 163.1322\n",
      "====> Test set loss: 165.4854\n",
      "Train Epoch: 181 [0/697932 (0%)]\tLoss: 162.167203\n",
      "Train Epoch: 181 [100000/697932 (14%)]\tLoss: 161.986125\n",
      "Train Epoch: 181 [200000/697932 (29%)]\tLoss: 160.726016\n",
      "Train Epoch: 181 [300000/697932 (43%)]\tLoss: 161.754969\n",
      "Train Epoch: 181 [400000/697932 (57%)]\tLoss: 163.822547\n",
      "Train Epoch: 181 [500000/697932 (72%)]\tLoss: 164.144094\n",
      "Train Epoch: 181 [600000/697932 (86%)]\tLoss: 162.854828\n",
      "====> Epoch: 181 Average loss: 163.1457\n",
      "====> Test set loss: 165.3546\n",
      "Train Epoch: 182 [0/697932 (0%)]\tLoss: 162.541297\n",
      "Train Epoch: 182 [100000/697932 (14%)]\tLoss: 165.954781\n",
      "Train Epoch: 182 [200000/697932 (29%)]\tLoss: 161.530250\n",
      "Train Epoch: 182 [300000/697932 (43%)]\tLoss: 163.724203\n",
      "Train Epoch: 182 [400000/697932 (57%)]\tLoss: 163.263969\n",
      "Train Epoch: 182 [500000/697932 (72%)]\tLoss: 163.358438\n",
      "Train Epoch: 182 [600000/697932 (86%)]\tLoss: 164.471688\n",
      "====> Epoch: 182 Average loss: 163.1437\n",
      "====> Test set loss: 165.3975\n",
      "Train Epoch: 183 [0/697932 (0%)]\tLoss: 164.445687\n",
      "Train Epoch: 183 [100000/697932 (14%)]\tLoss: 164.560500\n",
      "Train Epoch: 183 [200000/697932 (29%)]\tLoss: 162.713313\n",
      "Train Epoch: 183 [300000/697932 (43%)]\tLoss: 163.566062\n",
      "Train Epoch: 183 [400000/697932 (57%)]\tLoss: 160.923937\n",
      "Train Epoch: 183 [500000/697932 (72%)]\tLoss: 164.654812\n",
      "Train Epoch: 183 [600000/697932 (86%)]\tLoss: 163.046641\n",
      "====> Epoch: 183 Average loss: 163.1478\n",
      "====> Test set loss: 165.5604\n",
      "Train Epoch: 184 [0/697932 (0%)]\tLoss: 161.298062\n",
      "Train Epoch: 184 [100000/697932 (14%)]\tLoss: 162.836813\n",
      "Train Epoch: 184 [200000/697932 (29%)]\tLoss: 163.422734\n",
      "Train Epoch: 184 [300000/697932 (43%)]\tLoss: 165.095063\n",
      "Train Epoch: 184 [400000/697932 (57%)]\tLoss: 163.681641\n",
      "Train Epoch: 184 [500000/697932 (72%)]\tLoss: 163.432469\n",
      "Train Epoch: 184 [600000/697932 (86%)]\tLoss: 164.657062\n",
      "====> Epoch: 184 Average loss: 163.0938\n",
      "====> Test set loss: 165.3455\n",
      "Train Epoch: 185 [0/697932 (0%)]\tLoss: 165.523687\n",
      "Train Epoch: 185 [100000/697932 (14%)]\tLoss: 163.368641\n",
      "Train Epoch: 185 [200000/697932 (29%)]\tLoss: 166.136109\n",
      "Train Epoch: 185 [300000/697932 (43%)]\tLoss: 163.201641\n",
      "Train Epoch: 185 [400000/697932 (57%)]\tLoss: 163.380234\n",
      "Train Epoch: 185 [500000/697932 (72%)]\tLoss: 163.411453\n",
      "Train Epoch: 185 [600000/697932 (86%)]\tLoss: 163.445109\n",
      "====> Epoch: 185 Average loss: 163.1029\n",
      "====> Test set loss: 165.2409\n",
      "Train Epoch: 186 [0/697932 (0%)]\tLoss: 160.232438\n",
      "Train Epoch: 186 [100000/697932 (14%)]\tLoss: 162.592828\n",
      "Train Epoch: 186 [200000/697932 (29%)]\tLoss: 162.816328\n",
      "Train Epoch: 186 [300000/697932 (43%)]\tLoss: 161.189016\n",
      "Train Epoch: 186 [400000/697932 (57%)]\tLoss: 163.871609\n",
      "Train Epoch: 186 [500000/697932 (72%)]\tLoss: 160.698156\n",
      "Train Epoch: 186 [600000/697932 (86%)]\tLoss: 163.861656\n",
      "====> Epoch: 186 Average loss: 163.0889\n",
      "====> Test set loss: 165.2605\n",
      "Train Epoch: 187 [0/697932 (0%)]\tLoss: 162.385406\n",
      "Train Epoch: 187 [100000/697932 (14%)]\tLoss: 165.415109\n",
      "Train Epoch: 187 [200000/697932 (29%)]\tLoss: 163.269297\n",
      "Train Epoch: 187 [300000/697932 (43%)]\tLoss: 162.630344\n",
      "Train Epoch: 187 [400000/697932 (57%)]\tLoss: 163.338922\n",
      "Train Epoch: 187 [500000/697932 (72%)]\tLoss: 163.599609\n",
      "Train Epoch: 187 [600000/697932 (86%)]\tLoss: 163.076500\n",
      "====> Epoch: 187 Average loss: 163.1419\n",
      "====> Test set loss: 165.3260\n",
      "Train Epoch: 188 [0/697932 (0%)]\tLoss: 162.785359\n",
      "Train Epoch: 188 [100000/697932 (14%)]\tLoss: 162.297297\n",
      "Train Epoch: 188 [200000/697932 (29%)]\tLoss: 161.177281\n",
      "Train Epoch: 188 [300000/697932 (43%)]\tLoss: 162.171750\n",
      "Train Epoch: 188 [400000/697932 (57%)]\tLoss: 162.034906\n",
      "Train Epoch: 188 [500000/697932 (72%)]\tLoss: 164.531187\n",
      "Train Epoch: 188 [600000/697932 (86%)]\tLoss: 161.676047\n",
      "====> Epoch: 188 Average loss: 163.0775\n",
      "====> Test set loss: 165.2959\n",
      "Train Epoch: 189 [0/697932 (0%)]\tLoss: 160.936500\n",
      "Train Epoch: 189 [100000/697932 (14%)]\tLoss: 165.152609\n",
      "Train Epoch: 189 [200000/697932 (29%)]\tLoss: 162.632016\n",
      "Train Epoch: 189 [300000/697932 (43%)]\tLoss: 165.494000\n",
      "Train Epoch: 189 [400000/697932 (57%)]\tLoss: 163.393781\n",
      "Train Epoch: 189 [500000/697932 (72%)]\tLoss: 163.630578\n",
      "Train Epoch: 189 [600000/697932 (86%)]\tLoss: 160.053672\n",
      "====> Epoch: 189 Average loss: 162.9827\n",
      "====> Test set loss: 165.1450\n",
      "Train Epoch: 190 [0/697932 (0%)]\tLoss: 163.048781\n",
      "Train Epoch: 190 [100000/697932 (14%)]\tLoss: 163.934937\n",
      "Train Epoch: 190 [200000/697932 (29%)]\tLoss: 163.532672\n",
      "Train Epoch: 190 [300000/697932 (43%)]\tLoss: 161.786000\n",
      "Train Epoch: 190 [400000/697932 (57%)]\tLoss: 163.061359\n",
      "Train Epoch: 190 [500000/697932 (72%)]\tLoss: 163.475969\n",
      "Train Epoch: 190 [600000/697932 (86%)]\tLoss: 161.234500\n",
      "====> Epoch: 190 Average loss: 163.0199\n",
      "====> Test set loss: 165.2616\n",
      "Train Epoch: 191 [0/697932 (0%)]\tLoss: 162.711734\n",
      "Train Epoch: 191 [100000/697932 (14%)]\tLoss: 162.337187\n",
      "Train Epoch: 191 [200000/697932 (29%)]\tLoss: 162.642406\n",
      "Train Epoch: 191 [300000/697932 (43%)]\tLoss: 165.215734\n",
      "Train Epoch: 191 [400000/697932 (57%)]\tLoss: 163.533859\n",
      "Train Epoch: 191 [500000/697932 (72%)]\tLoss: 162.949844\n",
      "Train Epoch: 191 [600000/697932 (86%)]\tLoss: 163.792641\n",
      "====> Epoch: 191 Average loss: 163.0107\n",
      "====> Test set loss: 165.1966\n",
      "Train Epoch: 192 [0/697932 (0%)]\tLoss: 165.759484\n",
      "Train Epoch: 192 [100000/697932 (14%)]\tLoss: 160.904297\n",
      "Train Epoch: 192 [200000/697932 (29%)]\tLoss: 164.965016\n",
      "Train Epoch: 192 [300000/697932 (43%)]\tLoss: 161.271047\n",
      "Train Epoch: 192 [400000/697932 (57%)]\tLoss: 165.817266\n",
      "Train Epoch: 192 [500000/697932 (72%)]\tLoss: 163.252625\n",
      "Train Epoch: 192 [600000/697932 (86%)]\tLoss: 163.847109\n",
      "====> Epoch: 192 Average loss: 163.0427\n",
      "====> Test set loss: 165.4996\n",
      "Train Epoch: 193 [0/697932 (0%)]\tLoss: 163.721625\n",
      "Train Epoch: 193 [100000/697932 (14%)]\tLoss: 160.732000\n",
      "Train Epoch: 193 [200000/697932 (29%)]\tLoss: 164.187969\n",
      "Train Epoch: 193 [300000/697932 (43%)]\tLoss: 163.346188\n",
      "Train Epoch: 193 [400000/697932 (57%)]\tLoss: 163.558766\n",
      "Train Epoch: 193 [500000/697932 (72%)]\tLoss: 165.217469\n",
      "Train Epoch: 193 [600000/697932 (86%)]\tLoss: 160.788328\n",
      "====> Epoch: 193 Average loss: 163.0373\n",
      "====> Test set loss: 165.4351\n",
      "Train Epoch: 194 [0/697932 (0%)]\tLoss: 160.701234\n",
      "Train Epoch: 194 [100000/697932 (14%)]\tLoss: 163.394328\n",
      "Train Epoch: 194 [200000/697932 (29%)]\tLoss: 162.095594\n",
      "Train Epoch: 194 [300000/697932 (43%)]\tLoss: 162.568187\n",
      "Train Epoch: 194 [400000/697932 (57%)]\tLoss: 165.383016\n",
      "Train Epoch: 194 [500000/697932 (72%)]\tLoss: 165.230406\n",
      "Train Epoch: 194 [600000/697932 (86%)]\tLoss: 165.948297\n",
      "====> Epoch: 194 Average loss: 162.9966\n",
      "====> Test set loss: 165.0415\n",
      "Train Epoch: 195 [0/697932 (0%)]\tLoss: 162.768219\n",
      "Train Epoch: 195 [100000/697932 (14%)]\tLoss: 162.321766\n",
      "Train Epoch: 195 [200000/697932 (29%)]\tLoss: 164.553094\n",
      "Train Epoch: 195 [300000/697932 (43%)]\tLoss: 163.015734\n",
      "Train Epoch: 195 [400000/697932 (57%)]\tLoss: 162.258187\n",
      "Train Epoch: 195 [500000/697932 (72%)]\tLoss: 162.345188\n",
      "Train Epoch: 195 [600000/697932 (86%)]\tLoss: 164.200797\n",
      "====> Epoch: 195 Average loss: 162.9571\n",
      "====> Test set loss: 165.3162\n",
      "Train Epoch: 196 [0/697932 (0%)]\tLoss: 165.796922\n",
      "Train Epoch: 196 [100000/697932 (14%)]\tLoss: 163.016687\n",
      "Train Epoch: 196 [200000/697932 (29%)]\tLoss: 163.767047\n",
      "Train Epoch: 196 [300000/697932 (43%)]\tLoss: 163.649234\n",
      "Train Epoch: 196 [400000/697932 (57%)]\tLoss: 165.128359\n",
      "Train Epoch: 196 [500000/697932 (72%)]\tLoss: 163.352234\n",
      "Train Epoch: 196 [600000/697932 (86%)]\tLoss: 163.244000\n",
      "====> Epoch: 196 Average loss: 162.8988\n",
      "====> Test set loss: 165.3859\n",
      "Train Epoch: 197 [0/697932 (0%)]\tLoss: 164.295219\n",
      "Train Epoch: 197 [100000/697932 (14%)]\tLoss: 165.487594\n",
      "Train Epoch: 197 [200000/697932 (29%)]\tLoss: 165.732344\n",
      "Train Epoch: 197 [300000/697932 (43%)]\tLoss: 162.717344\n",
      "Train Epoch: 197 [400000/697932 (57%)]\tLoss: 163.230531\n",
      "Train Epoch: 197 [500000/697932 (72%)]\tLoss: 162.712906\n",
      "Train Epoch: 197 [600000/697932 (86%)]\tLoss: 164.326031\n",
      "====> Epoch: 197 Average loss: 162.9818\n",
      "====> Test set loss: 165.2199\n",
      "Train Epoch: 198 [0/697932 (0%)]\tLoss: 162.348516\n",
      "Train Epoch: 198 [100000/697932 (14%)]\tLoss: 163.158984\n",
      "Train Epoch: 198 [200000/697932 (29%)]\tLoss: 164.582219\n",
      "Train Epoch: 198 [300000/697932 (43%)]\tLoss: 161.503656\n",
      "Train Epoch: 198 [400000/697932 (57%)]\tLoss: 163.215688\n",
      "Train Epoch: 198 [500000/697932 (72%)]\tLoss: 165.121344\n",
      "Train Epoch: 198 [600000/697932 (86%)]\tLoss: 164.223000\n",
      "====> Epoch: 198 Average loss: 162.9925\n",
      "====> Test set loss: 165.3458\n",
      "Train Epoch: 199 [0/697932 (0%)]\tLoss: 162.780219\n",
      "Train Epoch: 199 [100000/697932 (14%)]\tLoss: 164.374109\n",
      "Train Epoch: 199 [200000/697932 (29%)]\tLoss: 162.880172\n",
      "Train Epoch: 199 [300000/697932 (43%)]\tLoss: 163.360250\n",
      "Train Epoch: 199 [400000/697932 (57%)]\tLoss: 162.716906\n",
      "Train Epoch: 199 [500000/697932 (72%)]\tLoss: 161.075172\n",
      "Train Epoch: 199 [600000/697932 (86%)]\tLoss: 158.455062\n",
      "====> Epoch: 199 Average loss: 162.8764\n",
      "====> Test set loss: 165.3224\n",
      "Train Epoch: 200 [0/697932 (0%)]\tLoss: 163.000484\n",
      "Train Epoch: 200 [100000/697932 (14%)]\tLoss: 162.593328\n",
      "Train Epoch: 200 [200000/697932 (29%)]\tLoss: 162.473703\n",
      "Train Epoch: 200 [300000/697932 (43%)]\tLoss: 164.852156\n",
      "Train Epoch: 200 [400000/697932 (57%)]\tLoss: 162.334438\n",
      "Train Epoch: 200 [500000/697932 (72%)]\tLoss: 162.158094\n",
      "Train Epoch: 200 [600000/697932 (86%)]\tLoss: 165.366437\n",
      "====> Epoch: 200 Average loss: 162.9547\n",
      "====> Test set loss: 165.2400\n",
      "Train Epoch: 201 [0/697932 (0%)]\tLoss: 160.552219\n",
      "Train Epoch: 201 [100000/697932 (14%)]\tLoss: 160.759109\n",
      "Train Epoch: 201 [200000/697932 (29%)]\tLoss: 161.740172\n",
      "Train Epoch: 201 [300000/697932 (43%)]\tLoss: 162.677172\n",
      "Train Epoch: 201 [400000/697932 (57%)]\tLoss: 162.527406\n",
      "Train Epoch: 201 [500000/697932 (72%)]\tLoss: 163.072953\n",
      "Train Epoch: 201 [600000/697932 (86%)]\tLoss: 164.712859\n",
      "====> Epoch: 201 Average loss: 162.9696\n",
      "====> Test set loss: 165.3262\n",
      "Train Epoch: 202 [0/697932 (0%)]\tLoss: 165.939188\n",
      "Train Epoch: 202 [100000/697932 (14%)]\tLoss: 163.034312\n",
      "Train Epoch: 202 [200000/697932 (29%)]\tLoss: 164.771234\n",
      "Train Epoch: 202 [300000/697932 (43%)]\tLoss: 162.516375\n",
      "Train Epoch: 202 [400000/697932 (57%)]\tLoss: 163.333500\n",
      "Train Epoch: 202 [500000/697932 (72%)]\tLoss: 160.771172\n",
      "Train Epoch: 202 [600000/697932 (86%)]\tLoss: 162.081531\n",
      "====> Epoch: 202 Average loss: 162.9272\n",
      "====> Test set loss: 165.2512\n",
      "Train Epoch: 203 [0/697932 (0%)]\tLoss: 163.117312\n",
      "Train Epoch: 203 [100000/697932 (14%)]\tLoss: 159.808672\n",
      "Train Epoch: 203 [200000/697932 (29%)]\tLoss: 161.609484\n",
      "Train Epoch: 203 [300000/697932 (43%)]\tLoss: 164.317219\n",
      "Train Epoch: 203 [400000/697932 (57%)]\tLoss: 162.678453\n",
      "Train Epoch: 203 [500000/697932 (72%)]\tLoss: 163.180078\n",
      "Train Epoch: 203 [600000/697932 (86%)]\tLoss: 163.617547\n",
      "====> Epoch: 203 Average loss: 162.9417\n",
      "====> Test set loss: 165.0432\n",
      "Train Epoch: 204 [0/697932 (0%)]\tLoss: 162.219406\n",
      "Train Epoch: 204 [100000/697932 (14%)]\tLoss: 162.032969\n",
      "Train Epoch: 204 [200000/697932 (29%)]\tLoss: 162.113812\n",
      "Train Epoch: 204 [300000/697932 (43%)]\tLoss: 163.253781\n",
      "Train Epoch: 204 [400000/697932 (57%)]\tLoss: 163.799625\n",
      "Train Epoch: 204 [500000/697932 (72%)]\tLoss: 160.611437\n",
      "Train Epoch: 204 [600000/697932 (86%)]\tLoss: 164.509266\n",
      "====> Epoch: 204 Average loss: 162.8802\n",
      "====> Test set loss: 165.2626\n",
      "Train Epoch: 205 [0/697932 (0%)]\tLoss: 162.251563\n",
      "Train Epoch: 205 [100000/697932 (14%)]\tLoss: 163.292375\n",
      "Train Epoch: 205 [200000/697932 (29%)]\tLoss: 161.183891\n",
      "Train Epoch: 205 [300000/697932 (43%)]\tLoss: 162.965203\n",
      "Train Epoch: 205 [400000/697932 (57%)]\tLoss: 163.989094\n",
      "Train Epoch: 205 [500000/697932 (72%)]\tLoss: 162.362828\n",
      "Train Epoch: 205 [600000/697932 (86%)]\tLoss: 162.650797\n",
      "====> Epoch: 205 Average loss: 162.9277\n",
      "====> Test set loss: 165.2837\n",
      "Train Epoch: 206 [0/697932 (0%)]\tLoss: 163.231719\n",
      "Train Epoch: 206 [100000/697932 (14%)]\tLoss: 164.087656\n",
      "Train Epoch: 206 [200000/697932 (29%)]\tLoss: 164.545516\n",
      "Train Epoch: 206 [300000/697932 (43%)]\tLoss: 162.773953\n",
      "Train Epoch: 206 [400000/697932 (57%)]\tLoss: 164.738328\n",
      "Train Epoch: 206 [500000/697932 (72%)]\tLoss: 162.394328\n",
      "Train Epoch: 206 [600000/697932 (86%)]\tLoss: 161.194063\n",
      "====> Epoch: 206 Average loss: 162.9423\n",
      "====> Test set loss: 165.2117\n",
      "Train Epoch: 207 [0/697932 (0%)]\tLoss: 157.995375\n",
      "Train Epoch: 207 [100000/697932 (14%)]\tLoss: 162.237875\n",
      "Train Epoch: 207 [200000/697932 (29%)]\tLoss: 160.846578\n",
      "Train Epoch: 207 [300000/697932 (43%)]\tLoss: 162.810219\n",
      "Train Epoch: 207 [400000/697932 (57%)]\tLoss: 167.305922\n",
      "Train Epoch: 207 [500000/697932 (72%)]\tLoss: 162.887719\n",
      "Train Epoch: 207 [600000/697932 (86%)]\tLoss: 165.312594\n",
      "====> Epoch: 207 Average loss: 162.9388\n",
      "====> Test set loss: 164.9552\n",
      "Train Epoch: 208 [0/697932 (0%)]\tLoss: 163.307422\n",
      "Train Epoch: 208 [100000/697932 (14%)]\tLoss: 161.783859\n",
      "Train Epoch: 208 [200000/697932 (29%)]\tLoss: 160.825578\n",
      "Train Epoch: 208 [300000/697932 (43%)]\tLoss: 163.521672\n",
      "Train Epoch: 208 [400000/697932 (57%)]\tLoss: 161.310125\n",
      "Train Epoch: 208 [500000/697932 (72%)]\tLoss: 165.232750\n",
      "Train Epoch: 208 [600000/697932 (86%)]\tLoss: 162.242281\n",
      "====> Epoch: 208 Average loss: 162.8105\n",
      "====> Test set loss: 165.1894\n",
      "Train Epoch: 209 [0/697932 (0%)]\tLoss: 157.519969\n",
      "Train Epoch: 209 [100000/697932 (14%)]\tLoss: 163.356031\n",
      "Train Epoch: 209 [200000/697932 (29%)]\tLoss: 164.370078\n",
      "Train Epoch: 209 [300000/697932 (43%)]\tLoss: 163.044891\n",
      "Train Epoch: 209 [400000/697932 (57%)]\tLoss: 163.592609\n",
      "Train Epoch: 209 [500000/697932 (72%)]\tLoss: 163.411469\n",
      "Train Epoch: 209 [600000/697932 (86%)]\tLoss: 165.205594\n",
      "====> Epoch: 209 Average loss: 162.8471\n",
      "====> Test set loss: 164.9546\n",
      "Train Epoch: 210 [0/697932 (0%)]\tLoss: 163.656313\n",
      "Train Epoch: 210 [100000/697932 (14%)]\tLoss: 165.672500\n",
      "Train Epoch: 210 [200000/697932 (29%)]\tLoss: 161.941531\n",
      "Train Epoch: 210 [300000/697932 (43%)]\tLoss: 160.132609\n",
      "Train Epoch: 210 [400000/697932 (57%)]\tLoss: 163.501094\n",
      "Train Epoch: 210 [500000/697932 (72%)]\tLoss: 167.130469\n",
      "Train Epoch: 210 [600000/697932 (86%)]\tLoss: 162.307172\n",
      "====> Epoch: 210 Average loss: 162.8263\n",
      "====> Test set loss: 165.1848\n",
      "Train Epoch: 211 [0/697932 (0%)]\tLoss: 162.965875\n",
      "Train Epoch: 211 [100000/697932 (14%)]\tLoss: 161.590469\n",
      "Train Epoch: 211 [200000/697932 (29%)]\tLoss: 164.800172\n",
      "Train Epoch: 211 [300000/697932 (43%)]\tLoss: 160.253609\n",
      "Train Epoch: 211 [400000/697932 (57%)]\tLoss: 163.864891\n",
      "Train Epoch: 211 [500000/697932 (72%)]\tLoss: 162.476422\n",
      "Train Epoch: 211 [600000/697932 (86%)]\tLoss: 163.327953\n",
      "====> Epoch: 211 Average loss: 162.9006\n",
      "====> Test set loss: 165.2207\n",
      "Train Epoch: 212 [0/697932 (0%)]\tLoss: 164.426750\n",
      "Train Epoch: 212 [100000/697932 (14%)]\tLoss: 162.147953\n",
      "Train Epoch: 212 [200000/697932 (29%)]\tLoss: 163.525859\n",
      "Train Epoch: 212 [300000/697932 (43%)]\tLoss: 166.702687\n",
      "Train Epoch: 212 [400000/697932 (57%)]\tLoss: 166.821516\n",
      "Train Epoch: 212 [500000/697932 (72%)]\tLoss: 164.133172\n",
      "Train Epoch: 212 [600000/697932 (86%)]\tLoss: 163.353906\n",
      "====> Epoch: 212 Average loss: 162.7882\n",
      "====> Test set loss: 165.0467\n",
      "Train Epoch: 213 [0/697932 (0%)]\tLoss: 163.818203\n",
      "Train Epoch: 213 [100000/697932 (14%)]\tLoss: 161.523484\n",
      "Train Epoch: 213 [200000/697932 (29%)]\tLoss: 163.956531\n",
      "Train Epoch: 213 [300000/697932 (43%)]\tLoss: 160.725844\n",
      "Train Epoch: 213 [400000/697932 (57%)]\tLoss: 160.247234\n",
      "Train Epoch: 213 [500000/697932 (72%)]\tLoss: 161.498437\n",
      "Train Epoch: 213 [600000/697932 (86%)]\tLoss: 166.360391\n",
      "====> Epoch: 213 Average loss: 162.7839\n",
      "====> Test set loss: 165.1464\n",
      "Train Epoch: 214 [0/697932 (0%)]\tLoss: 163.795703\n",
      "Train Epoch: 214 [100000/697932 (14%)]\tLoss: 161.041328\n",
      "Train Epoch: 214 [200000/697932 (29%)]\tLoss: 162.759531\n",
      "Train Epoch: 214 [300000/697932 (43%)]\tLoss: 165.464703\n",
      "Train Epoch: 214 [400000/697932 (57%)]\tLoss: 161.254141\n",
      "Train Epoch: 214 [500000/697932 (72%)]\tLoss: 165.174297\n",
      "Train Epoch: 214 [600000/697932 (86%)]\tLoss: 162.968609\n",
      "====> Epoch: 214 Average loss: 162.7875\n",
      "====> Test set loss: 165.0345\n",
      "Train Epoch: 215 [0/697932 (0%)]\tLoss: 161.621688\n",
      "Train Epoch: 215 [100000/697932 (14%)]\tLoss: 163.178937\n",
      "Train Epoch: 215 [200000/697932 (29%)]\tLoss: 166.928313\n",
      "Train Epoch: 215 [300000/697932 (43%)]\tLoss: 160.957562\n",
      "Train Epoch: 215 [400000/697932 (57%)]\tLoss: 161.900656\n",
      "Train Epoch: 215 [500000/697932 (72%)]\tLoss: 164.683250\n",
      "Train Epoch: 215 [600000/697932 (86%)]\tLoss: 163.557219\n",
      "====> Epoch: 215 Average loss: 162.7569\n",
      "====> Test set loss: 165.0210\n",
      "Train Epoch: 216 [0/697932 (0%)]\tLoss: 161.263641\n",
      "Train Epoch: 216 [100000/697932 (14%)]\tLoss: 165.031000\n",
      "Train Epoch: 216 [200000/697932 (29%)]\tLoss: 161.713031\n",
      "Train Epoch: 216 [300000/697932 (43%)]\tLoss: 161.267937\n",
      "Train Epoch: 216 [400000/697932 (57%)]\tLoss: 164.080406\n",
      "Train Epoch: 216 [500000/697932 (72%)]\tLoss: 161.482687\n",
      "Train Epoch: 216 [600000/697932 (86%)]\tLoss: 163.064984\n",
      "====> Epoch: 216 Average loss: 162.7867\n",
      "====> Test set loss: 165.0551\n",
      "Train Epoch: 217 [0/697932 (0%)]\tLoss: 161.529937\n",
      "Train Epoch: 217 [100000/697932 (14%)]\tLoss: 162.399156\n",
      "Train Epoch: 217 [200000/697932 (29%)]\tLoss: 162.746813\n",
      "Train Epoch: 217 [300000/697932 (43%)]\tLoss: 161.345453\n",
      "Train Epoch: 217 [400000/697932 (57%)]\tLoss: 166.507937\n",
      "Train Epoch: 217 [500000/697932 (72%)]\tLoss: 161.259594\n",
      "Train Epoch: 217 [600000/697932 (86%)]\tLoss: 164.426063\n",
      "====> Epoch: 217 Average loss: 162.8053\n",
      "====> Test set loss: 165.1257\n",
      "Train Epoch: 218 [0/697932 (0%)]\tLoss: 164.251922\n",
      "Train Epoch: 218 [100000/697932 (14%)]\tLoss: 165.131219\n",
      "Train Epoch: 218 [200000/697932 (29%)]\tLoss: 160.082484\n",
      "Train Epoch: 218 [300000/697932 (43%)]\tLoss: 163.005188\n",
      "Train Epoch: 218 [400000/697932 (57%)]\tLoss: 163.303812\n",
      "Train Epoch: 218 [500000/697932 (72%)]\tLoss: 162.039781\n",
      "Train Epoch: 218 [600000/697932 (86%)]\tLoss: 161.193672\n",
      "====> Epoch: 218 Average loss: 162.8065\n",
      "====> Test set loss: 165.1057\n",
      "Train Epoch: 219 [0/697932 (0%)]\tLoss: 161.111281\n",
      "Train Epoch: 219 [100000/697932 (14%)]\tLoss: 160.624656\n",
      "Train Epoch: 219 [200000/697932 (29%)]\tLoss: 162.569547\n",
      "Train Epoch: 219 [300000/697932 (43%)]\tLoss: 163.341375\n",
      "Train Epoch: 219 [400000/697932 (57%)]\tLoss: 161.310125\n",
      "Train Epoch: 219 [500000/697932 (72%)]\tLoss: 164.788344\n",
      "Train Epoch: 219 [600000/697932 (86%)]\tLoss: 159.794234\n",
      "====> Epoch: 219 Average loss: 162.7811\n",
      "====> Test set loss: 165.3670\n",
      "Train Epoch: 220 [0/697932 (0%)]\tLoss: 163.517656\n",
      "Train Epoch: 220 [100000/697932 (14%)]\tLoss: 164.031000\n",
      "Train Epoch: 220 [200000/697932 (29%)]\tLoss: 163.492422\n",
      "Train Epoch: 220 [300000/697932 (43%)]\tLoss: 163.913312\n",
      "Train Epoch: 220 [400000/697932 (57%)]\tLoss: 158.860719\n",
      "Train Epoch: 220 [500000/697932 (72%)]\tLoss: 163.405859\n",
      "Train Epoch: 220 [600000/697932 (86%)]\tLoss: 162.626187\n",
      "====> Epoch: 220 Average loss: 162.8563\n",
      "====> Test set loss: 165.3083\n",
      "Train Epoch: 221 [0/697932 (0%)]\tLoss: 161.596344\n",
      "Train Epoch: 221 [100000/697932 (14%)]\tLoss: 162.148438\n",
      "Train Epoch: 221 [200000/697932 (29%)]\tLoss: 161.865312\n",
      "Train Epoch: 221 [300000/697932 (43%)]\tLoss: 162.555359\n",
      "Train Epoch: 221 [400000/697932 (57%)]\tLoss: 162.987422\n",
      "Train Epoch: 221 [500000/697932 (72%)]\tLoss: 163.913078\n",
      "Train Epoch: 221 [600000/697932 (86%)]\tLoss: 163.330312\n",
      "====> Epoch: 221 Average loss: 162.7861\n",
      "====> Test set loss: 165.0471\n",
      "Train Epoch: 222 [0/697932 (0%)]\tLoss: 164.597438\n",
      "Train Epoch: 222 [100000/697932 (14%)]\tLoss: 165.837047\n",
      "Train Epoch: 222 [200000/697932 (29%)]\tLoss: 165.266641\n",
      "Train Epoch: 222 [300000/697932 (43%)]\tLoss: 163.519187\n",
      "Train Epoch: 222 [400000/697932 (57%)]\tLoss: 161.115938\n",
      "Train Epoch: 222 [500000/697932 (72%)]\tLoss: 163.025500\n",
      "Train Epoch: 222 [600000/697932 (86%)]\tLoss: 161.773047\n",
      "====> Epoch: 222 Average loss: 162.7305\n",
      "====> Test set loss: 164.9937\n",
      "Train Epoch: 223 [0/697932 (0%)]\tLoss: 163.822937\n",
      "Train Epoch: 223 [100000/697932 (14%)]\tLoss: 159.233828\n",
      "Train Epoch: 223 [200000/697932 (29%)]\tLoss: 164.301703\n",
      "Train Epoch: 223 [300000/697932 (43%)]\tLoss: 165.416328\n",
      "Train Epoch: 223 [400000/697932 (57%)]\tLoss: 160.955641\n",
      "Train Epoch: 223 [500000/697932 (72%)]\tLoss: 163.744063\n",
      "Train Epoch: 223 [600000/697932 (86%)]\tLoss: 164.729187\n",
      "====> Epoch: 223 Average loss: 162.7316\n",
      "====> Test set loss: 165.0056\n",
      "Train Epoch: 224 [0/697932 (0%)]\tLoss: 161.920047\n",
      "Train Epoch: 224 [100000/697932 (14%)]\tLoss: 161.724422\n",
      "Train Epoch: 224 [200000/697932 (29%)]\tLoss: 164.858937\n",
      "Train Epoch: 224 [300000/697932 (43%)]\tLoss: 163.519859\n",
      "Train Epoch: 224 [400000/697932 (57%)]\tLoss: 161.192781\n",
      "Train Epoch: 224 [500000/697932 (72%)]\tLoss: 165.000406\n",
      "Train Epoch: 224 [600000/697932 (86%)]\tLoss: 163.962859\n",
      "====> Epoch: 224 Average loss: 162.7621\n",
      "====> Test set loss: 165.1003\n",
      "Train Epoch: 225 [0/697932 (0%)]\tLoss: 160.838391\n",
      "Train Epoch: 225 [100000/697932 (14%)]\tLoss: 161.029484\n",
      "Train Epoch: 225 [200000/697932 (29%)]\tLoss: 160.144750\n",
      "Train Epoch: 225 [300000/697932 (43%)]\tLoss: 161.302484\n",
      "Train Epoch: 225 [400000/697932 (57%)]\tLoss: 164.087906\n",
      "Train Epoch: 225 [500000/697932 (72%)]\tLoss: 161.228578\n",
      "Train Epoch: 225 [600000/697932 (86%)]\tLoss: 162.678109\n",
      "====> Epoch: 225 Average loss: 162.8158\n",
      "====> Test set loss: 165.0823\n",
      "Train Epoch: 226 [0/697932 (0%)]\tLoss: 163.084422\n",
      "Train Epoch: 226 [100000/697932 (14%)]\tLoss: 162.101141\n",
      "Train Epoch: 226 [200000/697932 (29%)]\tLoss: 163.421656\n",
      "Train Epoch: 226 [300000/697932 (43%)]\tLoss: 164.535578\n",
      "Train Epoch: 226 [400000/697932 (57%)]\tLoss: 162.267188\n",
      "Train Epoch: 226 [500000/697932 (72%)]\tLoss: 165.080391\n",
      "Train Epoch: 226 [600000/697932 (86%)]\tLoss: 165.538594\n",
      "====> Epoch: 226 Average loss: 162.7937\n",
      "====> Test set loss: 165.0597\n",
      "Train Epoch: 227 [0/697932 (0%)]\tLoss: 162.777375\n",
      "Train Epoch: 227 [100000/697932 (14%)]\tLoss: 163.875547\n",
      "Train Epoch: 227 [200000/697932 (29%)]\tLoss: 163.771484\n",
      "Train Epoch: 227 [300000/697932 (43%)]\tLoss: 161.061625\n",
      "Train Epoch: 227 [400000/697932 (57%)]\tLoss: 162.636703\n",
      "Train Epoch: 227 [500000/697932 (72%)]\tLoss: 163.966609\n",
      "Train Epoch: 227 [600000/697932 (86%)]\tLoss: 163.246609\n",
      "====> Epoch: 227 Average loss: 162.6532\n",
      "====> Test set loss: 164.9241\n",
      "Train Epoch: 228 [0/697932 (0%)]\tLoss: 161.579969\n",
      "Train Epoch: 228 [100000/697932 (14%)]\tLoss: 162.456641\n",
      "Train Epoch: 228 [200000/697932 (29%)]\tLoss: 164.246844\n",
      "Train Epoch: 228 [300000/697932 (43%)]\tLoss: 164.061766\n",
      "Train Epoch: 228 [400000/697932 (57%)]\tLoss: 162.663000\n",
      "Train Epoch: 228 [500000/697932 (72%)]\tLoss: 160.599391\n",
      "Train Epoch: 228 [600000/697932 (86%)]\tLoss: 161.778859\n",
      "====> Epoch: 228 Average loss: 162.7107\n",
      "====> Test set loss: 165.2442\n",
      "Train Epoch: 229 [0/697932 (0%)]\tLoss: 162.691969\n",
      "Train Epoch: 229 [100000/697932 (14%)]\tLoss: 163.183094\n",
      "Train Epoch: 229 [200000/697932 (29%)]\tLoss: 160.359234\n",
      "Train Epoch: 229 [300000/697932 (43%)]\tLoss: 163.273203\n",
      "Train Epoch: 229 [400000/697932 (57%)]\tLoss: 163.541328\n",
      "Train Epoch: 229 [500000/697932 (72%)]\tLoss: 162.588313\n",
      "Train Epoch: 229 [600000/697932 (86%)]\tLoss: 161.784281\n",
      "====> Epoch: 229 Average loss: 162.6576\n",
      "====> Test set loss: 164.9616\n",
      "Train Epoch: 230 [0/697932 (0%)]\tLoss: 161.462734\n",
      "Train Epoch: 230 [100000/697932 (14%)]\tLoss: 159.636328\n",
      "Train Epoch: 230 [200000/697932 (29%)]\tLoss: 160.482063\n",
      "Train Epoch: 230 [300000/697932 (43%)]\tLoss: 161.595922\n",
      "Train Epoch: 230 [400000/697932 (57%)]\tLoss: 163.231797\n",
      "Train Epoch: 230 [500000/697932 (72%)]\tLoss: 160.559125\n",
      "Train Epoch: 230 [600000/697932 (86%)]\tLoss: 162.733313\n",
      "====> Epoch: 230 Average loss: 162.7670\n",
      "====> Test set loss: 165.0576\n",
      "Train Epoch: 231 [0/697932 (0%)]\tLoss: 164.212203\n",
      "Train Epoch: 231 [100000/697932 (14%)]\tLoss: 162.642250\n",
      "Train Epoch: 231 [200000/697932 (29%)]\tLoss: 162.034000\n",
      "Train Epoch: 231 [300000/697932 (43%)]\tLoss: 160.983234\n",
      "Train Epoch: 231 [400000/697932 (57%)]\tLoss: 163.028297\n",
      "Train Epoch: 231 [500000/697932 (72%)]\tLoss: 162.232031\n",
      "Train Epoch: 231 [600000/697932 (86%)]\tLoss: 163.299766\n",
      "====> Epoch: 231 Average loss: 162.6768\n",
      "====> Test set loss: 165.2599\n",
      "Train Epoch: 232 [0/697932 (0%)]\tLoss: 163.836719\n",
      "Train Epoch: 232 [100000/697932 (14%)]\tLoss: 160.855875\n",
      "Train Epoch: 232 [200000/697932 (29%)]\tLoss: 163.452484\n",
      "Train Epoch: 232 [300000/697932 (43%)]\tLoss: 163.344625\n",
      "Train Epoch: 232 [400000/697932 (57%)]\tLoss: 163.133375\n",
      "Train Epoch: 232 [500000/697932 (72%)]\tLoss: 161.970188\n",
      "Train Epoch: 232 [600000/697932 (86%)]\tLoss: 163.320562\n",
      "====> Epoch: 232 Average loss: 162.5798\n",
      "====> Test set loss: 164.9010\n",
      "Train Epoch: 233 [0/697932 (0%)]\tLoss: 164.211016\n",
      "Train Epoch: 233 [100000/697932 (14%)]\tLoss: 160.485234\n",
      "Train Epoch: 233 [200000/697932 (29%)]\tLoss: 163.392188\n",
      "Train Epoch: 233 [300000/697932 (43%)]\tLoss: 163.763328\n",
      "Train Epoch: 233 [400000/697932 (57%)]\tLoss: 163.112750\n",
      "Train Epoch: 233 [500000/697932 (72%)]\tLoss: 160.901031\n",
      "Train Epoch: 233 [600000/697932 (86%)]\tLoss: 163.369500\n",
      "====> Epoch: 233 Average loss: 162.6406\n",
      "====> Test set loss: 165.0447\n",
      "Train Epoch: 234 [0/697932 (0%)]\tLoss: 161.137547\n",
      "Train Epoch: 234 [100000/697932 (14%)]\tLoss: 164.231094\n",
      "Train Epoch: 234 [200000/697932 (29%)]\tLoss: 162.400781\n",
      "Train Epoch: 234 [300000/697932 (43%)]\tLoss: 161.690750\n",
      "Train Epoch: 234 [400000/697932 (57%)]\tLoss: 160.624406\n",
      "Train Epoch: 234 [500000/697932 (72%)]\tLoss: 162.586578\n",
      "Train Epoch: 234 [600000/697932 (86%)]\tLoss: 163.296609\n",
      "====> Epoch: 234 Average loss: 162.6273\n",
      "====> Test set loss: 164.9329\n",
      "Train Epoch: 235 [0/697932 (0%)]\tLoss: 162.585172\n",
      "Train Epoch: 235 [100000/697932 (14%)]\tLoss: 160.893063\n",
      "Train Epoch: 235 [200000/697932 (29%)]\tLoss: 162.748594\n",
      "Train Epoch: 235 [300000/697932 (43%)]\tLoss: 160.903047\n",
      "Train Epoch: 235 [400000/697932 (57%)]\tLoss: 165.076281\n",
      "Train Epoch: 235 [500000/697932 (72%)]\tLoss: 161.337437\n",
      "Train Epoch: 235 [600000/697932 (86%)]\tLoss: 164.372750\n",
      "====> Epoch: 235 Average loss: 162.6976\n",
      "====> Test set loss: 165.0297\n",
      "Train Epoch: 236 [0/697932 (0%)]\tLoss: 161.083969\n",
      "Train Epoch: 236 [100000/697932 (14%)]\tLoss: 160.810734\n",
      "Train Epoch: 236 [200000/697932 (29%)]\tLoss: 162.394359\n",
      "Train Epoch: 236 [300000/697932 (43%)]\tLoss: 162.629531\n",
      "Train Epoch: 236 [400000/697932 (57%)]\tLoss: 161.627062\n",
      "Train Epoch: 236 [500000/697932 (72%)]\tLoss: 166.197266\n",
      "Train Epoch: 236 [600000/697932 (86%)]\tLoss: 162.964000\n",
      "====> Epoch: 236 Average loss: 162.6768\n",
      "====> Test set loss: 164.9238\n",
      "Train Epoch: 237 [0/697932 (0%)]\tLoss: 163.625734\n",
      "Train Epoch: 237 [100000/697932 (14%)]\tLoss: 161.488250\n",
      "Train Epoch: 237 [200000/697932 (29%)]\tLoss: 163.067000\n",
      "Train Epoch: 237 [300000/697932 (43%)]\tLoss: 164.086266\n",
      "Train Epoch: 237 [400000/697932 (57%)]\tLoss: 161.333250\n",
      "Train Epoch: 237 [500000/697932 (72%)]\tLoss: 163.607750\n",
      "Train Epoch: 237 [600000/697932 (86%)]\tLoss: 163.491422\n",
      "====> Epoch: 237 Average loss: 162.6188\n",
      "====> Test set loss: 164.9938\n",
      "Train Epoch: 238 [0/697932 (0%)]\tLoss: 162.026094\n",
      "Train Epoch: 238 [100000/697932 (14%)]\tLoss: 165.717813\n",
      "Train Epoch: 238 [200000/697932 (29%)]\tLoss: 160.732063\n",
      "Train Epoch: 238 [300000/697932 (43%)]\tLoss: 166.001047\n",
      "Train Epoch: 238 [400000/697932 (57%)]\tLoss: 163.478688\n",
      "Train Epoch: 238 [500000/697932 (72%)]\tLoss: 161.591531\n",
      "Train Epoch: 238 [600000/697932 (86%)]\tLoss: 164.090203\n",
      "====> Epoch: 238 Average loss: 162.6624\n",
      "====> Test set loss: 165.2087\n",
      "Train Epoch: 239 [0/697932 (0%)]\tLoss: 163.508531\n",
      "Train Epoch: 239 [100000/697932 (14%)]\tLoss: 162.611172\n",
      "Train Epoch: 239 [200000/697932 (29%)]\tLoss: 163.603578\n",
      "Train Epoch: 239 [300000/697932 (43%)]\tLoss: 163.657156\n",
      "Train Epoch: 239 [400000/697932 (57%)]\tLoss: 161.421984\n",
      "Train Epoch: 239 [500000/697932 (72%)]\tLoss: 162.645094\n",
      "Train Epoch: 239 [600000/697932 (86%)]\tLoss: 165.653609\n",
      "====> Epoch: 239 Average loss: 162.6081\n",
      "====> Test set loss: 165.3320\n",
      "Train Epoch: 240 [0/697932 (0%)]\tLoss: 161.104500\n",
      "Train Epoch: 240 [100000/697932 (14%)]\tLoss: 160.103141\n",
      "Train Epoch: 240 [200000/697932 (29%)]\tLoss: 164.611266\n",
      "Train Epoch: 240 [300000/697932 (43%)]\tLoss: 162.133969\n",
      "Train Epoch: 240 [400000/697932 (57%)]\tLoss: 161.138891\n",
      "Train Epoch: 240 [500000/697932 (72%)]\tLoss: 162.096250\n",
      "Train Epoch: 240 [600000/697932 (86%)]\tLoss: 166.629078\n",
      "====> Epoch: 240 Average loss: 162.6819\n",
      "====> Test set loss: 164.9253\n",
      "Train Epoch: 241 [0/697932 (0%)]\tLoss: 163.019016\n",
      "Train Epoch: 241 [100000/697932 (14%)]\tLoss: 162.349203\n",
      "Train Epoch: 241 [200000/697932 (29%)]\tLoss: 162.188625\n",
      "Train Epoch: 241 [300000/697932 (43%)]\tLoss: 162.189562\n",
      "Train Epoch: 241 [400000/697932 (57%)]\tLoss: 160.424500\n",
      "Train Epoch: 241 [500000/697932 (72%)]\tLoss: 162.515422\n",
      "Train Epoch: 241 [600000/697932 (86%)]\tLoss: 160.117531\n",
      "====> Epoch: 241 Average loss: 162.6100\n",
      "====> Test set loss: 164.8955\n",
      "Train Epoch: 242 [0/697932 (0%)]\tLoss: 163.396297\n",
      "Train Epoch: 242 [100000/697932 (14%)]\tLoss: 161.289922\n",
      "Train Epoch: 242 [200000/697932 (29%)]\tLoss: 162.789875\n",
      "Train Epoch: 242 [300000/697932 (43%)]\tLoss: 163.864812\n",
      "Train Epoch: 242 [400000/697932 (57%)]\tLoss: 163.161531\n",
      "Train Epoch: 242 [500000/697932 (72%)]\tLoss: 162.303484\n",
      "Train Epoch: 242 [600000/697932 (86%)]\tLoss: 161.835266\n",
      "====> Epoch: 242 Average loss: 162.6137\n",
      "====> Test set loss: 164.9127\n",
      "Train Epoch: 243 [0/697932 (0%)]\tLoss: 162.653953\n",
      "Train Epoch: 243 [100000/697932 (14%)]\tLoss: 163.823391\n",
      "Train Epoch: 243 [200000/697932 (29%)]\tLoss: 164.284250\n",
      "Train Epoch: 243 [300000/697932 (43%)]\tLoss: 162.050125\n",
      "Train Epoch: 243 [400000/697932 (57%)]\tLoss: 163.053922\n",
      "Train Epoch: 243 [500000/697932 (72%)]\tLoss: 163.043453\n",
      "Train Epoch: 243 [600000/697932 (86%)]\tLoss: 161.755266\n",
      "====> Epoch: 243 Average loss: 162.6519\n",
      "====> Test set loss: 165.1079\n",
      "Train Epoch: 244 [0/697932 (0%)]\tLoss: 162.631969\n",
      "Train Epoch: 244 [100000/697932 (14%)]\tLoss: 161.887703\n",
      "Train Epoch: 244 [200000/697932 (29%)]\tLoss: 160.779297\n",
      "Train Epoch: 244 [300000/697932 (43%)]\tLoss: 165.866078\n",
      "Train Epoch: 244 [400000/697932 (57%)]\tLoss: 161.674297\n",
      "Train Epoch: 244 [500000/697932 (72%)]\tLoss: 161.193938\n",
      "Train Epoch: 244 [600000/697932 (86%)]\tLoss: 163.625359\n",
      "====> Epoch: 244 Average loss: 162.6466\n",
      "====> Test set loss: 164.8959\n",
      "Train Epoch: 245 [0/697932 (0%)]\tLoss: 162.870328\n",
      "Train Epoch: 245 [100000/697932 (14%)]\tLoss: 163.393641\n",
      "Train Epoch: 245 [200000/697932 (29%)]\tLoss: 163.373875\n",
      "Train Epoch: 245 [300000/697932 (43%)]\tLoss: 161.930922\n",
      "Train Epoch: 245 [400000/697932 (57%)]\tLoss: 162.317281\n",
      "Train Epoch: 245 [500000/697932 (72%)]\tLoss: 158.421375\n",
      "Train Epoch: 245 [600000/697932 (86%)]\tLoss: 163.022531\n",
      "====> Epoch: 245 Average loss: 162.6148\n",
      "====> Test set loss: 165.1415\n",
      "Train Epoch: 246 [0/697932 (0%)]\tLoss: 162.944594\n",
      "Train Epoch: 246 [100000/697932 (14%)]\tLoss: 163.439344\n",
      "Train Epoch: 246 [200000/697932 (29%)]\tLoss: 162.911719\n",
      "Train Epoch: 246 [300000/697932 (43%)]\tLoss: 161.936000\n",
      "Train Epoch: 246 [400000/697932 (57%)]\tLoss: 163.660078\n",
      "Train Epoch: 246 [500000/697932 (72%)]\tLoss: 165.310859\n",
      "Train Epoch: 246 [600000/697932 (86%)]\tLoss: 162.796250\n",
      "====> Epoch: 246 Average loss: 162.5984\n",
      "====> Test set loss: 165.1278\n",
      "Train Epoch: 247 [0/697932 (0%)]\tLoss: 161.296984\n",
      "Train Epoch: 247 [100000/697932 (14%)]\tLoss: 160.973344\n",
      "Train Epoch: 247 [200000/697932 (29%)]\tLoss: 158.753938\n",
      "Train Epoch: 247 [300000/697932 (43%)]\tLoss: 163.852062\n",
      "Train Epoch: 247 [400000/697932 (57%)]\tLoss: 161.393828\n",
      "Train Epoch: 247 [500000/697932 (72%)]\tLoss: 161.099656\n",
      "Train Epoch: 247 [600000/697932 (86%)]\tLoss: 161.914625\n",
      "====> Epoch: 247 Average loss: 162.6507\n",
      "====> Test set loss: 164.8373\n",
      "Train Epoch: 248 [0/697932 (0%)]\tLoss: 161.799219\n",
      "Train Epoch: 248 [100000/697932 (14%)]\tLoss: 161.970438\n",
      "Train Epoch: 248 [200000/697932 (29%)]\tLoss: 164.047781\n",
      "Train Epoch: 248 [300000/697932 (43%)]\tLoss: 165.350094\n",
      "Train Epoch: 248 [400000/697932 (57%)]\tLoss: 162.086219\n",
      "Train Epoch: 248 [500000/697932 (72%)]\tLoss: 160.355734\n",
      "Train Epoch: 248 [600000/697932 (86%)]\tLoss: 161.182187\n",
      "====> Epoch: 248 Average loss: 162.5835\n",
      "====> Test set loss: 164.9056\n",
      "Train Epoch: 249 [0/697932 (0%)]\tLoss: 162.475469\n",
      "Train Epoch: 249 [100000/697932 (14%)]\tLoss: 162.958984\n",
      "Train Epoch: 249 [200000/697932 (29%)]\tLoss: 162.881031\n",
      "Train Epoch: 249 [300000/697932 (43%)]\tLoss: 162.008281\n",
      "Train Epoch: 249 [400000/697932 (57%)]\tLoss: 161.443750\n",
      "Train Epoch: 249 [500000/697932 (72%)]\tLoss: 162.120172\n",
      "Train Epoch: 249 [600000/697932 (86%)]\tLoss: 161.839297\n",
      "====> Epoch: 249 Average loss: 162.5616\n",
      "====> Test set loss: 164.8552\n",
      "Train Epoch: 250 [0/697932 (0%)]\tLoss: 162.977984\n",
      "Train Epoch: 250 [100000/697932 (14%)]\tLoss: 164.190844\n",
      "Train Epoch: 250 [200000/697932 (29%)]\tLoss: 161.577453\n",
      "Train Epoch: 250 [300000/697932 (43%)]\tLoss: 158.927234\n",
      "Train Epoch: 250 [400000/697932 (57%)]\tLoss: 160.567375\n",
      "Train Epoch: 250 [500000/697932 (72%)]\tLoss: 161.350328\n",
      "Train Epoch: 250 [600000/697932 (86%)]\tLoss: 163.865453\n",
      "====> Epoch: 250 Average loss: 162.5665\n",
      "====> Test set loss: 164.9056\n",
      "Train Epoch: 251 [0/697932 (0%)]\tLoss: 162.834422\n",
      "Train Epoch: 251 [100000/697932 (14%)]\tLoss: 161.957922\n",
      "Train Epoch: 251 [200000/697932 (29%)]\tLoss: 160.603875\n",
      "Train Epoch: 251 [300000/697932 (43%)]\tLoss: 161.888062\n",
      "Train Epoch: 251 [400000/697932 (57%)]\tLoss: 161.855484\n",
      "Train Epoch: 251 [500000/697932 (72%)]\tLoss: 163.432125\n",
      "Train Epoch: 251 [600000/697932 (86%)]\tLoss: 161.857891\n",
      "====> Epoch: 251 Average loss: 162.6058\n",
      "====> Test set loss: 165.0015\n",
      "Train Epoch: 252 [0/697932 (0%)]\tLoss: 165.013344\n",
      "Train Epoch: 252 [100000/697932 (14%)]\tLoss: 164.354297\n",
      "Train Epoch: 252 [200000/697932 (29%)]\tLoss: 163.205672\n",
      "Train Epoch: 252 [300000/697932 (43%)]\tLoss: 161.935156\n",
      "Train Epoch: 252 [400000/697932 (57%)]\tLoss: 161.555656\n",
      "Train Epoch: 252 [500000/697932 (72%)]\tLoss: 161.780844\n",
      "Train Epoch: 252 [600000/697932 (86%)]\tLoss: 162.612250\n",
      "====> Epoch: 252 Average loss: 162.6350\n",
      "====> Test set loss: 165.1222\n",
      "Train Epoch: 253 [0/697932 (0%)]\tLoss: 162.639922\n",
      "Train Epoch: 253 [100000/697932 (14%)]\tLoss: 160.569500\n",
      "Train Epoch: 253 [200000/697932 (29%)]\tLoss: 162.075312\n",
      "Train Epoch: 253 [300000/697932 (43%)]\tLoss: 161.087312\n",
      "Train Epoch: 253 [400000/697932 (57%)]\tLoss: 162.842078\n",
      "Train Epoch: 253 [500000/697932 (72%)]\tLoss: 161.669719\n",
      "Train Epoch: 253 [600000/697932 (86%)]\tLoss: 163.242250\n",
      "====> Epoch: 253 Average loss: 162.6351\n",
      "====> Test set loss: 165.0316\n",
      "Train Epoch: 254 [0/697932 (0%)]\tLoss: 163.842344\n",
      "Train Epoch: 254 [100000/697932 (14%)]\tLoss: 164.039484\n",
      "Train Epoch: 254 [200000/697932 (29%)]\tLoss: 159.591078\n",
      "Train Epoch: 254 [300000/697932 (43%)]\tLoss: 164.001609\n",
      "Train Epoch: 254 [400000/697932 (57%)]\tLoss: 160.274516\n",
      "Train Epoch: 254 [500000/697932 (72%)]\tLoss: 161.607891\n",
      "Train Epoch: 254 [600000/697932 (86%)]\tLoss: 161.060969\n",
      "====> Epoch: 254 Average loss: 162.5071\n",
      "====> Test set loss: 165.0664\n",
      "Train Epoch: 255 [0/697932 (0%)]\tLoss: 163.844531\n",
      "Train Epoch: 255 [100000/697932 (14%)]\tLoss: 161.585703\n",
      "Train Epoch: 255 [200000/697932 (29%)]\tLoss: 163.089656\n",
      "Train Epoch: 255 [300000/697932 (43%)]\tLoss: 163.601328\n",
      "Train Epoch: 255 [400000/697932 (57%)]\tLoss: 162.457016\n",
      "Train Epoch: 255 [500000/697932 (72%)]\tLoss: 162.040375\n",
      "Train Epoch: 255 [600000/697932 (86%)]\tLoss: 163.516406\n",
      "====> Epoch: 255 Average loss: 162.5125\n",
      "====> Test set loss: 164.9788\n",
      "Train Epoch: 256 [0/697932 (0%)]\tLoss: 161.976500\n",
      "Train Epoch: 256 [100000/697932 (14%)]\tLoss: 162.525359\n",
      "Train Epoch: 256 [200000/697932 (29%)]\tLoss: 160.842875\n",
      "Train Epoch: 256 [300000/697932 (43%)]\tLoss: 162.458203\n",
      "Train Epoch: 256 [400000/697932 (57%)]\tLoss: 162.453250\n",
      "Train Epoch: 256 [500000/697932 (72%)]\tLoss: 164.364406\n",
      "Train Epoch: 256 [600000/697932 (86%)]\tLoss: 162.970953\n",
      "====> Epoch: 256 Average loss: 162.5868\n",
      "====> Test set loss: 164.8069\n",
      "Train Epoch: 257 [0/697932 (0%)]\tLoss: 163.724187\n",
      "Train Epoch: 257 [100000/697932 (14%)]\tLoss: 161.937094\n",
      "Train Epoch: 257 [200000/697932 (29%)]\tLoss: 161.142063\n",
      "Train Epoch: 257 [300000/697932 (43%)]\tLoss: 162.267266\n",
      "Train Epoch: 257 [400000/697932 (57%)]\tLoss: 162.909156\n",
      "Train Epoch: 257 [500000/697932 (72%)]\tLoss: 161.371781\n",
      "Train Epoch: 257 [600000/697932 (86%)]\tLoss: 163.125500\n",
      "====> Epoch: 257 Average loss: 162.5337\n",
      "====> Test set loss: 164.9021\n",
      "Train Epoch: 258 [0/697932 (0%)]\tLoss: 162.599453\n",
      "Train Epoch: 258 [100000/697932 (14%)]\tLoss: 161.419641\n",
      "Train Epoch: 258 [200000/697932 (29%)]\tLoss: 162.065906\n",
      "Train Epoch: 258 [300000/697932 (43%)]\tLoss: 160.463063\n",
      "Train Epoch: 258 [400000/697932 (57%)]\tLoss: 161.754953\n",
      "Train Epoch: 258 [500000/697932 (72%)]\tLoss: 163.305219\n",
      "Train Epoch: 258 [600000/697932 (86%)]\tLoss: 163.079391\n",
      "====> Epoch: 258 Average loss: 162.4896\n",
      "====> Test set loss: 164.8352\n",
      "Train Epoch: 259 [0/697932 (0%)]\tLoss: 164.801875\n",
      "Train Epoch: 259 [100000/697932 (14%)]\tLoss: 162.179828\n",
      "Train Epoch: 259 [200000/697932 (29%)]\tLoss: 161.256906\n",
      "Train Epoch: 259 [300000/697932 (43%)]\tLoss: 162.567297\n",
      "Train Epoch: 259 [400000/697932 (57%)]\tLoss: 161.211766\n",
      "Train Epoch: 259 [500000/697932 (72%)]\tLoss: 165.278969\n",
      "Train Epoch: 259 [600000/697932 (86%)]\tLoss: 164.041828\n",
      "====> Epoch: 259 Average loss: 162.5707\n",
      "====> Test set loss: 164.8228\n",
      "Train Epoch: 260 [0/697932 (0%)]\tLoss: 163.390297\n",
      "Train Epoch: 260 [100000/697932 (14%)]\tLoss: 163.696516\n",
      "Train Epoch: 260 [200000/697932 (29%)]\tLoss: 160.204937\n",
      "Train Epoch: 260 [300000/697932 (43%)]\tLoss: 161.452875\n",
      "Train Epoch: 260 [400000/697932 (57%)]\tLoss: 161.733016\n",
      "Train Epoch: 260 [500000/697932 (72%)]\tLoss: 162.543891\n",
      "Train Epoch: 260 [600000/697932 (86%)]\tLoss: 163.279234\n",
      "====> Epoch: 260 Average loss: 162.5057\n",
      "====> Test set loss: 165.0899\n",
      "Train Epoch: 261 [0/697932 (0%)]\tLoss: 161.457281\n",
      "Train Epoch: 261 [100000/697932 (14%)]\tLoss: 163.148078\n",
      "Train Epoch: 261 [200000/697932 (29%)]\tLoss: 161.257484\n",
      "Train Epoch: 261 [300000/697932 (43%)]\tLoss: 161.155797\n",
      "Train Epoch: 261 [400000/697932 (57%)]\tLoss: 161.693797\n",
      "Train Epoch: 261 [500000/697932 (72%)]\tLoss: 164.590297\n",
      "Train Epoch: 261 [600000/697932 (86%)]\tLoss: 161.746547\n",
      "====> Epoch: 261 Average loss: 162.5963\n",
      "====> Test set loss: 164.7981\n",
      "Train Epoch: 262 [0/697932 (0%)]\tLoss: 162.976641\n",
      "Train Epoch: 262 [100000/697932 (14%)]\tLoss: 163.976453\n",
      "Train Epoch: 262 [200000/697932 (29%)]\tLoss: 163.665453\n",
      "Train Epoch: 262 [300000/697932 (43%)]\tLoss: 162.983766\n",
      "Train Epoch: 262 [400000/697932 (57%)]\tLoss: 162.912937\n",
      "Train Epoch: 262 [500000/697932 (72%)]\tLoss: 165.552266\n",
      "Train Epoch: 262 [600000/697932 (86%)]\tLoss: 161.595500\n",
      "====> Epoch: 262 Average loss: 162.5272\n",
      "====> Test set loss: 165.0810\n",
      "Train Epoch: 263 [0/697932 (0%)]\tLoss: 163.882438\n",
      "Train Epoch: 263 [100000/697932 (14%)]\tLoss: 165.888734\n",
      "Train Epoch: 263 [200000/697932 (29%)]\tLoss: 161.604016\n",
      "Train Epoch: 263 [300000/697932 (43%)]\tLoss: 163.218609\n",
      "Train Epoch: 263 [400000/697932 (57%)]\tLoss: 163.749734\n",
      "Train Epoch: 263 [500000/697932 (72%)]\tLoss: 166.024203\n",
      "Train Epoch: 263 [600000/697932 (86%)]\tLoss: 163.463656\n",
      "====> Epoch: 263 Average loss: 162.5208\n",
      "====> Test set loss: 164.9584\n",
      "Train Epoch: 264 [0/697932 (0%)]\tLoss: 163.266125\n",
      "Train Epoch: 264 [100000/697932 (14%)]\tLoss: 163.355797\n",
      "Train Epoch: 264 [200000/697932 (29%)]\tLoss: 162.820484\n",
      "Train Epoch: 264 [300000/697932 (43%)]\tLoss: 160.793531\n",
      "Train Epoch: 264 [400000/697932 (57%)]\tLoss: 162.679812\n",
      "Train Epoch: 264 [500000/697932 (72%)]\tLoss: 163.706453\n",
      "Train Epoch: 264 [600000/697932 (86%)]\tLoss: 163.168609\n",
      "====> Epoch: 264 Average loss: 162.5065\n",
      "====> Test set loss: 164.9691\n",
      "Train Epoch: 265 [0/697932 (0%)]\tLoss: 162.717594\n",
      "Train Epoch: 265 [100000/697932 (14%)]\tLoss: 163.149406\n",
      "Train Epoch: 265 [200000/697932 (29%)]\tLoss: 165.939594\n",
      "Train Epoch: 265 [300000/697932 (43%)]\tLoss: 164.232188\n",
      "Train Epoch: 265 [400000/697932 (57%)]\tLoss: 162.017453\n",
      "Train Epoch: 265 [500000/697932 (72%)]\tLoss: 162.102188\n",
      "Train Epoch: 265 [600000/697932 (86%)]\tLoss: 163.425187\n",
      "====> Epoch: 265 Average loss: 162.3963\n",
      "====> Test set loss: 164.7357\n",
      "Train Epoch: 266 [0/697932 (0%)]\tLoss: 161.614875\n",
      "Train Epoch: 266 [100000/697932 (14%)]\tLoss: 161.035859\n",
      "Train Epoch: 266 [200000/697932 (29%)]\tLoss: 161.937234\n",
      "Train Epoch: 266 [300000/697932 (43%)]\tLoss: 163.439031\n",
      "Train Epoch: 266 [400000/697932 (57%)]\tLoss: 162.727766\n",
      "Train Epoch: 266 [500000/697932 (72%)]\tLoss: 160.176312\n",
      "Train Epoch: 266 [600000/697932 (86%)]\tLoss: 161.952109\n",
      "====> Epoch: 266 Average loss: 162.4148\n",
      "====> Test set loss: 164.9402\n",
      "Train Epoch: 267 [0/697932 (0%)]\tLoss: 164.635234\n",
      "Train Epoch: 267 [100000/697932 (14%)]\tLoss: 160.970594\n",
      "Train Epoch: 267 [200000/697932 (29%)]\tLoss: 162.556797\n",
      "Train Epoch: 267 [300000/697932 (43%)]\tLoss: 162.118687\n",
      "Train Epoch: 267 [400000/697932 (57%)]\tLoss: 161.577406\n",
      "Train Epoch: 267 [500000/697932 (72%)]\tLoss: 161.306500\n",
      "Train Epoch: 267 [600000/697932 (86%)]\tLoss: 161.853312\n",
      "====> Epoch: 267 Average loss: 162.4570\n",
      "====> Test set loss: 164.9063\n",
      "Train Epoch: 268 [0/697932 (0%)]\tLoss: 159.987641\n",
      "Train Epoch: 268 [100000/697932 (14%)]\tLoss: 163.145938\n",
      "Train Epoch: 268 [200000/697932 (29%)]\tLoss: 161.728609\n",
      "Train Epoch: 268 [300000/697932 (43%)]\tLoss: 162.805531\n",
      "Train Epoch: 268 [400000/697932 (57%)]\tLoss: 162.126344\n",
      "Train Epoch: 268 [500000/697932 (72%)]\tLoss: 162.641359\n",
      "Train Epoch: 268 [600000/697932 (86%)]\tLoss: 164.406484\n",
      "====> Epoch: 268 Average loss: 162.5113\n",
      "====> Test set loss: 165.0863\n",
      "Train Epoch: 269 [0/697932 (0%)]\tLoss: 162.183125\n",
      "Train Epoch: 269 [100000/697932 (14%)]\tLoss: 159.699844\n",
      "Train Epoch: 269 [200000/697932 (29%)]\tLoss: 161.634922\n",
      "Train Epoch: 269 [300000/697932 (43%)]\tLoss: 162.451453\n",
      "Train Epoch: 269 [400000/697932 (57%)]\tLoss: 162.622234\n",
      "Train Epoch: 269 [500000/697932 (72%)]\tLoss: 165.314453\n",
      "Train Epoch: 269 [600000/697932 (86%)]\tLoss: 161.851422\n",
      "====> Epoch: 269 Average loss: 162.4846\n",
      "====> Test set loss: 164.9847\n",
      "Train Epoch: 270 [0/697932 (0%)]\tLoss: 162.566578\n",
      "Train Epoch: 270 [100000/697932 (14%)]\tLoss: 160.645141\n",
      "Train Epoch: 270 [200000/697932 (29%)]\tLoss: 164.371609\n",
      "Train Epoch: 270 [300000/697932 (43%)]\tLoss: 162.915031\n",
      "Train Epoch: 270 [400000/697932 (57%)]\tLoss: 162.600859\n",
      "Train Epoch: 270 [500000/697932 (72%)]\tLoss: 160.012406\n",
      "Train Epoch: 270 [600000/697932 (86%)]\tLoss: 162.019813\n",
      "====> Epoch: 270 Average loss: 162.4151\n",
      "====> Test set loss: 164.7242\n",
      "Train Epoch: 271 [0/697932 (0%)]\tLoss: 160.354250\n",
      "Train Epoch: 271 [100000/697932 (14%)]\tLoss: 160.865016\n",
      "Train Epoch: 271 [200000/697932 (29%)]\tLoss: 162.296031\n",
      "Train Epoch: 271 [300000/697932 (43%)]\tLoss: 162.614141\n",
      "Train Epoch: 271 [400000/697932 (57%)]\tLoss: 162.548781\n",
      "Train Epoch: 271 [500000/697932 (72%)]\tLoss: 164.547203\n",
      "Train Epoch: 271 [600000/697932 (86%)]\tLoss: 161.613359\n",
      "====> Epoch: 271 Average loss: 162.3899\n",
      "====> Test set loss: 164.7423\n",
      "Train Epoch: 272 [0/697932 (0%)]\tLoss: 164.637438\n",
      "Train Epoch: 272 [100000/697932 (14%)]\tLoss: 166.481812\n",
      "Train Epoch: 272 [200000/697932 (29%)]\tLoss: 161.762016\n",
      "Train Epoch: 272 [300000/697932 (43%)]\tLoss: 164.658000\n",
      "Train Epoch: 272 [400000/697932 (57%)]\tLoss: 163.017063\n",
      "Train Epoch: 272 [500000/697932 (72%)]\tLoss: 162.299484\n",
      "Train Epoch: 272 [600000/697932 (86%)]\tLoss: 162.613078\n",
      "====> Epoch: 272 Average loss: 162.4579\n",
      "====> Test set loss: 164.9026\n",
      "Train Epoch: 273 [0/697932 (0%)]\tLoss: 161.443813\n",
      "Train Epoch: 273 [100000/697932 (14%)]\tLoss: 163.571016\n",
      "Train Epoch: 273 [200000/697932 (29%)]\tLoss: 162.672484\n",
      "Train Epoch: 273 [300000/697932 (43%)]\tLoss: 164.526641\n",
      "Train Epoch: 273 [400000/697932 (57%)]\tLoss: 162.253688\n",
      "Train Epoch: 273 [500000/697932 (72%)]\tLoss: 163.762297\n",
      "Train Epoch: 273 [600000/697932 (86%)]\tLoss: 162.763766\n",
      "====> Epoch: 273 Average loss: 162.4317\n",
      "====> Test set loss: 164.7130\n",
      "Train Epoch: 274 [0/697932 (0%)]\tLoss: 163.638531\n",
      "Train Epoch: 274 [100000/697932 (14%)]\tLoss: 167.070469\n",
      "Train Epoch: 274 [200000/697932 (29%)]\tLoss: 163.359844\n",
      "Train Epoch: 274 [300000/697932 (43%)]\tLoss: 162.202359\n",
      "Train Epoch: 274 [400000/697932 (57%)]\tLoss: 163.298047\n",
      "Train Epoch: 274 [500000/697932 (72%)]\tLoss: 162.719766\n",
      "Train Epoch: 274 [600000/697932 (86%)]\tLoss: 162.022469\n",
      "====> Epoch: 274 Average loss: 162.4090\n",
      "====> Test set loss: 164.8605\n",
      "Train Epoch: 275 [0/697932 (0%)]\tLoss: 164.712250\n",
      "Train Epoch: 275 [100000/697932 (14%)]\tLoss: 164.629969\n",
      "Train Epoch: 275 [200000/697932 (29%)]\tLoss: 166.029500\n",
      "Train Epoch: 275 [300000/697932 (43%)]\tLoss: 161.060688\n",
      "Train Epoch: 275 [400000/697932 (57%)]\tLoss: 160.487594\n",
      "Train Epoch: 275 [500000/697932 (72%)]\tLoss: 161.064516\n",
      "Train Epoch: 275 [600000/697932 (86%)]\tLoss: 163.548375\n",
      "====> Epoch: 275 Average loss: 162.3179\n",
      "====> Test set loss: 164.9701\n",
      "Train Epoch: 276 [0/697932 (0%)]\tLoss: 162.864188\n",
      "Train Epoch: 276 [100000/697932 (14%)]\tLoss: 163.537531\n",
      "Train Epoch: 276 [200000/697932 (29%)]\tLoss: 161.433844\n",
      "Train Epoch: 276 [300000/697932 (43%)]\tLoss: 160.946672\n",
      "Train Epoch: 276 [400000/697932 (57%)]\tLoss: 163.225937\n",
      "Train Epoch: 276 [500000/697932 (72%)]\tLoss: 162.648547\n",
      "Train Epoch: 276 [600000/697932 (86%)]\tLoss: 162.419953\n",
      "====> Epoch: 276 Average loss: 162.3894\n",
      "====> Test set loss: 164.8446\n",
      "Train Epoch: 277 [0/697932 (0%)]\tLoss: 163.693453\n",
      "Train Epoch: 277 [100000/697932 (14%)]\tLoss: 159.935937\n",
      "Train Epoch: 277 [200000/697932 (29%)]\tLoss: 162.643047\n",
      "Train Epoch: 277 [300000/697932 (43%)]\tLoss: 163.353672\n",
      "Train Epoch: 277 [400000/697932 (57%)]\tLoss: 162.145375\n",
      "Train Epoch: 277 [500000/697932 (72%)]\tLoss: 164.675203\n",
      "Train Epoch: 277 [600000/697932 (86%)]\tLoss: 163.665469\n",
      "====> Epoch: 277 Average loss: 162.3482\n",
      "====> Test set loss: 164.8154\n",
      "Train Epoch: 278 [0/697932 (0%)]\tLoss: 161.641094\n",
      "Train Epoch: 278 [100000/697932 (14%)]\tLoss: 161.168859\n",
      "Train Epoch: 278 [200000/697932 (29%)]\tLoss: 163.868297\n",
      "Train Epoch: 278 [300000/697932 (43%)]\tLoss: 163.142203\n",
      "Train Epoch: 278 [400000/697932 (57%)]\tLoss: 161.814344\n",
      "Train Epoch: 278 [500000/697932 (72%)]\tLoss: 161.834688\n",
      "Train Epoch: 278 [600000/697932 (86%)]\tLoss: 162.281109\n",
      "====> Epoch: 278 Average loss: 162.4293\n",
      "====> Test set loss: 164.7151\n",
      "Train Epoch: 279 [0/697932 (0%)]\tLoss: 160.128234\n",
      "Train Epoch: 279 [100000/697932 (14%)]\tLoss: 162.518250\n",
      "Train Epoch: 279 [200000/697932 (29%)]\tLoss: 162.691703\n",
      "Train Epoch: 279 [300000/697932 (43%)]\tLoss: 161.199031\n",
      "Train Epoch: 279 [400000/697932 (57%)]\tLoss: 163.516156\n",
      "Train Epoch: 279 [500000/697932 (72%)]\tLoss: 162.494547\n",
      "Train Epoch: 279 [600000/697932 (86%)]\tLoss: 164.254672\n",
      "====> Epoch: 279 Average loss: 162.3161\n",
      "====> Test set loss: 164.8398\n",
      "Train Epoch: 280 [0/697932 (0%)]\tLoss: 165.007016\n",
      "Train Epoch: 280 [100000/697932 (14%)]\tLoss: 162.816781\n",
      "Train Epoch: 280 [200000/697932 (29%)]\tLoss: 160.500797\n",
      "Train Epoch: 280 [300000/697932 (43%)]\tLoss: 162.347547\n",
      "Train Epoch: 280 [400000/697932 (57%)]\tLoss: 163.011188\n",
      "Train Epoch: 280 [500000/697932 (72%)]\tLoss: 164.170156\n",
      "Train Epoch: 280 [600000/697932 (86%)]\tLoss: 162.334937\n",
      "====> Epoch: 280 Average loss: 162.4055\n",
      "====> Test set loss: 164.7533\n",
      "Train Epoch: 281 [0/697932 (0%)]\tLoss: 161.301938\n",
      "Train Epoch: 281 [100000/697932 (14%)]\tLoss: 162.551078\n",
      "Train Epoch: 281 [200000/697932 (29%)]\tLoss: 161.465062\n",
      "Train Epoch: 281 [300000/697932 (43%)]\tLoss: 162.222953\n",
      "Train Epoch: 281 [400000/697932 (57%)]\tLoss: 161.582719\n",
      "Train Epoch: 281 [500000/697932 (72%)]\tLoss: 161.099813\n",
      "Train Epoch: 281 [600000/697932 (86%)]\tLoss: 161.913891\n",
      "====> Epoch: 281 Average loss: 162.3333\n",
      "====> Test set loss: 164.9104\n",
      "Train Epoch: 282 [0/697932 (0%)]\tLoss: 165.103797\n",
      "Train Epoch: 282 [100000/697932 (14%)]\tLoss: 160.197500\n",
      "Train Epoch: 282 [200000/697932 (29%)]\tLoss: 160.919953\n",
      "Train Epoch: 282 [300000/697932 (43%)]\tLoss: 161.460719\n",
      "Train Epoch: 282 [400000/697932 (57%)]\tLoss: 161.706656\n",
      "Train Epoch: 282 [500000/697932 (72%)]\tLoss: 164.098719\n",
      "Train Epoch: 282 [600000/697932 (86%)]\tLoss: 163.042812\n",
      "====> Epoch: 282 Average loss: 162.3860\n",
      "====> Test set loss: 164.8092\n",
      "Train Epoch: 283 [0/697932 (0%)]\tLoss: 162.593562\n",
      "Train Epoch: 283 [100000/697932 (14%)]\tLoss: 163.583953\n",
      "Train Epoch: 283 [200000/697932 (29%)]\tLoss: 161.546562\n",
      "Train Epoch: 283 [300000/697932 (43%)]\tLoss: 159.536922\n",
      "Train Epoch: 283 [400000/697932 (57%)]\tLoss: 162.984766\n",
      "Train Epoch: 283 [500000/697932 (72%)]\tLoss: 161.586219\n",
      "Train Epoch: 283 [600000/697932 (86%)]\tLoss: 161.521156\n",
      "====> Epoch: 283 Average loss: 162.3618\n",
      "====> Test set loss: 164.7597\n",
      "Train Epoch: 284 [0/697932 (0%)]\tLoss: 163.203859\n",
      "Train Epoch: 284 [100000/697932 (14%)]\tLoss: 163.148406\n",
      "Train Epoch: 284 [200000/697932 (29%)]\tLoss: 158.583844\n",
      "Train Epoch: 284 [300000/697932 (43%)]\tLoss: 164.926359\n",
      "Train Epoch: 284 [400000/697932 (57%)]\tLoss: 164.430313\n",
      "Train Epoch: 284 [500000/697932 (72%)]\tLoss: 159.525328\n",
      "Train Epoch: 284 [600000/697932 (86%)]\tLoss: 162.738875\n",
      "====> Epoch: 284 Average loss: 162.3502\n",
      "====> Test set loss: 164.7377\n",
      "Train Epoch: 285 [0/697932 (0%)]\tLoss: 162.517422\n",
      "Train Epoch: 285 [100000/697932 (14%)]\tLoss: 161.082266\n",
      "Train Epoch: 285 [200000/697932 (29%)]\tLoss: 160.898250\n",
      "Train Epoch: 285 [300000/697932 (43%)]\tLoss: 161.624609\n",
      "Train Epoch: 285 [400000/697932 (57%)]\tLoss: 162.243000\n",
      "Train Epoch: 285 [500000/697932 (72%)]\tLoss: 161.459875\n",
      "Train Epoch: 285 [600000/697932 (86%)]\tLoss: 163.441969\n",
      "====> Epoch: 285 Average loss: 162.3272\n",
      "====> Test set loss: 164.8071\n",
      "Train Epoch: 286 [0/697932 (0%)]\tLoss: 158.882703\n",
      "Train Epoch: 286 [100000/697932 (14%)]\tLoss: 162.215141\n",
      "Train Epoch: 286 [200000/697932 (29%)]\tLoss: 161.454156\n",
      "Train Epoch: 286 [300000/697932 (43%)]\tLoss: 163.961281\n",
      "Train Epoch: 286 [400000/697932 (57%)]\tLoss: 161.064094\n",
      "Train Epoch: 286 [500000/697932 (72%)]\tLoss: 161.003875\n",
      "Train Epoch: 286 [600000/697932 (86%)]\tLoss: 162.724219\n",
      "====> Epoch: 286 Average loss: 162.3929\n",
      "====> Test set loss: 164.9270\n",
      "Train Epoch: 287 [0/697932 (0%)]\tLoss: 162.114031\n",
      "Train Epoch: 287 [100000/697932 (14%)]\tLoss: 162.943234\n",
      "Train Epoch: 287 [200000/697932 (29%)]\tLoss: 161.105641\n",
      "Train Epoch: 287 [300000/697932 (43%)]\tLoss: 162.203719\n",
      "Train Epoch: 287 [400000/697932 (57%)]\tLoss: 159.693625\n",
      "Train Epoch: 287 [500000/697932 (72%)]\tLoss: 162.629562\n",
      "Train Epoch: 287 [600000/697932 (86%)]\tLoss: 163.307594\n",
      "====> Epoch: 287 Average loss: 162.3930\n",
      "====> Test set loss: 164.8094\n",
      "Train Epoch: 288 [0/697932 (0%)]\tLoss: 164.371000\n",
      "Train Epoch: 288 [100000/697932 (14%)]\tLoss: 162.495047\n",
      "Train Epoch: 288 [200000/697932 (29%)]\tLoss: 164.525875\n",
      "Train Epoch: 288 [300000/697932 (43%)]\tLoss: 163.119453\n",
      "Train Epoch: 288 [400000/697932 (57%)]\tLoss: 163.564312\n",
      "Train Epoch: 288 [500000/697932 (72%)]\tLoss: 165.121656\n",
      "Train Epoch: 288 [600000/697932 (86%)]\tLoss: 162.013937\n",
      "====> Epoch: 288 Average loss: 162.2826\n",
      "====> Test set loss: 165.2049\n",
      "Train Epoch: 289 [0/697932 (0%)]\tLoss: 164.316328\n",
      "Train Epoch: 289 [100000/697932 (14%)]\tLoss: 161.949703\n",
      "Train Epoch: 289 [200000/697932 (29%)]\tLoss: 164.976438\n",
      "Train Epoch: 289 [300000/697932 (43%)]\tLoss: 160.041875\n",
      "Train Epoch: 289 [400000/697932 (57%)]\tLoss: 162.284750\n",
      "Train Epoch: 289 [500000/697932 (72%)]\tLoss: 164.857328\n",
      "Train Epoch: 289 [600000/697932 (86%)]\tLoss: 163.000609\n",
      "====> Epoch: 289 Average loss: 162.3073\n",
      "====> Test set loss: 164.7865\n",
      "Train Epoch: 290 [0/697932 (0%)]\tLoss: 162.769359\n",
      "Train Epoch: 290 [100000/697932 (14%)]\tLoss: 163.997719\n",
      "Train Epoch: 290 [200000/697932 (29%)]\tLoss: 162.642875\n",
      "Train Epoch: 290 [300000/697932 (43%)]\tLoss: 161.707047\n",
      "Train Epoch: 290 [400000/697932 (57%)]\tLoss: 161.625922\n",
      "Train Epoch: 290 [500000/697932 (72%)]\tLoss: 161.877391\n",
      "Train Epoch: 290 [600000/697932 (86%)]\tLoss: 164.123328\n",
      "====> Epoch: 290 Average loss: 162.3389\n",
      "====> Test set loss: 164.8920\n",
      "Train Epoch: 291 [0/697932 (0%)]\tLoss: 161.201063\n",
      "Train Epoch: 291 [100000/697932 (14%)]\tLoss: 163.757516\n",
      "Train Epoch: 291 [200000/697932 (29%)]\tLoss: 164.392047\n",
      "Train Epoch: 291 [300000/697932 (43%)]\tLoss: 164.853203\n",
      "Train Epoch: 291 [400000/697932 (57%)]\tLoss: 164.908859\n",
      "Train Epoch: 291 [500000/697932 (72%)]\tLoss: 161.089953\n",
      "Train Epoch: 291 [600000/697932 (86%)]\tLoss: 160.781781\n",
      "====> Epoch: 291 Average loss: 162.3320\n",
      "====> Test set loss: 164.6462\n",
      "Train Epoch: 292 [0/697932 (0%)]\tLoss: 160.580500\n",
      "Train Epoch: 292 [100000/697932 (14%)]\tLoss: 164.103109\n",
      "Train Epoch: 292 [200000/697932 (29%)]\tLoss: 162.825344\n",
      "Train Epoch: 292 [300000/697932 (43%)]\tLoss: 161.288781\n",
      "Train Epoch: 292 [400000/697932 (57%)]\tLoss: 162.823594\n",
      "Train Epoch: 292 [500000/697932 (72%)]\tLoss: 163.484375\n",
      "Train Epoch: 292 [600000/697932 (86%)]\tLoss: 163.816094\n",
      "====> Epoch: 292 Average loss: 162.4052\n",
      "====> Test set loss: 164.8687\n",
      "Train Epoch: 293 [0/697932 (0%)]\tLoss: 162.281781\n",
      "Train Epoch: 293 [100000/697932 (14%)]\tLoss: 163.108563\n",
      "Train Epoch: 293 [200000/697932 (29%)]\tLoss: 165.274844\n",
      "Train Epoch: 293 [300000/697932 (43%)]\tLoss: 161.983531\n",
      "Train Epoch: 293 [400000/697932 (57%)]\tLoss: 163.164125\n",
      "Train Epoch: 293 [500000/697932 (72%)]\tLoss: 160.336297\n",
      "Train Epoch: 293 [600000/697932 (86%)]\tLoss: 162.320344\n",
      "====> Epoch: 293 Average loss: 162.3362\n",
      "====> Test set loss: 164.8516\n",
      "Train Epoch: 294 [0/697932 (0%)]\tLoss: 160.744656\n",
      "Train Epoch: 294 [100000/697932 (14%)]\tLoss: 162.856875\n",
      "Train Epoch: 294 [200000/697932 (29%)]\tLoss: 164.427938\n",
      "Train Epoch: 294 [300000/697932 (43%)]\tLoss: 163.666859\n",
      "Train Epoch: 294 [400000/697932 (57%)]\tLoss: 163.370594\n",
      "Train Epoch: 294 [500000/697932 (72%)]\tLoss: 162.095844\n",
      "Train Epoch: 294 [600000/697932 (86%)]\tLoss: 162.495859\n",
      "====> Epoch: 294 Average loss: 162.2933\n",
      "====> Test set loss: 164.8690\n",
      "Train Epoch: 295 [0/697932 (0%)]\tLoss: 165.145516\n",
      "Train Epoch: 295 [100000/697932 (14%)]\tLoss: 162.341469\n",
      "Train Epoch: 295 [200000/697932 (29%)]\tLoss: 162.122594\n",
      "Train Epoch: 295 [300000/697932 (43%)]\tLoss: 162.950375\n",
      "Train Epoch: 295 [400000/697932 (57%)]\tLoss: 162.744906\n",
      "Train Epoch: 295 [500000/697932 (72%)]\tLoss: 162.878922\n",
      "Train Epoch: 295 [600000/697932 (86%)]\tLoss: 160.372156\n",
      "====> Epoch: 295 Average loss: 162.3693\n",
      "====> Test set loss: 164.8256\n",
      "Train Epoch: 296 [0/697932 (0%)]\tLoss: 163.485828\n",
      "Train Epoch: 296 [100000/697932 (14%)]\tLoss: 160.957562\n",
      "Train Epoch: 296 [200000/697932 (29%)]\tLoss: 160.955469\n",
      "Train Epoch: 296 [300000/697932 (43%)]\tLoss: 162.267875\n",
      "Train Epoch: 296 [400000/697932 (57%)]\tLoss: 166.581141\n",
      "Train Epoch: 296 [500000/697932 (72%)]\tLoss: 164.531609\n",
      "Train Epoch: 296 [600000/697932 (86%)]\tLoss: 161.182344\n",
      "====> Epoch: 296 Average loss: 162.3414\n",
      "====> Test set loss: 164.7168\n",
      "Train Epoch: 297 [0/697932 (0%)]\tLoss: 162.626703\n",
      "Train Epoch: 297 [100000/697932 (14%)]\tLoss: 161.971047\n",
      "Train Epoch: 297 [200000/697932 (29%)]\tLoss: 164.661188\n",
      "Train Epoch: 297 [300000/697932 (43%)]\tLoss: 162.229125\n",
      "Train Epoch: 297 [400000/697932 (57%)]\tLoss: 160.021703\n",
      "Train Epoch: 297 [500000/697932 (72%)]\tLoss: 162.875953\n",
      "Train Epoch: 297 [600000/697932 (86%)]\tLoss: 165.234922\n",
      "====> Epoch: 297 Average loss: 162.2813\n",
      "====> Test set loss: 164.6526\n",
      "Train Epoch: 298 [0/697932 (0%)]\tLoss: 160.787922\n",
      "Train Epoch: 298 [100000/697932 (14%)]\tLoss: 160.642328\n",
      "Train Epoch: 298 [200000/697932 (29%)]\tLoss: 163.920531\n",
      "Train Epoch: 298 [300000/697932 (43%)]\tLoss: 165.663078\n",
      "Train Epoch: 298 [400000/697932 (57%)]\tLoss: 162.111359\n",
      "Train Epoch: 298 [500000/697932 (72%)]\tLoss: 160.224625\n",
      "Train Epoch: 298 [600000/697932 (86%)]\tLoss: 163.019688\n",
      "====> Epoch: 298 Average loss: 162.3078\n",
      "====> Test set loss: 164.7725\n",
      "Train Epoch: 299 [0/697932 (0%)]\tLoss: 160.520656\n",
      "Train Epoch: 299 [100000/697932 (14%)]\tLoss: 163.324703\n",
      "Train Epoch: 299 [200000/697932 (29%)]\tLoss: 158.558375\n",
      "Train Epoch: 299 [300000/697932 (43%)]\tLoss: 160.696031\n",
      "Train Epoch: 299 [400000/697932 (57%)]\tLoss: 160.728781\n",
      "Train Epoch: 299 [500000/697932 (72%)]\tLoss: 167.356031\n",
      "Train Epoch: 299 [600000/697932 (86%)]\tLoss: 162.409781\n",
      "====> Epoch: 299 Average loss: 162.3497\n",
      "====> Test set loss: 164.6197\n",
      "Train Epoch: 300 [0/697932 (0%)]\tLoss: 160.970219\n",
      "Train Epoch: 300 [100000/697932 (14%)]\tLoss: 165.754844\n",
      "Train Epoch: 300 [200000/697932 (29%)]\tLoss: 166.853500\n",
      "Train Epoch: 300 [300000/697932 (43%)]\tLoss: 159.795781\n",
      "Train Epoch: 300 [400000/697932 (57%)]\tLoss: 162.266578\n",
      "Train Epoch: 300 [500000/697932 (72%)]\tLoss: 164.220219\n",
      "Train Epoch: 300 [600000/697932 (86%)]\tLoss: 160.308375\n",
      "====> Epoch: 300 Average loss: 162.3385\n",
      "====> Test set loss: 164.8497\n",
      "Train Epoch: 301 [0/697932 (0%)]\tLoss: 162.850125\n",
      "Train Epoch: 301 [100000/697932 (14%)]\tLoss: 162.141031\n",
      "Train Epoch: 301 [200000/697932 (29%)]\tLoss: 161.731016\n",
      "Train Epoch: 301 [300000/697932 (43%)]\tLoss: 163.794234\n",
      "Train Epoch: 301 [400000/697932 (57%)]\tLoss: 163.335719\n",
      "Train Epoch: 301 [500000/697932 (72%)]\tLoss: 161.402609\n",
      "Train Epoch: 301 [600000/697932 (86%)]\tLoss: 163.131266\n",
      "====> Epoch: 301 Average loss: 162.2162\n",
      "====> Test set loss: 164.6657\n",
      "Train Epoch: 302 [0/697932 (0%)]\tLoss: 160.255094\n",
      "Train Epoch: 302 [100000/697932 (14%)]\tLoss: 162.790891\n",
      "Train Epoch: 302 [200000/697932 (29%)]\tLoss: 162.924953\n",
      "Train Epoch: 302 [300000/697932 (43%)]\tLoss: 162.203203\n",
      "Train Epoch: 302 [400000/697932 (57%)]\tLoss: 161.263187\n",
      "Train Epoch: 302 [500000/697932 (72%)]\tLoss: 160.259781\n",
      "Train Epoch: 302 [600000/697932 (86%)]\tLoss: 161.495563\n",
      "====> Epoch: 302 Average loss: 162.2684\n",
      "====> Test set loss: 164.6567\n",
      "Train Epoch: 303 [0/697932 (0%)]\tLoss: 162.611562\n",
      "Train Epoch: 303 [100000/697932 (14%)]\tLoss: 158.313281\n",
      "Train Epoch: 303 [200000/697932 (29%)]\tLoss: 162.483281\n",
      "Train Epoch: 303 [300000/697932 (43%)]\tLoss: 161.005703\n",
      "Train Epoch: 303 [400000/697932 (57%)]\tLoss: 162.057047\n",
      "Train Epoch: 303 [500000/697932 (72%)]\tLoss: 161.944437\n",
      "Train Epoch: 303 [600000/697932 (86%)]\tLoss: 162.976094\n",
      "====> Epoch: 303 Average loss: 162.3198\n",
      "====> Test set loss: 164.6544\n",
      "Train Epoch: 304 [0/697932 (0%)]\tLoss: 163.237359\n",
      "Train Epoch: 304 [100000/697932 (14%)]\tLoss: 162.142500\n",
      "Train Epoch: 304 [200000/697932 (29%)]\tLoss: 161.131969\n",
      "Train Epoch: 304 [300000/697932 (43%)]\tLoss: 157.951203\n",
      "Train Epoch: 304 [400000/697932 (57%)]\tLoss: 161.696547\n",
      "Train Epoch: 304 [500000/697932 (72%)]\tLoss: 162.588875\n",
      "Train Epoch: 304 [600000/697932 (86%)]\tLoss: 164.495187\n",
      "====> Epoch: 304 Average loss: 162.3197\n",
      "====> Test set loss: 164.6712\n",
      "Train Epoch: 305 [0/697932 (0%)]\tLoss: 163.380234\n",
      "Train Epoch: 305 [100000/697932 (14%)]\tLoss: 162.783406\n",
      "Train Epoch: 305 [200000/697932 (29%)]\tLoss: 163.185344\n",
      "Train Epoch: 305 [300000/697932 (43%)]\tLoss: 159.460906\n",
      "Train Epoch: 305 [400000/697932 (57%)]\tLoss: 161.237656\n",
      "Train Epoch: 305 [500000/697932 (72%)]\tLoss: 161.853391\n",
      "Train Epoch: 305 [600000/697932 (86%)]\tLoss: 163.170969\n",
      "====> Epoch: 305 Average loss: 162.2535\n",
      "====> Test set loss: 164.8208\n",
      "Train Epoch: 306 [0/697932 (0%)]\tLoss: 163.600750\n",
      "Train Epoch: 306 [100000/697932 (14%)]\tLoss: 157.565609\n",
      "Train Epoch: 306 [200000/697932 (29%)]\tLoss: 162.631609\n",
      "Train Epoch: 306 [300000/697932 (43%)]\tLoss: 162.506344\n",
      "Train Epoch: 306 [400000/697932 (57%)]\tLoss: 159.085656\n",
      "Train Epoch: 306 [500000/697932 (72%)]\tLoss: 160.931234\n",
      "Train Epoch: 306 [600000/697932 (86%)]\tLoss: 162.228781\n",
      "====> Epoch: 306 Average loss: 162.1912\n",
      "====> Test set loss: 164.5844\n",
      "Train Epoch: 307 [0/697932 (0%)]\tLoss: 163.194766\n",
      "Train Epoch: 307 [100000/697932 (14%)]\tLoss: 161.921844\n",
      "Train Epoch: 307 [200000/697932 (29%)]\tLoss: 163.766844\n",
      "Train Epoch: 307 [300000/697932 (43%)]\tLoss: 163.653906\n",
      "Train Epoch: 307 [400000/697932 (57%)]\tLoss: 163.458937\n",
      "Train Epoch: 307 [500000/697932 (72%)]\tLoss: 161.890234\n",
      "Train Epoch: 307 [600000/697932 (86%)]\tLoss: 162.723797\n",
      "====> Epoch: 307 Average loss: 162.2308\n",
      "====> Test set loss: 164.5741\n",
      "Train Epoch: 308 [0/697932 (0%)]\tLoss: 163.562578\n",
      "Train Epoch: 308 [100000/697932 (14%)]\tLoss: 160.782625\n",
      "Train Epoch: 308 [200000/697932 (29%)]\tLoss: 162.067703\n",
      "Train Epoch: 308 [300000/697932 (43%)]\tLoss: 161.659094\n",
      "Train Epoch: 308 [400000/697932 (57%)]\tLoss: 166.964281\n",
      "Train Epoch: 308 [500000/697932 (72%)]\tLoss: 163.215609\n",
      "Train Epoch: 308 [600000/697932 (86%)]\tLoss: 160.348906\n",
      "====> Epoch: 308 Average loss: 162.1968\n",
      "====> Test set loss: 164.5447\n",
      "Train Epoch: 309 [0/697932 (0%)]\tLoss: 161.383656\n",
      "Train Epoch: 309 [100000/697932 (14%)]\tLoss: 163.622141\n",
      "Train Epoch: 309 [200000/697932 (29%)]\tLoss: 162.899891\n",
      "Train Epoch: 309 [300000/697932 (43%)]\tLoss: 158.953578\n",
      "Train Epoch: 309 [400000/697932 (57%)]\tLoss: 163.648641\n",
      "Train Epoch: 309 [500000/697932 (72%)]\tLoss: 161.455391\n",
      "Train Epoch: 309 [600000/697932 (86%)]\tLoss: 161.572453\n",
      "====> Epoch: 309 Average loss: 162.2119\n",
      "====> Test set loss: 164.7357\n",
      "Train Epoch: 310 [0/697932 (0%)]\tLoss: 163.538703\n",
      "Train Epoch: 310 [100000/697932 (14%)]\tLoss: 164.145297\n",
      "Train Epoch: 310 [200000/697932 (29%)]\tLoss: 165.436203\n",
      "Train Epoch: 310 [300000/697932 (43%)]\tLoss: 159.534672\n",
      "Train Epoch: 310 [400000/697932 (57%)]\tLoss: 164.795625\n",
      "Train Epoch: 310 [500000/697932 (72%)]\tLoss: 162.746641\n",
      "Train Epoch: 310 [600000/697932 (86%)]\tLoss: 160.958016\n",
      "====> Epoch: 310 Average loss: 162.2387\n",
      "====> Test set loss: 164.7031\n",
      "Train Epoch: 311 [0/697932 (0%)]\tLoss: 165.516547\n",
      "Train Epoch: 311 [100000/697932 (14%)]\tLoss: 160.839625\n",
      "Train Epoch: 311 [200000/697932 (29%)]\tLoss: 162.361766\n",
      "Train Epoch: 311 [300000/697932 (43%)]\tLoss: 163.873375\n",
      "Train Epoch: 311 [400000/697932 (57%)]\tLoss: 163.072266\n",
      "Train Epoch: 311 [500000/697932 (72%)]\tLoss: 162.774469\n",
      "Train Epoch: 311 [600000/697932 (86%)]\tLoss: 162.365984\n",
      "====> Epoch: 311 Average loss: 162.3050\n",
      "====> Test set loss: 164.6211\n",
      "Train Epoch: 312 [0/697932 (0%)]\tLoss: 161.416719\n",
      "Train Epoch: 312 [100000/697932 (14%)]\tLoss: 161.215859\n",
      "Train Epoch: 312 [200000/697932 (29%)]\tLoss: 165.116109\n",
      "Train Epoch: 312 [300000/697932 (43%)]\tLoss: 163.224187\n",
      "Train Epoch: 312 [400000/697932 (57%)]\tLoss: 161.087656\n",
      "Train Epoch: 312 [500000/697932 (72%)]\tLoss: 163.567734\n",
      "Train Epoch: 312 [600000/697932 (86%)]\tLoss: 161.589406\n",
      "====> Epoch: 312 Average loss: 162.2131\n",
      "====> Test set loss: 164.7908\n",
      "Train Epoch: 313 [0/697932 (0%)]\tLoss: 164.660328\n",
      "Train Epoch: 313 [100000/697932 (14%)]\tLoss: 164.314812\n",
      "Train Epoch: 313 [200000/697932 (29%)]\tLoss: 163.765953\n",
      "Train Epoch: 313 [300000/697932 (43%)]\tLoss: 161.037906\n",
      "Train Epoch: 313 [400000/697932 (57%)]\tLoss: 164.020797\n",
      "Train Epoch: 313 [500000/697932 (72%)]\tLoss: 162.037859\n",
      "Train Epoch: 313 [600000/697932 (86%)]\tLoss: 161.566578\n",
      "====> Epoch: 313 Average loss: 162.2828\n",
      "====> Test set loss: 165.1275\n",
      "Train Epoch: 314 [0/697932 (0%)]\tLoss: 161.781516\n",
      "Train Epoch: 314 [100000/697932 (14%)]\tLoss: 163.075344\n",
      "Train Epoch: 314 [200000/697932 (29%)]\tLoss: 161.125188\n",
      "Train Epoch: 314 [300000/697932 (43%)]\tLoss: 159.074672\n",
      "Train Epoch: 314 [400000/697932 (57%)]\tLoss: 162.918859\n",
      "Train Epoch: 314 [500000/697932 (72%)]\tLoss: 161.866328\n",
      "Train Epoch: 314 [600000/697932 (86%)]\tLoss: 162.024063\n",
      "====> Epoch: 314 Average loss: 162.2444\n",
      "====> Test set loss: 164.6782\n",
      "Train Epoch: 315 [0/697932 (0%)]\tLoss: 159.356141\n",
      "Train Epoch: 315 [100000/697932 (14%)]\tLoss: 162.258641\n",
      "Train Epoch: 315 [200000/697932 (29%)]\tLoss: 162.021828\n",
      "Train Epoch: 315 [300000/697932 (43%)]\tLoss: 159.999828\n",
      "Train Epoch: 315 [400000/697932 (57%)]\tLoss: 165.025266\n",
      "Train Epoch: 315 [500000/697932 (72%)]\tLoss: 163.144328\n",
      "Train Epoch: 315 [600000/697932 (86%)]\tLoss: 162.146531\n",
      "====> Epoch: 315 Average loss: 162.1999\n",
      "====> Test set loss: 164.8052\n",
      "Train Epoch: 316 [0/697932 (0%)]\tLoss: 163.349500\n",
      "Train Epoch: 316 [100000/697932 (14%)]\tLoss: 161.309172\n",
      "Train Epoch: 316 [200000/697932 (29%)]\tLoss: 162.319094\n",
      "Train Epoch: 316 [300000/697932 (43%)]\tLoss: 162.053469\n",
      "Train Epoch: 316 [400000/697932 (57%)]\tLoss: 159.431453\n",
      "Train Epoch: 316 [500000/697932 (72%)]\tLoss: 161.154547\n",
      "Train Epoch: 316 [600000/697932 (86%)]\tLoss: 164.120187\n",
      "====> Epoch: 316 Average loss: 162.2541\n",
      "====> Test set loss: 164.6929\n",
      "Train Epoch: 317 [0/697932 (0%)]\tLoss: 159.696812\n",
      "Train Epoch: 317 [100000/697932 (14%)]\tLoss: 162.722047\n",
      "Train Epoch: 317 [200000/697932 (29%)]\tLoss: 161.121125\n",
      "Train Epoch: 317 [300000/697932 (43%)]\tLoss: 162.151125\n",
      "Train Epoch: 317 [400000/697932 (57%)]\tLoss: 161.161859\n",
      "Train Epoch: 317 [500000/697932 (72%)]\tLoss: 163.706063\n",
      "Train Epoch: 317 [600000/697932 (86%)]\tLoss: 163.991000\n",
      "====> Epoch: 317 Average loss: 162.2117\n",
      "====> Test set loss: 164.6203\n",
      "Train Epoch: 318 [0/697932 (0%)]\tLoss: 162.094281\n",
      "Train Epoch: 318 [100000/697932 (14%)]\tLoss: 160.800984\n",
      "Train Epoch: 318 [200000/697932 (29%)]\tLoss: 161.114781\n",
      "Train Epoch: 318 [300000/697932 (43%)]\tLoss: 160.554703\n",
      "Train Epoch: 318 [400000/697932 (57%)]\tLoss: 162.675563\n",
      "Train Epoch: 318 [500000/697932 (72%)]\tLoss: 160.312641\n",
      "Train Epoch: 318 [600000/697932 (86%)]\tLoss: 161.408391\n",
      "====> Epoch: 318 Average loss: 162.0950\n",
      "====> Test set loss: 164.6375\n",
      "Train Epoch: 319 [0/697932 (0%)]\tLoss: 161.798266\n",
      "Train Epoch: 319 [100000/697932 (14%)]\tLoss: 161.201578\n",
      "Train Epoch: 319 [200000/697932 (29%)]\tLoss: 161.886562\n",
      "Train Epoch: 319 [300000/697932 (43%)]\tLoss: 161.321453\n",
      "Train Epoch: 319 [400000/697932 (57%)]\tLoss: 161.349203\n",
      "Train Epoch: 319 [500000/697932 (72%)]\tLoss: 161.729437\n",
      "Train Epoch: 319 [600000/697932 (86%)]\tLoss: 163.610719\n",
      "====> Epoch: 319 Average loss: 162.2232\n",
      "====> Test set loss: 164.7240\n",
      "Train Epoch: 320 [0/697932 (0%)]\tLoss: 160.838828\n",
      "Train Epoch: 320 [100000/697932 (14%)]\tLoss: 160.082437\n",
      "Train Epoch: 320 [200000/697932 (29%)]\tLoss: 162.883563\n",
      "Train Epoch: 320 [300000/697932 (43%)]\tLoss: 162.461516\n",
      "Train Epoch: 320 [400000/697932 (57%)]\tLoss: 161.905047\n",
      "Train Epoch: 320 [500000/697932 (72%)]\tLoss: 162.864234\n",
      "Train Epoch: 320 [600000/697932 (86%)]\tLoss: 162.341438\n",
      "====> Epoch: 320 Average loss: 162.2529\n",
      "====> Test set loss: 164.6800\n",
      "Train Epoch: 321 [0/697932 (0%)]\tLoss: 165.103953\n",
      "Train Epoch: 321 [100000/697932 (14%)]\tLoss: 159.221000\n",
      "Train Epoch: 321 [200000/697932 (29%)]\tLoss: 160.461328\n",
      "Train Epoch: 321 [300000/697932 (43%)]\tLoss: 163.848813\n",
      "Train Epoch: 321 [400000/697932 (57%)]\tLoss: 157.868125\n",
      "Train Epoch: 321 [500000/697932 (72%)]\tLoss: 162.400187\n",
      "Train Epoch: 321 [600000/697932 (86%)]\tLoss: 161.321000\n",
      "====> Epoch: 321 Average loss: 162.2955\n",
      "====> Test set loss: 164.6397\n",
      "Train Epoch: 322 [0/697932 (0%)]\tLoss: 165.867016\n",
      "Train Epoch: 322 [100000/697932 (14%)]\tLoss: 162.506844\n",
      "Train Epoch: 322 [200000/697932 (29%)]\tLoss: 162.052297\n",
      "Train Epoch: 322 [300000/697932 (43%)]\tLoss: 162.987469\n",
      "Train Epoch: 322 [400000/697932 (57%)]\tLoss: 165.484844\n",
      "Train Epoch: 322 [500000/697932 (72%)]\tLoss: 163.180328\n",
      "Train Epoch: 322 [600000/697932 (86%)]\tLoss: 161.571500\n",
      "====> Epoch: 322 Average loss: 162.2307\n",
      "====> Test set loss: 164.6906\n",
      "Train Epoch: 323 [0/697932 (0%)]\tLoss: 164.857687\n",
      "Train Epoch: 323 [100000/697932 (14%)]\tLoss: 162.080672\n",
      "Train Epoch: 323 [200000/697932 (29%)]\tLoss: 161.710531\n",
      "Train Epoch: 323 [300000/697932 (43%)]\tLoss: 160.338766\n",
      "Train Epoch: 323 [400000/697932 (57%)]\tLoss: 162.225469\n",
      "Train Epoch: 323 [500000/697932 (72%)]\tLoss: 162.672922\n",
      "Train Epoch: 323 [600000/697932 (86%)]\tLoss: 162.910828\n",
      "====> Epoch: 323 Average loss: 162.3792\n",
      "====> Test set loss: 164.8582\n",
      "Train Epoch: 324 [0/697932 (0%)]\tLoss: 161.634953\n",
      "Train Epoch: 324 [100000/697932 (14%)]\tLoss: 164.151563\n",
      "Train Epoch: 324 [200000/697932 (29%)]\tLoss: 164.433203\n",
      "Train Epoch: 324 [300000/697932 (43%)]\tLoss: 161.297375\n",
      "Train Epoch: 324 [400000/697932 (57%)]\tLoss: 161.674672\n",
      "Train Epoch: 324 [500000/697932 (72%)]\tLoss: 164.202703\n",
      "Train Epoch: 324 [600000/697932 (86%)]\tLoss: 162.080078\n",
      "====> Epoch: 324 Average loss: 162.2842\n",
      "====> Test set loss: 164.7903\n",
      "Train Epoch: 325 [0/697932 (0%)]\tLoss: 164.300906\n",
      "Train Epoch: 325 [100000/697932 (14%)]\tLoss: 161.860391\n",
      "Train Epoch: 325 [200000/697932 (29%)]\tLoss: 162.280906\n",
      "Train Epoch: 325 [300000/697932 (43%)]\tLoss: 161.116609\n",
      "Train Epoch: 325 [400000/697932 (57%)]\tLoss: 163.534203\n",
      "Train Epoch: 325 [500000/697932 (72%)]\tLoss: 162.694500\n",
      "Train Epoch: 325 [600000/697932 (86%)]\tLoss: 159.776938\n",
      "====> Epoch: 325 Average loss: 162.3523\n",
      "====> Test set loss: 164.9162\n",
      "Train Epoch: 326 [0/697932 (0%)]\tLoss: 162.887313\n",
      "Train Epoch: 326 [100000/697932 (14%)]\tLoss: 160.996078\n",
      "Train Epoch: 326 [200000/697932 (29%)]\tLoss: 160.857937\n",
      "Train Epoch: 326 [300000/697932 (43%)]\tLoss: 160.517313\n",
      "Train Epoch: 326 [400000/697932 (57%)]\tLoss: 161.813813\n",
      "Train Epoch: 326 [500000/697932 (72%)]\tLoss: 160.420234\n",
      "Train Epoch: 326 [600000/697932 (86%)]\tLoss: 161.721500\n",
      "====> Epoch: 326 Average loss: 162.2723\n",
      "====> Test set loss: 164.9781\n",
      "Train Epoch: 327 [0/697932 (0%)]\tLoss: 161.726828\n",
      "Train Epoch: 327 [100000/697932 (14%)]\tLoss: 162.910859\n",
      "Train Epoch: 327 [200000/697932 (29%)]\tLoss: 161.965234\n",
      "Train Epoch: 327 [300000/697932 (43%)]\tLoss: 160.694906\n",
      "Train Epoch: 327 [400000/697932 (57%)]\tLoss: 164.048594\n",
      "Train Epoch: 327 [500000/697932 (72%)]\tLoss: 161.694875\n",
      "Train Epoch: 327 [600000/697932 (86%)]\tLoss: 161.477313\n",
      "====> Epoch: 327 Average loss: 162.2020\n",
      "====> Test set loss: 164.6259\n",
      "Train Epoch: 328 [0/697932 (0%)]\tLoss: 157.256016\n",
      "Train Epoch: 328 [100000/697932 (14%)]\tLoss: 159.428859\n",
      "Train Epoch: 328 [200000/697932 (29%)]\tLoss: 161.691984\n",
      "Train Epoch: 328 [300000/697932 (43%)]\tLoss: 163.032875\n",
      "Train Epoch: 328 [400000/697932 (57%)]\tLoss: 161.029969\n",
      "Train Epoch: 328 [500000/697932 (72%)]\tLoss: 163.773734\n",
      "Train Epoch: 328 [600000/697932 (86%)]\tLoss: 161.295719\n",
      "====> Epoch: 328 Average loss: 162.1591\n",
      "====> Test set loss: 164.8584\n",
      "Train Epoch: 329 [0/697932 (0%)]\tLoss: 160.312109\n",
      "Train Epoch: 329 [100000/697932 (14%)]\tLoss: 161.468531\n",
      "Train Epoch: 329 [200000/697932 (29%)]\tLoss: 159.263656\n",
      "Train Epoch: 329 [300000/697932 (43%)]\tLoss: 163.460094\n",
      "Train Epoch: 329 [400000/697932 (57%)]\tLoss: 158.727422\n",
      "Train Epoch: 329 [500000/697932 (72%)]\tLoss: 161.596531\n",
      "Train Epoch: 329 [600000/697932 (86%)]\tLoss: 163.446109\n",
      "====> Epoch: 329 Average loss: 162.1289\n",
      "====> Test set loss: 164.6162\n",
      "Train Epoch: 330 [0/697932 (0%)]\tLoss: 159.487859\n",
      "Train Epoch: 330 [100000/697932 (14%)]\tLoss: 162.955359\n",
      "Train Epoch: 330 [200000/697932 (29%)]\tLoss: 160.745078\n",
      "Train Epoch: 330 [300000/697932 (43%)]\tLoss: 159.032047\n",
      "Train Epoch: 330 [400000/697932 (57%)]\tLoss: 163.499063\n",
      "Train Epoch: 330 [500000/697932 (72%)]\tLoss: 161.942375\n",
      "Train Epoch: 330 [600000/697932 (86%)]\tLoss: 161.803328\n",
      "====> Epoch: 330 Average loss: 162.1482\n",
      "====> Test set loss: 164.8220\n",
      "Train Epoch: 331 [0/697932 (0%)]\tLoss: 161.453828\n",
      "Train Epoch: 331 [100000/697932 (14%)]\tLoss: 161.738203\n",
      "Train Epoch: 331 [200000/697932 (29%)]\tLoss: 163.575250\n",
      "Train Epoch: 331 [300000/697932 (43%)]\tLoss: 163.827734\n",
      "Train Epoch: 331 [400000/697932 (57%)]\tLoss: 162.961672\n",
      "Train Epoch: 331 [500000/697932 (72%)]\tLoss: 163.819359\n",
      "Train Epoch: 331 [600000/697932 (86%)]\tLoss: 160.354281\n",
      "====> Epoch: 331 Average loss: 162.1232\n",
      "====> Test set loss: 164.6080\n",
      "Train Epoch: 332 [0/697932 (0%)]\tLoss: 160.760469\n",
      "Train Epoch: 332 [100000/697932 (14%)]\tLoss: 162.068828\n",
      "Train Epoch: 332 [200000/697932 (29%)]\tLoss: 160.821906\n",
      "Train Epoch: 332 [300000/697932 (43%)]\tLoss: 163.668328\n",
      "Train Epoch: 332 [400000/697932 (57%)]\tLoss: 162.726203\n",
      "Train Epoch: 332 [500000/697932 (72%)]\tLoss: 161.737750\n",
      "Train Epoch: 332 [600000/697932 (86%)]\tLoss: 166.194609\n",
      "====> Epoch: 332 Average loss: 162.1263\n",
      "====> Test set loss: 164.7701\n",
      "Train Epoch: 333 [0/697932 (0%)]\tLoss: 163.504844\n",
      "Train Epoch: 333 [100000/697932 (14%)]\tLoss: 162.312250\n",
      "Train Epoch: 333 [200000/697932 (29%)]\tLoss: 160.859797\n",
      "Train Epoch: 333 [300000/697932 (43%)]\tLoss: 163.623172\n",
      "Train Epoch: 333 [400000/697932 (57%)]\tLoss: 159.737031\n",
      "Train Epoch: 333 [500000/697932 (72%)]\tLoss: 162.844328\n",
      "Train Epoch: 333 [600000/697932 (86%)]\tLoss: 162.228891\n",
      "====> Epoch: 333 Average loss: 162.0648\n",
      "====> Test set loss: 164.6377\n",
      "Train Epoch: 334 [0/697932 (0%)]\tLoss: 163.248844\n",
      "Train Epoch: 334 [100000/697932 (14%)]\tLoss: 162.058969\n",
      "Train Epoch: 334 [200000/697932 (29%)]\tLoss: 159.474797\n",
      "Train Epoch: 334 [300000/697932 (43%)]\tLoss: 162.851531\n",
      "Train Epoch: 334 [400000/697932 (57%)]\tLoss: 161.874562\n",
      "Train Epoch: 334 [500000/697932 (72%)]\tLoss: 164.253187\n",
      "Train Epoch: 334 [600000/697932 (86%)]\tLoss: 165.534531\n",
      "====> Epoch: 334 Average loss: 162.1620\n",
      "====> Test set loss: 164.6455\n",
      "Train Epoch: 335 [0/697932 (0%)]\tLoss: 159.902797\n",
      "Train Epoch: 335 [100000/697932 (14%)]\tLoss: 163.498844\n",
      "Train Epoch: 335 [200000/697932 (29%)]\tLoss: 162.157297\n",
      "Train Epoch: 335 [300000/697932 (43%)]\tLoss: 161.069203\n",
      "Train Epoch: 335 [400000/697932 (57%)]\tLoss: 162.045531\n",
      "Train Epoch: 335 [500000/697932 (72%)]\tLoss: 162.697250\n",
      "Train Epoch: 335 [600000/697932 (86%)]\tLoss: 160.291313\n",
      "====> Epoch: 335 Average loss: 162.1882\n",
      "====> Test set loss: 164.7494\n",
      "Train Epoch: 336 [0/697932 (0%)]\tLoss: 163.403484\n",
      "Train Epoch: 336 [100000/697932 (14%)]\tLoss: 162.732828\n",
      "Train Epoch: 336 [200000/697932 (29%)]\tLoss: 160.696891\n",
      "Train Epoch: 336 [300000/697932 (43%)]\tLoss: 162.811594\n",
      "Train Epoch: 336 [400000/697932 (57%)]\tLoss: 162.843828\n",
      "Train Epoch: 336 [500000/697932 (72%)]\tLoss: 164.566328\n",
      "Train Epoch: 336 [600000/697932 (86%)]\tLoss: 164.363844\n",
      "====> Epoch: 336 Average loss: 162.1130\n",
      "====> Test set loss: 164.6220\n",
      "Train Epoch: 337 [0/697932 (0%)]\tLoss: 161.603578\n",
      "Train Epoch: 337 [100000/697932 (14%)]\tLoss: 162.381828\n",
      "Train Epoch: 337 [200000/697932 (29%)]\tLoss: 162.290469\n",
      "Train Epoch: 337 [300000/697932 (43%)]\tLoss: 161.321078\n",
      "Train Epoch: 337 [400000/697932 (57%)]\tLoss: 162.359172\n",
      "Train Epoch: 337 [500000/697932 (72%)]\tLoss: 162.106000\n",
      "Train Epoch: 337 [600000/697932 (86%)]\tLoss: 158.519016\n",
      "====> Epoch: 337 Average loss: 162.1390\n",
      "====> Test set loss: 164.6450\n",
      "Train Epoch: 338 [0/697932 (0%)]\tLoss: 160.323500\n",
      "Train Epoch: 338 [100000/697932 (14%)]\tLoss: 162.106313\n",
      "Train Epoch: 338 [200000/697932 (29%)]\tLoss: 161.447234\n",
      "Train Epoch: 338 [300000/697932 (43%)]\tLoss: 162.733797\n",
      "Train Epoch: 338 [400000/697932 (57%)]\tLoss: 162.079344\n",
      "Train Epoch: 338 [500000/697932 (72%)]\tLoss: 162.534562\n",
      "Train Epoch: 338 [600000/697932 (86%)]\tLoss: 160.451984\n",
      "====> Epoch: 338 Average loss: 162.1796\n",
      "====> Test set loss: 164.6956\n",
      "Train Epoch: 339 [0/697932 (0%)]\tLoss: 160.234953\n",
      "Train Epoch: 339 [100000/697932 (14%)]\tLoss: 161.209781\n",
      "Train Epoch: 339 [200000/697932 (29%)]\tLoss: 161.565313\n",
      "Train Epoch: 339 [300000/697932 (43%)]\tLoss: 161.684562\n",
      "Train Epoch: 339 [400000/697932 (57%)]\tLoss: 161.747359\n",
      "Train Epoch: 339 [500000/697932 (72%)]\tLoss: 162.013500\n",
      "Train Epoch: 339 [600000/697932 (86%)]\tLoss: 163.074656\n",
      "====> Epoch: 339 Average loss: 162.1175\n",
      "====> Test set loss: 164.8458\n",
      "Train Epoch: 340 [0/697932 (0%)]\tLoss: 160.039109\n",
      "Train Epoch: 340 [100000/697932 (14%)]\tLoss: 161.035641\n",
      "Train Epoch: 340 [200000/697932 (29%)]\tLoss: 159.438141\n",
      "Train Epoch: 340 [300000/697932 (43%)]\tLoss: 161.615609\n",
      "Train Epoch: 340 [400000/697932 (57%)]\tLoss: 162.923625\n",
      "Train Epoch: 340 [500000/697932 (72%)]\tLoss: 159.933812\n",
      "Train Epoch: 340 [600000/697932 (86%)]\tLoss: 163.984047\n",
      "====> Epoch: 340 Average loss: 162.1357\n",
      "====> Test set loss: 164.8283\n",
      "Train Epoch: 341 [0/697932 (0%)]\tLoss: 161.597187\n",
      "Train Epoch: 341 [100000/697932 (14%)]\tLoss: 164.769016\n",
      "Train Epoch: 341 [200000/697932 (29%)]\tLoss: 160.006812\n",
      "Train Epoch: 341 [300000/697932 (43%)]\tLoss: 163.208141\n",
      "Train Epoch: 341 [400000/697932 (57%)]\tLoss: 161.593891\n",
      "Train Epoch: 341 [500000/697932 (72%)]\tLoss: 161.027203\n",
      "Train Epoch: 341 [600000/697932 (86%)]\tLoss: 162.090594\n",
      "====> Epoch: 341 Average loss: 162.1657\n",
      "====> Test set loss: 164.5488\n",
      "Train Epoch: 342 [0/697932 (0%)]\tLoss: 160.496422\n",
      "Train Epoch: 342 [100000/697932 (14%)]\tLoss: 164.135281\n",
      "Train Epoch: 342 [200000/697932 (29%)]\tLoss: 161.064687\n",
      "Train Epoch: 342 [300000/697932 (43%)]\tLoss: 163.761859\n",
      "Train Epoch: 342 [400000/697932 (57%)]\tLoss: 162.934734\n",
      "Train Epoch: 342 [500000/697932 (72%)]\tLoss: 163.829875\n",
      "Train Epoch: 342 [600000/697932 (86%)]\tLoss: 162.400844\n",
      "====> Epoch: 342 Average loss: 162.1576\n",
      "====> Test set loss: 164.7009\n",
      "Train Epoch: 343 [0/697932 (0%)]\tLoss: 162.598437\n",
      "Train Epoch: 343 [100000/697932 (14%)]\tLoss: 162.251953\n",
      "Train Epoch: 343 [200000/697932 (29%)]\tLoss: 158.820344\n",
      "Train Epoch: 343 [300000/697932 (43%)]\tLoss: 160.200437\n",
      "Train Epoch: 343 [400000/697932 (57%)]\tLoss: 160.562875\n",
      "Train Epoch: 343 [500000/697932 (72%)]\tLoss: 161.122062\n",
      "Train Epoch: 343 [600000/697932 (86%)]\tLoss: 163.490734\n",
      "====> Epoch: 343 Average loss: 162.1602\n",
      "====> Test set loss: 164.6612\n",
      "Train Epoch: 344 [0/697932 (0%)]\tLoss: 161.976234\n",
      "Train Epoch: 344 [100000/697932 (14%)]\tLoss: 162.925734\n",
      "Train Epoch: 344 [200000/697932 (29%)]\tLoss: 159.945531\n",
      "Train Epoch: 344 [300000/697932 (43%)]\tLoss: 160.713437\n",
      "Train Epoch: 344 [400000/697932 (57%)]\tLoss: 163.462750\n",
      "Train Epoch: 344 [500000/697932 (72%)]\tLoss: 162.330516\n",
      "Train Epoch: 344 [600000/697932 (86%)]\tLoss: 162.054656\n",
      "====> Epoch: 344 Average loss: 162.1105\n",
      "====> Test set loss: 164.6766\n",
      "Train Epoch: 345 [0/697932 (0%)]\tLoss: 161.066344\n",
      "Train Epoch: 345 [100000/697932 (14%)]\tLoss: 162.037703\n",
      "Train Epoch: 345 [200000/697932 (29%)]\tLoss: 163.315641\n",
      "Train Epoch: 345 [300000/697932 (43%)]\tLoss: 157.100406\n",
      "Train Epoch: 345 [400000/697932 (57%)]\tLoss: 163.541109\n",
      "Train Epoch: 345 [500000/697932 (72%)]\tLoss: 160.765641\n",
      "Train Epoch: 345 [600000/697932 (86%)]\tLoss: 161.407953\n",
      "====> Epoch: 345 Average loss: 162.0604\n",
      "====> Test set loss: 164.6862\n",
      "Train Epoch: 346 [0/697932 (0%)]\tLoss: 160.889656\n",
      "Train Epoch: 346 [100000/697932 (14%)]\tLoss: 160.411156\n",
      "Train Epoch: 346 [200000/697932 (29%)]\tLoss: 164.569984\n",
      "Train Epoch: 346 [300000/697932 (43%)]\tLoss: 164.296594\n",
      "Train Epoch: 346 [400000/697932 (57%)]\tLoss: 161.985453\n",
      "Train Epoch: 346 [500000/697932 (72%)]\tLoss: 161.391844\n",
      "Train Epoch: 346 [600000/697932 (86%)]\tLoss: 161.113656\n",
      "====> Epoch: 346 Average loss: 162.0977\n",
      "====> Test set loss: 164.4770\n",
      "Train Epoch: 347 [0/697932 (0%)]\tLoss: 161.917844\n",
      "Train Epoch: 347 [100000/697932 (14%)]\tLoss: 161.988484\n",
      "Train Epoch: 347 [200000/697932 (29%)]\tLoss: 158.873141\n",
      "Train Epoch: 347 [300000/697932 (43%)]\tLoss: 163.005641\n",
      "Train Epoch: 347 [400000/697932 (57%)]\tLoss: 160.233219\n",
      "Train Epoch: 347 [500000/697932 (72%)]\tLoss: 163.681281\n",
      "Train Epoch: 347 [600000/697932 (86%)]\tLoss: 161.849281\n",
      "====> Epoch: 347 Average loss: 162.1179\n",
      "====> Test set loss: 164.7005\n",
      "Train Epoch: 348 [0/697932 (0%)]\tLoss: 162.444797\n",
      "Train Epoch: 348 [100000/697932 (14%)]\tLoss: 162.129391\n",
      "Train Epoch: 348 [200000/697932 (29%)]\tLoss: 161.509234\n",
      "Train Epoch: 348 [300000/697932 (43%)]\tLoss: 163.478312\n",
      "Train Epoch: 348 [400000/697932 (57%)]\tLoss: 160.078516\n",
      "Train Epoch: 348 [500000/697932 (72%)]\tLoss: 161.101359\n",
      "Train Epoch: 348 [600000/697932 (86%)]\tLoss: 161.241375\n",
      "====> Epoch: 348 Average loss: 162.1389\n",
      "====> Test set loss: 164.6109\n",
      "Train Epoch: 349 [0/697932 (0%)]\tLoss: 161.992266\n",
      "Train Epoch: 349 [100000/697932 (14%)]\tLoss: 160.041750\n",
      "Train Epoch: 349 [200000/697932 (29%)]\tLoss: 165.321922\n",
      "Train Epoch: 349 [300000/697932 (43%)]\tLoss: 161.818453\n",
      "Train Epoch: 349 [400000/697932 (57%)]\tLoss: 162.071188\n",
      "Train Epoch: 349 [500000/697932 (72%)]\tLoss: 160.751500\n",
      "Train Epoch: 349 [600000/697932 (86%)]\tLoss: 163.710266\n",
      "====> Epoch: 349 Average loss: 161.9969\n",
      "====> Test set loss: 164.7131\n",
      "Train Epoch: 350 [0/697932 (0%)]\tLoss: 162.208562\n",
      "Train Epoch: 350 [100000/697932 (14%)]\tLoss: 161.442062\n",
      "Train Epoch: 350 [200000/697932 (29%)]\tLoss: 161.496813\n",
      "Train Epoch: 350 [300000/697932 (43%)]\tLoss: 161.748672\n",
      "Train Epoch: 350 [400000/697932 (57%)]\tLoss: 161.076609\n",
      "Train Epoch: 350 [500000/697932 (72%)]\tLoss: 163.388422\n",
      "Train Epoch: 350 [600000/697932 (86%)]\tLoss: 160.357953\n",
      "====> Epoch: 350 Average loss: 161.9810\n",
      "====> Test set loss: 164.7117\n",
      "Train Epoch: 351 [0/697932 (0%)]\tLoss: 160.851063\n",
      "Train Epoch: 351 [100000/697932 (14%)]\tLoss: 158.553453\n",
      "Train Epoch: 351 [200000/697932 (29%)]\tLoss: 163.019656\n",
      "Train Epoch: 351 [300000/697932 (43%)]\tLoss: 160.995031\n",
      "Train Epoch: 351 [400000/697932 (57%)]\tLoss: 162.815781\n",
      "Train Epoch: 351 [500000/697932 (72%)]\tLoss: 163.366125\n",
      "Train Epoch: 351 [600000/697932 (86%)]\tLoss: 162.170047\n",
      "====> Epoch: 351 Average loss: 161.9756\n",
      "====> Test set loss: 164.5545\n",
      "Train Epoch: 352 [0/697932 (0%)]\tLoss: 163.156250\n",
      "Train Epoch: 352 [100000/697932 (14%)]\tLoss: 161.885609\n",
      "Train Epoch: 352 [200000/697932 (29%)]\tLoss: 160.430625\n",
      "Train Epoch: 352 [300000/697932 (43%)]\tLoss: 161.853234\n",
      "Train Epoch: 352 [400000/697932 (57%)]\tLoss: 162.501453\n",
      "Train Epoch: 352 [500000/697932 (72%)]\tLoss: 163.689500\n",
      "Train Epoch: 352 [600000/697932 (86%)]\tLoss: 163.532703\n",
      "====> Epoch: 352 Average loss: 162.0343\n",
      "====> Test set loss: 164.5255\n",
      "Train Epoch: 353 [0/697932 (0%)]\tLoss: 160.722828\n",
      "Train Epoch: 353 [100000/697932 (14%)]\tLoss: 161.126688\n",
      "Train Epoch: 353 [200000/697932 (29%)]\tLoss: 161.844172\n",
      "Train Epoch: 353 [300000/697932 (43%)]\tLoss: 164.044516\n",
      "Train Epoch: 353 [400000/697932 (57%)]\tLoss: 164.283359\n",
      "Train Epoch: 353 [500000/697932 (72%)]\tLoss: 162.154250\n",
      "Train Epoch: 353 [600000/697932 (86%)]\tLoss: 162.497562\n",
      "====> Epoch: 353 Average loss: 162.1146\n",
      "====> Test set loss: 164.6215\n",
      "Train Epoch: 354 [0/697932 (0%)]\tLoss: 159.378750\n",
      "Train Epoch: 354 [100000/697932 (14%)]\tLoss: 160.629594\n",
      "Train Epoch: 354 [200000/697932 (29%)]\tLoss: 161.934438\n",
      "Train Epoch: 354 [300000/697932 (43%)]\tLoss: 161.291578\n",
      "Train Epoch: 354 [400000/697932 (57%)]\tLoss: 161.405766\n",
      "Train Epoch: 354 [500000/697932 (72%)]\tLoss: 162.318734\n",
      "Train Epoch: 354 [600000/697932 (86%)]\tLoss: 165.208328\n",
      "====> Epoch: 354 Average loss: 162.0719\n",
      "====> Test set loss: 164.4566\n",
      "Train Epoch: 355 [0/697932 (0%)]\tLoss: 159.176312\n",
      "Train Epoch: 355 [100000/697932 (14%)]\tLoss: 164.299844\n",
      "Train Epoch: 355 [200000/697932 (29%)]\tLoss: 162.119516\n",
      "Train Epoch: 355 [300000/697932 (43%)]\tLoss: 163.168750\n",
      "Train Epoch: 355 [400000/697932 (57%)]\tLoss: 161.976484\n",
      "Train Epoch: 355 [500000/697932 (72%)]\tLoss: 162.944031\n",
      "Train Epoch: 355 [600000/697932 (86%)]\tLoss: 160.417656\n",
      "====> Epoch: 355 Average loss: 161.9580\n",
      "====> Test set loss: 164.5137\n",
      "Train Epoch: 356 [0/697932 (0%)]\tLoss: 161.925344\n",
      "Train Epoch: 356 [100000/697932 (14%)]\tLoss: 161.446094\n",
      "Train Epoch: 356 [200000/697932 (29%)]\tLoss: 158.899188\n",
      "Train Epoch: 356 [300000/697932 (43%)]\tLoss: 159.760516\n",
      "Train Epoch: 356 [400000/697932 (57%)]\tLoss: 162.979234\n",
      "Train Epoch: 356 [500000/697932 (72%)]\tLoss: 163.898000\n",
      "Train Epoch: 356 [600000/697932 (86%)]\tLoss: 162.620578\n",
      "====> Epoch: 356 Average loss: 162.0348\n",
      "====> Test set loss: 164.7554\n",
      "Train Epoch: 357 [0/697932 (0%)]\tLoss: 160.792172\n",
      "Train Epoch: 357 [100000/697932 (14%)]\tLoss: 163.128750\n",
      "Train Epoch: 357 [200000/697932 (29%)]\tLoss: 162.259172\n",
      "Train Epoch: 357 [300000/697932 (43%)]\tLoss: 162.301063\n",
      "Train Epoch: 357 [400000/697932 (57%)]\tLoss: 160.939531\n",
      "Train Epoch: 357 [500000/697932 (72%)]\tLoss: 162.082234\n",
      "Train Epoch: 357 [600000/697932 (86%)]\tLoss: 161.046984\n",
      "====> Epoch: 357 Average loss: 162.0847\n",
      "====> Test set loss: 164.6989\n",
      "Train Epoch: 358 [0/697932 (0%)]\tLoss: 163.280516\n",
      "Train Epoch: 358 [100000/697932 (14%)]\tLoss: 161.234766\n",
      "Train Epoch: 358 [200000/697932 (29%)]\tLoss: 162.982313\n",
      "Train Epoch: 358 [300000/697932 (43%)]\tLoss: 163.464172\n",
      "Train Epoch: 358 [400000/697932 (57%)]\tLoss: 162.573266\n",
      "Train Epoch: 358 [500000/697932 (72%)]\tLoss: 163.057703\n",
      "Train Epoch: 358 [600000/697932 (86%)]\tLoss: 163.148000\n",
      "====> Epoch: 358 Average loss: 162.0400\n",
      "====> Test set loss: 164.6993\n",
      "Train Epoch: 359 [0/697932 (0%)]\tLoss: 163.423047\n",
      "Train Epoch: 359 [100000/697932 (14%)]\tLoss: 165.314125\n",
      "Train Epoch: 359 [200000/697932 (29%)]\tLoss: 161.282250\n",
      "Train Epoch: 359 [300000/697932 (43%)]\tLoss: 163.496406\n",
      "Train Epoch: 359 [400000/697932 (57%)]\tLoss: 162.517984\n",
      "Train Epoch: 359 [500000/697932 (72%)]\tLoss: 158.798172\n",
      "Train Epoch: 359 [600000/697932 (86%)]\tLoss: 159.378062\n",
      "====> Epoch: 359 Average loss: 162.0709\n",
      "====> Test set loss: 164.5135\n",
      "Train Epoch: 360 [0/697932 (0%)]\tLoss: 161.629719\n",
      "Train Epoch: 360 [100000/697932 (14%)]\tLoss: 159.429047\n",
      "Train Epoch: 360 [200000/697932 (29%)]\tLoss: 160.935844\n",
      "Train Epoch: 360 [300000/697932 (43%)]\tLoss: 159.813422\n",
      "Train Epoch: 360 [400000/697932 (57%)]\tLoss: 161.949813\n",
      "Train Epoch: 360 [500000/697932 (72%)]\tLoss: 161.270219\n",
      "Train Epoch: 360 [600000/697932 (86%)]\tLoss: 164.291000\n",
      "====> Epoch: 360 Average loss: 162.0383\n",
      "====> Test set loss: 164.5606\n",
      "Train Epoch: 361 [0/697932 (0%)]\tLoss: 162.761359\n",
      "Train Epoch: 361 [100000/697932 (14%)]\tLoss: 162.262969\n",
      "Train Epoch: 361 [200000/697932 (29%)]\tLoss: 162.951313\n",
      "Train Epoch: 361 [300000/697932 (43%)]\tLoss: 160.439281\n",
      "Train Epoch: 361 [400000/697932 (57%)]\tLoss: 160.687203\n",
      "Train Epoch: 361 [500000/697932 (72%)]\tLoss: 162.500578\n",
      "Train Epoch: 361 [600000/697932 (86%)]\tLoss: 165.179719\n",
      "====> Epoch: 361 Average loss: 162.0753\n",
      "====> Test set loss: 164.6365\n",
      "Train Epoch: 362 [0/697932 (0%)]\tLoss: 161.091891\n",
      "Train Epoch: 362 [100000/697932 (14%)]\tLoss: 160.516750\n",
      "Train Epoch: 362 [200000/697932 (29%)]\tLoss: 162.418094\n",
      "Train Epoch: 362 [300000/697932 (43%)]\tLoss: 160.372781\n",
      "Train Epoch: 362 [400000/697932 (57%)]\tLoss: 163.065359\n",
      "Train Epoch: 362 [500000/697932 (72%)]\tLoss: 159.727109\n",
      "Train Epoch: 362 [600000/697932 (86%)]\tLoss: 159.418844\n",
      "====> Epoch: 362 Average loss: 162.0517\n",
      "====> Test set loss: 164.6078\n",
      "Train Epoch: 363 [0/697932 (0%)]\tLoss: 160.695859\n",
      "Train Epoch: 363 [100000/697932 (14%)]\tLoss: 162.506156\n",
      "Train Epoch: 363 [200000/697932 (29%)]\tLoss: 161.690328\n",
      "Train Epoch: 363 [300000/697932 (43%)]\tLoss: 160.486781\n",
      "Train Epoch: 363 [400000/697932 (57%)]\tLoss: 162.822422\n",
      "Train Epoch: 363 [500000/697932 (72%)]\tLoss: 161.573797\n",
      "Train Epoch: 363 [600000/697932 (86%)]\tLoss: 161.322328\n",
      "====> Epoch: 363 Average loss: 162.0118\n",
      "====> Test set loss: 164.7560\n",
      "Train Epoch: 364 [0/697932 (0%)]\tLoss: 162.038250\n",
      "Train Epoch: 364 [100000/697932 (14%)]\tLoss: 163.902203\n",
      "Train Epoch: 364 [200000/697932 (29%)]\tLoss: 163.671578\n",
      "Train Epoch: 364 [300000/697932 (43%)]\tLoss: 161.634906\n",
      "Train Epoch: 364 [400000/697932 (57%)]\tLoss: 162.275094\n",
      "Train Epoch: 364 [500000/697932 (72%)]\tLoss: 162.424797\n",
      "Train Epoch: 364 [600000/697932 (86%)]\tLoss: 161.294453\n",
      "====> Epoch: 364 Average loss: 162.1142\n",
      "====> Test set loss: 164.8579\n",
      "Train Epoch: 365 [0/697932 (0%)]\tLoss: 159.305281\n",
      "Train Epoch: 365 [100000/697932 (14%)]\tLoss: 161.510047\n",
      "Train Epoch: 365 [200000/697932 (29%)]\tLoss: 164.744422\n",
      "Train Epoch: 365 [300000/697932 (43%)]\tLoss: 161.781281\n",
      "Train Epoch: 365 [400000/697932 (57%)]\tLoss: 164.796875\n",
      "Train Epoch: 365 [500000/697932 (72%)]\tLoss: 161.582719\n",
      "Train Epoch: 365 [600000/697932 (86%)]\tLoss: 163.240781\n",
      "====> Epoch: 365 Average loss: 162.0821\n",
      "====> Test set loss: 164.8454\n",
      "Train Epoch: 366 [0/697932 (0%)]\tLoss: 164.050828\n",
      "Train Epoch: 366 [100000/697932 (14%)]\tLoss: 161.223078\n",
      "Train Epoch: 366 [200000/697932 (29%)]\tLoss: 159.551719\n",
      "Train Epoch: 366 [300000/697932 (43%)]\tLoss: 161.169437\n",
      "Train Epoch: 366 [400000/697932 (57%)]\tLoss: 165.042234\n",
      "Train Epoch: 366 [500000/697932 (72%)]\tLoss: 164.266938\n",
      "Train Epoch: 366 [600000/697932 (86%)]\tLoss: 162.801094\n",
      "====> Epoch: 366 Average loss: 161.9980\n",
      "====> Test set loss: 164.4950\n",
      "Train Epoch: 367 [0/697932 (0%)]\tLoss: 164.086469\n",
      "Train Epoch: 367 [100000/697932 (14%)]\tLoss: 159.558922\n",
      "Train Epoch: 367 [200000/697932 (29%)]\tLoss: 160.826859\n",
      "Train Epoch: 367 [300000/697932 (43%)]\tLoss: 163.633234\n",
      "Train Epoch: 367 [400000/697932 (57%)]\tLoss: 162.088594\n",
      "Train Epoch: 367 [500000/697932 (72%)]\tLoss: 160.622875\n",
      "Train Epoch: 367 [600000/697932 (86%)]\tLoss: 161.297172\n",
      "====> Epoch: 367 Average loss: 161.9198\n",
      "====> Test set loss: 164.5868\n",
      "Train Epoch: 368 [0/697932 (0%)]\tLoss: 160.198922\n",
      "Train Epoch: 368 [100000/697932 (14%)]\tLoss: 163.619438\n",
      "Train Epoch: 368 [200000/697932 (29%)]\tLoss: 161.945328\n",
      "Train Epoch: 368 [300000/697932 (43%)]\tLoss: 162.329766\n",
      "Train Epoch: 368 [400000/697932 (57%)]\tLoss: 160.760078\n",
      "Train Epoch: 368 [500000/697932 (72%)]\tLoss: 160.105172\n",
      "Train Epoch: 368 [600000/697932 (86%)]\tLoss: 161.796375\n",
      "====> Epoch: 368 Average loss: 161.8797\n",
      "====> Test set loss: 164.4830\n",
      "Train Epoch: 369 [0/697932 (0%)]\tLoss: 161.587344\n",
      "Train Epoch: 369 [100000/697932 (14%)]\tLoss: 164.203078\n",
      "Train Epoch: 369 [200000/697932 (29%)]\tLoss: 165.871938\n",
      "Train Epoch: 369 [300000/697932 (43%)]\tLoss: 160.166359\n",
      "Train Epoch: 369 [400000/697932 (57%)]\tLoss: 161.628984\n",
      "Train Epoch: 369 [500000/697932 (72%)]\tLoss: 161.879953\n",
      "Train Epoch: 369 [600000/697932 (86%)]\tLoss: 160.016609\n",
      "====> Epoch: 369 Average loss: 162.0285\n",
      "====> Test set loss: 164.6914\n",
      "Train Epoch: 370 [0/697932 (0%)]\tLoss: 163.685359\n",
      "Train Epoch: 370 [100000/697932 (14%)]\tLoss: 160.307906\n",
      "Train Epoch: 370 [200000/697932 (29%)]\tLoss: 162.896000\n",
      "Train Epoch: 370 [300000/697932 (43%)]\tLoss: 159.143859\n",
      "Train Epoch: 370 [400000/697932 (57%)]\tLoss: 159.767437\n",
      "Train Epoch: 370 [500000/697932 (72%)]\tLoss: 162.385063\n",
      "Train Epoch: 370 [600000/697932 (86%)]\tLoss: 164.858937\n",
      "====> Epoch: 370 Average loss: 162.0123\n",
      "====> Test set loss: 164.5499\n",
      "Train Epoch: 371 [0/697932 (0%)]\tLoss: 164.005984\n",
      "Train Epoch: 371 [100000/697932 (14%)]\tLoss: 162.463922\n",
      "Train Epoch: 371 [200000/697932 (29%)]\tLoss: 161.800328\n",
      "Train Epoch: 371 [300000/697932 (43%)]\tLoss: 159.656734\n",
      "Train Epoch: 371 [400000/697932 (57%)]\tLoss: 161.876953\n",
      "Train Epoch: 371 [500000/697932 (72%)]\tLoss: 162.967656\n",
      "Train Epoch: 371 [600000/697932 (86%)]\tLoss: 162.464109\n",
      "====> Epoch: 371 Average loss: 162.0699\n",
      "====> Test set loss: 164.5976\n",
      "Train Epoch: 372 [0/697932 (0%)]\tLoss: 164.123859\n",
      "Train Epoch: 372 [100000/697932 (14%)]\tLoss: 159.862844\n",
      "Train Epoch: 372 [200000/697932 (29%)]\tLoss: 161.417484\n",
      "Train Epoch: 372 [300000/697932 (43%)]\tLoss: 162.765500\n",
      "Train Epoch: 372 [400000/697932 (57%)]\tLoss: 161.845922\n",
      "Train Epoch: 372 [500000/697932 (72%)]\tLoss: 162.771062\n",
      "Train Epoch: 372 [600000/697932 (86%)]\tLoss: 158.461578\n",
      "====> Epoch: 372 Average loss: 162.0327\n",
      "====> Test set loss: 164.4690\n",
      "Train Epoch: 373 [0/697932 (0%)]\tLoss: 162.964937\n",
      "Train Epoch: 373 [100000/697932 (14%)]\tLoss: 161.941812\n",
      "Train Epoch: 373 [200000/697932 (29%)]\tLoss: 163.119641\n",
      "Train Epoch: 373 [300000/697932 (43%)]\tLoss: 159.462187\n",
      "Train Epoch: 373 [400000/697932 (57%)]\tLoss: 161.748078\n",
      "Train Epoch: 373 [500000/697932 (72%)]\tLoss: 163.934609\n",
      "Train Epoch: 373 [600000/697932 (86%)]\tLoss: 161.032906\n",
      "====> Epoch: 373 Average loss: 161.8309\n",
      "====> Test set loss: 164.6308\n",
      "Train Epoch: 374 [0/697932 (0%)]\tLoss: 165.898516\n",
      "Train Epoch: 374 [100000/697932 (14%)]\tLoss: 163.167547\n",
      "Train Epoch: 374 [200000/697932 (29%)]\tLoss: 161.716375\n",
      "Train Epoch: 374 [300000/697932 (43%)]\tLoss: 160.996312\n",
      "Train Epoch: 374 [400000/697932 (57%)]\tLoss: 159.748781\n",
      "Train Epoch: 374 [500000/697932 (72%)]\tLoss: 161.194000\n",
      "Train Epoch: 374 [600000/697932 (86%)]\tLoss: 161.376313\n",
      "====> Epoch: 374 Average loss: 161.8858\n",
      "====> Test set loss: 164.6047\n",
      "Train Epoch: 375 [0/697932 (0%)]\tLoss: 163.929625\n",
      "Train Epoch: 375 [100000/697932 (14%)]\tLoss: 164.046562\n",
      "Train Epoch: 375 [200000/697932 (29%)]\tLoss: 160.117656\n",
      "Train Epoch: 375 [300000/697932 (43%)]\tLoss: 162.416500\n",
      "Train Epoch: 375 [400000/697932 (57%)]\tLoss: 162.352422\n",
      "Train Epoch: 375 [500000/697932 (72%)]\tLoss: 161.573609\n",
      "Train Epoch: 375 [600000/697932 (86%)]\tLoss: 162.427719\n",
      "====> Epoch: 375 Average loss: 161.9360\n",
      "====> Test set loss: 164.7115\n",
      "Train Epoch: 376 [0/697932 (0%)]\tLoss: 161.444219\n",
      "Train Epoch: 376 [100000/697932 (14%)]\tLoss: 165.317656\n",
      "Train Epoch: 376 [200000/697932 (29%)]\tLoss: 160.494750\n",
      "Train Epoch: 376 [300000/697932 (43%)]\tLoss: 162.837156\n",
      "Train Epoch: 376 [400000/697932 (57%)]\tLoss: 162.367172\n",
      "Train Epoch: 376 [500000/697932 (72%)]\tLoss: 161.494031\n",
      "Train Epoch: 376 [600000/697932 (86%)]\tLoss: 160.412125\n",
      "====> Epoch: 376 Average loss: 161.9372\n",
      "====> Test set loss: 164.4225\n",
      "Train Epoch: 377 [0/697932 (0%)]\tLoss: 163.837484\n",
      "Train Epoch: 377 [100000/697932 (14%)]\tLoss: 163.427891\n",
      "Train Epoch: 377 [200000/697932 (29%)]\tLoss: 161.076422\n",
      "Train Epoch: 377 [300000/697932 (43%)]\tLoss: 163.870391\n",
      "Train Epoch: 377 [400000/697932 (57%)]\tLoss: 160.401781\n",
      "Train Epoch: 377 [500000/697932 (72%)]\tLoss: 161.828812\n",
      "Train Epoch: 377 [600000/697932 (86%)]\tLoss: 160.607156\n",
      "====> Epoch: 377 Average loss: 161.9751\n",
      "====> Test set loss: 164.8359\n",
      "Train Epoch: 378 [0/697932 (0%)]\tLoss: 164.342047\n",
      "Train Epoch: 378 [100000/697932 (14%)]\tLoss: 159.479172\n",
      "Train Epoch: 378 [200000/697932 (29%)]\tLoss: 161.921531\n",
      "Train Epoch: 378 [300000/697932 (43%)]\tLoss: 162.475781\n",
      "Train Epoch: 378 [400000/697932 (57%)]\tLoss: 164.318094\n",
      "Train Epoch: 378 [500000/697932 (72%)]\tLoss: 162.126906\n",
      "Train Epoch: 378 [600000/697932 (86%)]\tLoss: 162.556312\n",
      "====> Epoch: 378 Average loss: 161.9706\n",
      "====> Test set loss: 164.6227\n",
      "Train Epoch: 379 [0/697932 (0%)]\tLoss: 162.288813\n",
      "Train Epoch: 379 [100000/697932 (14%)]\tLoss: 161.764969\n",
      "Train Epoch: 379 [200000/697932 (29%)]\tLoss: 159.907969\n",
      "Train Epoch: 379 [300000/697932 (43%)]\tLoss: 159.453469\n",
      "Train Epoch: 379 [400000/697932 (57%)]\tLoss: 161.904203\n",
      "Train Epoch: 379 [500000/697932 (72%)]\tLoss: 159.715422\n",
      "Train Epoch: 379 [600000/697932 (86%)]\tLoss: 161.781328\n",
      "====> Epoch: 379 Average loss: 161.9305\n",
      "====> Test set loss: 164.4605\n",
      "Train Epoch: 380 [0/697932 (0%)]\tLoss: 162.392031\n",
      "Train Epoch: 380 [100000/697932 (14%)]\tLoss: 159.866734\n",
      "Train Epoch: 380 [200000/697932 (29%)]\tLoss: 164.266719\n",
      "Train Epoch: 380 [300000/697932 (43%)]\tLoss: 162.400734\n",
      "Train Epoch: 380 [400000/697932 (57%)]\tLoss: 159.311469\n",
      "Train Epoch: 380 [500000/697932 (72%)]\tLoss: 160.957437\n",
      "Train Epoch: 380 [600000/697932 (86%)]\tLoss: 162.113750\n",
      "====> Epoch: 380 Average loss: 161.8826\n",
      "====> Test set loss: 164.4054\n",
      "Train Epoch: 381 [0/697932 (0%)]\tLoss: 161.947125\n",
      "Train Epoch: 381 [100000/697932 (14%)]\tLoss: 163.246109\n",
      "Train Epoch: 381 [200000/697932 (29%)]\tLoss: 163.046406\n",
      "Train Epoch: 381 [300000/697932 (43%)]\tLoss: 162.015578\n",
      "Train Epoch: 381 [400000/697932 (57%)]\tLoss: 161.709156\n",
      "Train Epoch: 381 [500000/697932 (72%)]\tLoss: 163.155359\n",
      "Train Epoch: 381 [600000/697932 (86%)]\tLoss: 163.545141\n",
      "====> Epoch: 381 Average loss: 161.8095\n",
      "====> Test set loss: 164.5170\n",
      "Train Epoch: 382 [0/697932 (0%)]\tLoss: 160.961328\n",
      "Train Epoch: 382 [100000/697932 (14%)]\tLoss: 161.777469\n",
      "Train Epoch: 382 [200000/697932 (29%)]\tLoss: 160.337266\n",
      "Train Epoch: 382 [300000/697932 (43%)]\tLoss: 162.213766\n",
      "Train Epoch: 382 [400000/697932 (57%)]\tLoss: 161.737672\n",
      "Train Epoch: 382 [500000/697932 (72%)]\tLoss: 160.518328\n",
      "Train Epoch: 382 [600000/697932 (86%)]\tLoss: 161.662156\n",
      "====> Epoch: 382 Average loss: 161.8679\n",
      "====> Test set loss: 164.5092\n",
      "Train Epoch: 383 [0/697932 (0%)]\tLoss: 162.762500\n",
      "Train Epoch: 383 [100000/697932 (14%)]\tLoss: 164.391609\n",
      "Train Epoch: 383 [200000/697932 (29%)]\tLoss: 161.131438\n",
      "Train Epoch: 383 [300000/697932 (43%)]\tLoss: 162.497891\n",
      "Train Epoch: 383 [400000/697932 (57%)]\tLoss: 160.850359\n",
      "Train Epoch: 383 [500000/697932 (72%)]\tLoss: 160.905250\n",
      "Train Epoch: 383 [600000/697932 (86%)]\tLoss: 161.203234\n",
      "====> Epoch: 383 Average loss: 161.8712\n",
      "====> Test set loss: 164.5437\n",
      "Train Epoch: 384 [0/697932 (0%)]\tLoss: 162.199219\n",
      "Train Epoch: 384 [100000/697932 (14%)]\tLoss: 159.041281\n",
      "Train Epoch: 384 [200000/697932 (29%)]\tLoss: 165.548266\n",
      "Train Epoch: 384 [300000/697932 (43%)]\tLoss: 161.748250\n",
      "Train Epoch: 384 [400000/697932 (57%)]\tLoss: 162.858094\n",
      "Train Epoch: 384 [500000/697932 (72%)]\tLoss: 159.891781\n",
      "Train Epoch: 384 [600000/697932 (86%)]\tLoss: 162.038703\n",
      "====> Epoch: 384 Average loss: 161.8473\n",
      "====> Test set loss: 164.5095\n",
      "Train Epoch: 385 [0/697932 (0%)]\tLoss: 162.104578\n",
      "Train Epoch: 385 [100000/697932 (14%)]\tLoss: 163.370891\n",
      "Train Epoch: 385 [200000/697932 (29%)]\tLoss: 163.168094\n",
      "Train Epoch: 385 [300000/697932 (43%)]\tLoss: 162.691172\n",
      "Train Epoch: 385 [400000/697932 (57%)]\tLoss: 162.802078\n",
      "Train Epoch: 385 [500000/697932 (72%)]\tLoss: 165.037406\n",
      "Train Epoch: 385 [600000/697932 (86%)]\tLoss: 163.794891\n",
      "====> Epoch: 385 Average loss: 161.8994\n",
      "====> Test set loss: 164.4751\n",
      "Train Epoch: 386 [0/697932 (0%)]\tLoss: 162.583859\n",
      "Train Epoch: 386 [100000/697932 (14%)]\tLoss: 160.077750\n",
      "Train Epoch: 386 [200000/697932 (29%)]\tLoss: 164.231219\n",
      "Train Epoch: 386 [300000/697932 (43%)]\tLoss: 159.802953\n",
      "Train Epoch: 386 [400000/697932 (57%)]\tLoss: 160.866953\n",
      "Train Epoch: 386 [500000/697932 (72%)]\tLoss: 162.231391\n",
      "Train Epoch: 386 [600000/697932 (86%)]\tLoss: 161.234094\n",
      "====> Epoch: 386 Average loss: 161.8470\n",
      "====> Test set loss: 164.3777\n",
      "Train Epoch: 387 [0/697932 (0%)]\tLoss: 160.933000\n",
      "Train Epoch: 387 [100000/697932 (14%)]\tLoss: 158.701469\n",
      "Train Epoch: 387 [200000/697932 (29%)]\tLoss: 159.866578\n",
      "Train Epoch: 387 [300000/697932 (43%)]\tLoss: 161.700578\n",
      "Train Epoch: 387 [400000/697932 (57%)]\tLoss: 160.673797\n",
      "Train Epoch: 387 [500000/697932 (72%)]\tLoss: 162.114750\n",
      "Train Epoch: 387 [600000/697932 (86%)]\tLoss: 163.204781\n",
      "====> Epoch: 387 Average loss: 161.8900\n",
      "====> Test set loss: 164.3908\n",
      "Train Epoch: 388 [0/697932 (0%)]\tLoss: 161.385531\n",
      "Train Epoch: 388 [100000/697932 (14%)]\tLoss: 162.260359\n",
      "Train Epoch: 388 [200000/697932 (29%)]\tLoss: 161.101687\n",
      "Train Epoch: 388 [300000/697932 (43%)]\tLoss: 161.143797\n",
      "Train Epoch: 388 [400000/697932 (57%)]\tLoss: 162.734844\n",
      "Train Epoch: 388 [500000/697932 (72%)]\tLoss: 166.510734\n",
      "Train Epoch: 388 [600000/697932 (86%)]\tLoss: 158.110500\n",
      "====> Epoch: 388 Average loss: 161.8526\n",
      "====> Test set loss: 164.4879\n",
      "Train Epoch: 389 [0/697932 (0%)]\tLoss: 161.032797\n",
      "Train Epoch: 389 [100000/697932 (14%)]\tLoss: 163.291859\n",
      "Train Epoch: 389 [200000/697932 (29%)]\tLoss: 160.656563\n",
      "Train Epoch: 389 [300000/697932 (43%)]\tLoss: 163.072484\n",
      "Train Epoch: 389 [400000/697932 (57%)]\tLoss: 159.991750\n",
      "Train Epoch: 389 [500000/697932 (72%)]\tLoss: 165.309469\n",
      "Train Epoch: 389 [600000/697932 (86%)]\tLoss: 159.454406\n",
      "====> Epoch: 389 Average loss: 161.8555\n",
      "====> Test set loss: 164.4269\n",
      "Train Epoch: 390 [0/697932 (0%)]\tLoss: 160.969250\n",
      "Train Epoch: 390 [100000/697932 (14%)]\tLoss: 163.077094\n",
      "Train Epoch: 390 [200000/697932 (29%)]\tLoss: 161.050062\n",
      "Train Epoch: 390 [300000/697932 (43%)]\tLoss: 162.896391\n",
      "Train Epoch: 390 [400000/697932 (57%)]\tLoss: 162.498969\n",
      "Train Epoch: 390 [500000/697932 (72%)]\tLoss: 161.372844\n",
      "Train Epoch: 390 [600000/697932 (86%)]\tLoss: 163.108703\n",
      "====> Epoch: 390 Average loss: 161.8971\n",
      "====> Test set loss: 164.6172\n",
      "Train Epoch: 391 [0/697932 (0%)]\tLoss: 163.842312\n",
      "Train Epoch: 391 [100000/697932 (14%)]\tLoss: 162.073047\n",
      "Train Epoch: 391 [200000/697932 (29%)]\tLoss: 160.645281\n",
      "Train Epoch: 391 [300000/697932 (43%)]\tLoss: 158.940734\n",
      "Train Epoch: 391 [400000/697932 (57%)]\tLoss: 164.060500\n",
      "Train Epoch: 391 [500000/697932 (72%)]\tLoss: 163.386891\n",
      "Train Epoch: 391 [600000/697932 (86%)]\tLoss: 164.891109\n",
      "====> Epoch: 391 Average loss: 161.8342\n",
      "====> Test set loss: 164.3602\n",
      "Train Epoch: 392 [0/697932 (0%)]\tLoss: 161.291859\n",
      "Train Epoch: 392 [100000/697932 (14%)]\tLoss: 164.726562\n",
      "Train Epoch: 392 [200000/697932 (29%)]\tLoss: 162.544703\n",
      "Train Epoch: 392 [300000/697932 (43%)]\tLoss: 163.375281\n",
      "Train Epoch: 392 [400000/697932 (57%)]\tLoss: 162.556672\n",
      "Train Epoch: 392 [500000/697932 (72%)]\tLoss: 162.569375\n",
      "Train Epoch: 392 [600000/697932 (86%)]\tLoss: 160.087016\n",
      "====> Epoch: 392 Average loss: 161.8355\n",
      "====> Test set loss: 164.4417\n",
      "Train Epoch: 393 [0/697932 (0%)]\tLoss: 161.317250\n",
      "Train Epoch: 393 [100000/697932 (14%)]\tLoss: 162.708250\n",
      "Train Epoch: 393 [200000/697932 (29%)]\tLoss: 163.808500\n",
      "Train Epoch: 393 [300000/697932 (43%)]\tLoss: 159.891266\n",
      "Train Epoch: 393 [400000/697932 (57%)]\tLoss: 163.126328\n",
      "Train Epoch: 393 [500000/697932 (72%)]\tLoss: 161.494859\n",
      "Train Epoch: 393 [600000/697932 (86%)]\tLoss: 159.663250\n",
      "====> Epoch: 393 Average loss: 161.7950\n",
      "====> Test set loss: 164.3710\n",
      "Train Epoch: 394 [0/697932 (0%)]\tLoss: 163.192313\n",
      "Train Epoch: 394 [100000/697932 (14%)]\tLoss: 163.089812\n",
      "Train Epoch: 394 [200000/697932 (29%)]\tLoss: 162.000687\n",
      "Train Epoch: 394 [300000/697932 (43%)]\tLoss: 163.281813\n",
      "Train Epoch: 394 [400000/697932 (57%)]\tLoss: 159.404922\n",
      "Train Epoch: 394 [500000/697932 (72%)]\tLoss: 160.393484\n",
      "Train Epoch: 394 [600000/697932 (86%)]\tLoss: 163.327078\n",
      "====> Epoch: 394 Average loss: 161.8347\n",
      "====> Test set loss: 164.4210\n",
      "Train Epoch: 395 [0/697932 (0%)]\tLoss: 161.290984\n",
      "Train Epoch: 395 [100000/697932 (14%)]\tLoss: 160.426656\n",
      "Train Epoch: 395 [200000/697932 (29%)]\tLoss: 160.763625\n",
      "Train Epoch: 395 [300000/697932 (43%)]\tLoss: 166.316047\n",
      "Train Epoch: 395 [400000/697932 (57%)]\tLoss: 161.295656\n",
      "Train Epoch: 395 [500000/697932 (72%)]\tLoss: 159.190266\n",
      "Train Epoch: 395 [600000/697932 (86%)]\tLoss: 161.829516\n",
      "====> Epoch: 395 Average loss: 161.8103\n",
      "====> Test set loss: 164.4520\n",
      "Train Epoch: 396 [0/697932 (0%)]\tLoss: 162.165234\n",
      "Train Epoch: 396 [100000/697932 (14%)]\tLoss: 162.405969\n",
      "Train Epoch: 396 [200000/697932 (29%)]\tLoss: 161.668516\n",
      "Train Epoch: 396 [300000/697932 (43%)]\tLoss: 164.507141\n",
      "Train Epoch: 396 [400000/697932 (57%)]\tLoss: 161.436297\n",
      "Train Epoch: 396 [500000/697932 (72%)]\tLoss: 162.372437\n",
      "Train Epoch: 396 [600000/697932 (86%)]\tLoss: 163.069406\n",
      "====> Epoch: 396 Average loss: 161.7799\n",
      "====> Test set loss: 164.4560\n",
      "Train Epoch: 397 [0/697932 (0%)]\tLoss: 159.672406\n",
      "Train Epoch: 397 [100000/697932 (14%)]\tLoss: 161.083406\n",
      "Train Epoch: 397 [200000/697932 (29%)]\tLoss: 157.357875\n",
      "Train Epoch: 397 [300000/697932 (43%)]\tLoss: 163.714516\n",
      "Train Epoch: 397 [400000/697932 (57%)]\tLoss: 160.877984\n",
      "Train Epoch: 397 [500000/697932 (72%)]\tLoss: 158.302047\n",
      "Train Epoch: 397 [600000/697932 (86%)]\tLoss: 161.927312\n",
      "====> Epoch: 397 Average loss: 161.7880\n",
      "====> Test set loss: 164.8353\n",
      "Train Epoch: 398 [0/697932 (0%)]\tLoss: 159.900344\n",
      "Train Epoch: 398 [100000/697932 (14%)]\tLoss: 161.580328\n",
      "Train Epoch: 398 [200000/697932 (29%)]\tLoss: 162.911234\n",
      "Train Epoch: 398 [300000/697932 (43%)]\tLoss: 161.838219\n",
      "Train Epoch: 398 [400000/697932 (57%)]\tLoss: 160.719891\n",
      "Train Epoch: 398 [500000/697932 (72%)]\tLoss: 159.669828\n",
      "Train Epoch: 398 [600000/697932 (86%)]\tLoss: 163.635500\n",
      "====> Epoch: 398 Average loss: 161.8618\n",
      "====> Test set loss: 164.6015\n",
      "Train Epoch: 399 [0/697932 (0%)]\tLoss: 160.684656\n",
      "Train Epoch: 399 [100000/697932 (14%)]\tLoss: 163.324375\n",
      "Train Epoch: 399 [200000/697932 (29%)]\tLoss: 162.438906\n",
      "Train Epoch: 399 [300000/697932 (43%)]\tLoss: 163.080469\n",
      "Train Epoch: 399 [400000/697932 (57%)]\tLoss: 162.019469\n",
      "Train Epoch: 399 [500000/697932 (72%)]\tLoss: 161.960906\n",
      "Train Epoch: 399 [600000/697932 (86%)]\tLoss: 158.963188\n",
      "====> Epoch: 399 Average loss: 161.8847\n",
      "====> Test set loss: 164.5687\n",
      "Train Epoch: 400 [0/697932 (0%)]\tLoss: 162.591438\n",
      "Train Epoch: 400 [100000/697932 (14%)]\tLoss: 159.417031\n",
      "Train Epoch: 400 [200000/697932 (29%)]\tLoss: 161.680187\n",
      "Train Epoch: 400 [300000/697932 (43%)]\tLoss: 161.507281\n",
      "Train Epoch: 400 [400000/697932 (57%)]\tLoss: 159.953484\n",
      "Train Epoch: 400 [500000/697932 (72%)]\tLoss: 162.788594\n",
      "Train Epoch: 400 [600000/697932 (86%)]\tLoss: 163.321281\n",
      "====> Epoch: 400 Average loss: 161.9129\n",
      "====> Test set loss: 164.4544\n",
      "Train Epoch: 401 [0/697932 (0%)]\tLoss: 160.875969\n",
      "Train Epoch: 401 [100000/697932 (14%)]\tLoss: 161.073859\n",
      "Train Epoch: 401 [200000/697932 (29%)]\tLoss: 160.559656\n",
      "Train Epoch: 401 [300000/697932 (43%)]\tLoss: 162.099312\n",
      "Train Epoch: 401 [400000/697932 (57%)]\tLoss: 161.549828\n",
      "Train Epoch: 401 [500000/697932 (72%)]\tLoss: 161.295594\n",
      "Train Epoch: 401 [600000/697932 (86%)]\tLoss: 160.706797\n",
      "====> Epoch: 401 Average loss: 161.8075\n",
      "====> Test set loss: 164.6252\n",
      "Train Epoch: 402 [0/697932 (0%)]\tLoss: 159.400016\n",
      "Train Epoch: 402 [100000/697932 (14%)]\tLoss: 160.107687\n",
      "Train Epoch: 402 [200000/697932 (29%)]\tLoss: 160.088719\n",
      "Train Epoch: 402 [300000/697932 (43%)]\tLoss: 163.376656\n",
      "Train Epoch: 402 [400000/697932 (57%)]\tLoss: 160.789922\n",
      "Train Epoch: 402 [500000/697932 (72%)]\tLoss: 161.642078\n",
      "Train Epoch: 402 [600000/697932 (86%)]\tLoss: 163.178531\n",
      "====> Epoch: 402 Average loss: 161.8317\n",
      "====> Test set loss: 164.3713\n",
      "Train Epoch: 403 [0/697932 (0%)]\tLoss: 158.403000\n",
      "Train Epoch: 403 [100000/697932 (14%)]\tLoss: 164.389250\n",
      "Train Epoch: 403 [200000/697932 (29%)]\tLoss: 159.656578\n",
      "Train Epoch: 403 [300000/697932 (43%)]\tLoss: 160.550359\n",
      "Train Epoch: 403 [400000/697932 (57%)]\tLoss: 163.892734\n",
      "Train Epoch: 403 [500000/697932 (72%)]\tLoss: 159.288469\n",
      "Train Epoch: 403 [600000/697932 (86%)]\tLoss: 159.839859\n",
      "====> Epoch: 403 Average loss: 161.7639\n",
      "====> Test set loss: 164.7072\n",
      "Train Epoch: 404 [0/697932 (0%)]\tLoss: 161.138484\n",
      "Train Epoch: 404 [100000/697932 (14%)]\tLoss: 160.771984\n",
      "Train Epoch: 404 [200000/697932 (29%)]\tLoss: 162.021562\n",
      "Train Epoch: 404 [300000/697932 (43%)]\tLoss: 163.076437\n",
      "Train Epoch: 404 [400000/697932 (57%)]\tLoss: 160.091328\n",
      "Train Epoch: 404 [500000/697932 (72%)]\tLoss: 163.265813\n",
      "Train Epoch: 404 [600000/697932 (86%)]\tLoss: 161.288109\n",
      "====> Epoch: 404 Average loss: 161.7824\n",
      "====> Test set loss: 164.4684\n",
      "Train Epoch: 405 [0/697932 (0%)]\tLoss: 160.319469\n",
      "Train Epoch: 405 [100000/697932 (14%)]\tLoss: 160.826766\n",
      "Train Epoch: 405 [200000/697932 (29%)]\tLoss: 162.311062\n",
      "Train Epoch: 405 [300000/697932 (43%)]\tLoss: 163.388922\n",
      "Train Epoch: 405 [400000/697932 (57%)]\tLoss: 163.736750\n",
      "Train Epoch: 405 [500000/697932 (72%)]\tLoss: 161.272281\n",
      "Train Epoch: 405 [600000/697932 (86%)]\tLoss: 161.985953\n",
      "====> Epoch: 405 Average loss: 161.7801\n",
      "====> Test set loss: 164.3932\n",
      "Train Epoch: 406 [0/697932 (0%)]\tLoss: 160.051047\n",
      "Train Epoch: 406 [100000/697932 (14%)]\tLoss: 162.078531\n",
      "Train Epoch: 406 [200000/697932 (29%)]\tLoss: 164.640984\n",
      "Train Epoch: 406 [300000/697932 (43%)]\tLoss: 161.331047\n",
      "Train Epoch: 406 [400000/697932 (57%)]\tLoss: 161.937125\n",
      "Train Epoch: 406 [500000/697932 (72%)]\tLoss: 163.728625\n",
      "Train Epoch: 406 [600000/697932 (86%)]\tLoss: 161.481250\n",
      "====> Epoch: 406 Average loss: 161.8180\n",
      "====> Test set loss: 164.5096\n",
      "Train Epoch: 407 [0/697932 (0%)]\tLoss: 163.522562\n",
      "Train Epoch: 407 [100000/697932 (14%)]\tLoss: 158.089172\n",
      "Train Epoch: 407 [200000/697932 (29%)]\tLoss: 162.399281\n",
      "Train Epoch: 407 [300000/697932 (43%)]\tLoss: 163.501094\n",
      "Train Epoch: 407 [400000/697932 (57%)]\tLoss: 160.436375\n",
      "Train Epoch: 407 [500000/697932 (72%)]\tLoss: 159.716234\n",
      "Train Epoch: 407 [600000/697932 (86%)]\tLoss: 161.784188\n",
      "====> Epoch: 407 Average loss: 161.7887\n",
      "====> Test set loss: 164.5327\n",
      "Train Epoch: 408 [0/697932 (0%)]\tLoss: 162.285000\n",
      "Train Epoch: 408 [100000/697932 (14%)]\tLoss: 165.104516\n",
      "Train Epoch: 408 [200000/697932 (29%)]\tLoss: 159.077375\n",
      "Train Epoch: 408 [300000/697932 (43%)]\tLoss: 159.041250\n",
      "Train Epoch: 408 [400000/697932 (57%)]\tLoss: 161.546188\n",
      "Train Epoch: 408 [500000/697932 (72%)]\tLoss: 161.376375\n",
      "Train Epoch: 408 [600000/697932 (86%)]\tLoss: 162.310078\n",
      "====> Epoch: 408 Average loss: 161.7529\n",
      "====> Test set loss: 164.5278\n",
      "Train Epoch: 409 [0/697932 (0%)]\tLoss: 160.977047\n",
      "Train Epoch: 409 [100000/697932 (14%)]\tLoss: 160.050781\n",
      "Train Epoch: 409 [200000/697932 (29%)]\tLoss: 165.409469\n",
      "Train Epoch: 409 [300000/697932 (43%)]\tLoss: 159.715125\n",
      "Train Epoch: 409 [400000/697932 (57%)]\tLoss: 160.971703\n",
      "Train Epoch: 409 [500000/697932 (72%)]\tLoss: 162.561875\n",
      "Train Epoch: 409 [600000/697932 (86%)]\tLoss: 162.082125\n",
      "====> Epoch: 409 Average loss: 161.8202\n",
      "====> Test set loss: 164.5282\n",
      "Train Epoch: 410 [0/697932 (0%)]\tLoss: 162.250344\n",
      "Train Epoch: 410 [100000/697932 (14%)]\tLoss: 160.876234\n",
      "Train Epoch: 410 [200000/697932 (29%)]\tLoss: 164.328172\n",
      "Train Epoch: 410 [300000/697932 (43%)]\tLoss: 162.500063\n",
      "Train Epoch: 410 [400000/697932 (57%)]\tLoss: 159.440422\n",
      "Train Epoch: 410 [500000/697932 (72%)]\tLoss: 162.792672\n",
      "Train Epoch: 410 [600000/697932 (86%)]\tLoss: 160.532391\n",
      "====> Epoch: 410 Average loss: 161.7493\n",
      "====> Test set loss: 164.4953\n",
      "Train Epoch: 411 [0/697932 (0%)]\tLoss: 163.907266\n",
      "Train Epoch: 411 [100000/697932 (14%)]\tLoss: 162.551109\n",
      "Train Epoch: 411 [200000/697932 (29%)]\tLoss: 159.252875\n",
      "Train Epoch: 411 [300000/697932 (43%)]\tLoss: 162.481641\n",
      "Train Epoch: 411 [400000/697932 (57%)]\tLoss: 163.152125\n",
      "Train Epoch: 411 [500000/697932 (72%)]\tLoss: 158.421625\n",
      "Train Epoch: 411 [600000/697932 (86%)]\tLoss: 162.062516\n",
      "====> Epoch: 411 Average loss: 161.7815\n",
      "====> Test set loss: 164.2794\n",
      "Train Epoch: 412 [0/697932 (0%)]\tLoss: 160.958906\n",
      "Train Epoch: 412 [100000/697932 (14%)]\tLoss: 158.736922\n",
      "Train Epoch: 412 [200000/697932 (29%)]\tLoss: 160.667156\n",
      "Train Epoch: 412 [300000/697932 (43%)]\tLoss: 162.621844\n",
      "Train Epoch: 412 [400000/697932 (57%)]\tLoss: 163.151219\n",
      "Train Epoch: 412 [500000/697932 (72%)]\tLoss: 160.811344\n",
      "Train Epoch: 412 [600000/697932 (86%)]\tLoss: 159.558266\n",
      "====> Epoch: 412 Average loss: 161.7992\n",
      "====> Test set loss: 164.5357\n",
      "Train Epoch: 413 [0/697932 (0%)]\tLoss: 160.571375\n",
      "Train Epoch: 413 [100000/697932 (14%)]\tLoss: 162.026781\n",
      "Train Epoch: 413 [200000/697932 (29%)]\tLoss: 162.204703\n",
      "Train Epoch: 413 [300000/697932 (43%)]\tLoss: 162.371641\n",
      "Train Epoch: 413 [400000/697932 (57%)]\tLoss: 163.869469\n",
      "Train Epoch: 413 [500000/697932 (72%)]\tLoss: 163.274406\n",
      "Train Epoch: 413 [600000/697932 (86%)]\tLoss: 163.434234\n",
      "====> Epoch: 413 Average loss: 161.9482\n",
      "====> Test set loss: 164.5283\n",
      "Train Epoch: 414 [0/697932 (0%)]\tLoss: 161.462047\n",
      "Train Epoch: 414 [100000/697932 (14%)]\tLoss: 164.951156\n",
      "Train Epoch: 414 [200000/697932 (29%)]\tLoss: 158.447922\n",
      "Train Epoch: 414 [300000/697932 (43%)]\tLoss: 160.495969\n",
      "Train Epoch: 414 [400000/697932 (57%)]\tLoss: 163.156062\n",
      "Train Epoch: 414 [500000/697932 (72%)]\tLoss: 161.949969\n",
      "Train Epoch: 414 [600000/697932 (86%)]\tLoss: 162.355703\n",
      "====> Epoch: 414 Average loss: 161.8124\n",
      "====> Test set loss: 164.5296\n",
      "Train Epoch: 415 [0/697932 (0%)]\tLoss: 159.192000\n",
      "Train Epoch: 415 [100000/697932 (14%)]\tLoss: 162.786109\n",
      "Train Epoch: 415 [200000/697932 (29%)]\tLoss: 162.151641\n",
      "Train Epoch: 415 [300000/697932 (43%)]\tLoss: 163.360281\n",
      "Train Epoch: 415 [400000/697932 (57%)]\tLoss: 161.163328\n",
      "Train Epoch: 415 [500000/697932 (72%)]\tLoss: 164.149188\n",
      "Train Epoch: 415 [600000/697932 (86%)]\tLoss: 162.481266\n",
      "====> Epoch: 415 Average loss: 161.8006\n",
      "====> Test set loss: 164.5576\n",
      "Train Epoch: 416 [0/697932 (0%)]\tLoss: 163.629562\n",
      "Train Epoch: 416 [100000/697932 (14%)]\tLoss: 163.088687\n",
      "Train Epoch: 416 [200000/697932 (29%)]\tLoss: 163.083109\n",
      "Train Epoch: 416 [300000/697932 (43%)]\tLoss: 161.331781\n",
      "Train Epoch: 416 [400000/697932 (57%)]\tLoss: 161.836047\n",
      "Train Epoch: 416 [500000/697932 (72%)]\tLoss: 161.081250\n",
      "Train Epoch: 416 [600000/697932 (86%)]\tLoss: 160.504031\n",
      "====> Epoch: 416 Average loss: 161.8133\n",
      "====> Test set loss: 164.4283\n",
      "Train Epoch: 417 [0/697932 (0%)]\tLoss: 162.413938\n",
      "Train Epoch: 417 [100000/697932 (14%)]\tLoss: 162.355453\n",
      "Train Epoch: 417 [200000/697932 (29%)]\tLoss: 160.821281\n",
      "Train Epoch: 417 [300000/697932 (43%)]\tLoss: 161.936656\n",
      "Train Epoch: 417 [400000/697932 (57%)]\tLoss: 160.266047\n",
      "Train Epoch: 417 [500000/697932 (72%)]\tLoss: 161.937875\n",
      "Train Epoch: 417 [600000/697932 (86%)]\tLoss: 164.516969\n",
      "====> Epoch: 417 Average loss: 161.8263\n",
      "====> Test set loss: 164.6058\n",
      "Train Epoch: 418 [0/697932 (0%)]\tLoss: 163.211281\n",
      "Train Epoch: 418 [100000/697932 (14%)]\tLoss: 158.469687\n",
      "Train Epoch: 418 [200000/697932 (29%)]\tLoss: 162.214031\n",
      "Train Epoch: 418 [300000/697932 (43%)]\tLoss: 162.726125\n",
      "Train Epoch: 418 [400000/697932 (57%)]\tLoss: 162.159078\n",
      "Train Epoch: 418 [500000/697932 (72%)]\tLoss: 160.547000\n",
      "Train Epoch: 418 [600000/697932 (86%)]\tLoss: 157.554359\n",
      "====> Epoch: 418 Average loss: 161.8141\n",
      "====> Test set loss: 164.5120\n",
      "Train Epoch: 419 [0/697932 (0%)]\tLoss: 161.144453\n",
      "Train Epoch: 419 [100000/697932 (14%)]\tLoss: 160.803984\n",
      "Train Epoch: 419 [200000/697932 (29%)]\tLoss: 162.415906\n",
      "Train Epoch: 419 [300000/697932 (43%)]\tLoss: 160.744297\n",
      "Train Epoch: 419 [400000/697932 (57%)]\tLoss: 159.002531\n",
      "Train Epoch: 419 [500000/697932 (72%)]\tLoss: 157.711219\n",
      "Train Epoch: 419 [600000/697932 (86%)]\tLoss: 162.105031\n",
      "====> Epoch: 419 Average loss: 161.8163\n",
      "====> Test set loss: 164.5330\n",
      "Train Epoch: 420 [0/697932 (0%)]\tLoss: 161.806297\n",
      "Train Epoch: 420 [100000/697932 (14%)]\tLoss: 163.904469\n",
      "Train Epoch: 420 [200000/697932 (29%)]\tLoss: 161.981562\n",
      "Train Epoch: 420 [300000/697932 (43%)]\tLoss: 163.989484\n",
      "Train Epoch: 420 [400000/697932 (57%)]\tLoss: 163.584172\n",
      "Train Epoch: 420 [500000/697932 (72%)]\tLoss: 163.714672\n",
      "Train Epoch: 420 [600000/697932 (86%)]\tLoss: 162.837359\n",
      "====> Epoch: 420 Average loss: 161.7656\n",
      "====> Test set loss: 164.3313\n",
      "Train Epoch: 421 [0/697932 (0%)]\tLoss: 162.161578\n",
      "Train Epoch: 421 [100000/697932 (14%)]\tLoss: 160.648781\n",
      "Train Epoch: 421 [200000/697932 (29%)]\tLoss: 161.730406\n",
      "Train Epoch: 421 [300000/697932 (43%)]\tLoss: 159.129000\n",
      "Train Epoch: 421 [400000/697932 (57%)]\tLoss: 163.071297\n",
      "Train Epoch: 421 [500000/697932 (72%)]\tLoss: 160.938031\n",
      "Train Epoch: 421 [600000/697932 (86%)]\tLoss: 163.818312\n",
      "====> Epoch: 421 Average loss: 161.7247\n",
      "====> Test set loss: 164.4850\n",
      "Train Epoch: 422 [0/697932 (0%)]\tLoss: 160.717656\n",
      "Train Epoch: 422 [100000/697932 (14%)]\tLoss: 163.413422\n",
      "Train Epoch: 422 [200000/697932 (29%)]\tLoss: 163.079141\n",
      "Train Epoch: 422 [300000/697932 (43%)]\tLoss: 165.064562\n",
      "Train Epoch: 422 [400000/697932 (57%)]\tLoss: 166.257906\n",
      "Train Epoch: 422 [500000/697932 (72%)]\tLoss: 160.306797\n",
      "Train Epoch: 422 [600000/697932 (86%)]\tLoss: 163.415406\n",
      "====> Epoch: 422 Average loss: 161.7966\n",
      "====> Test set loss: 164.3774\n",
      "Train Epoch: 423 [0/697932 (0%)]\tLoss: 162.956281\n",
      "Train Epoch: 423 [100000/697932 (14%)]\tLoss: 161.637500\n",
      "Train Epoch: 423 [200000/697932 (29%)]\tLoss: 162.548438\n",
      "Train Epoch: 423 [300000/697932 (43%)]\tLoss: 162.221359\n",
      "Train Epoch: 423 [400000/697932 (57%)]\tLoss: 159.901594\n",
      "Train Epoch: 423 [500000/697932 (72%)]\tLoss: 162.676281\n",
      "Train Epoch: 423 [600000/697932 (86%)]\tLoss: 162.806703\n",
      "====> Epoch: 423 Average loss: 161.7544\n",
      "====> Test set loss: 164.4341\n",
      "Train Epoch: 424 [0/697932 (0%)]\tLoss: 160.440375\n",
      "Train Epoch: 424 [100000/697932 (14%)]\tLoss: 161.121969\n",
      "Train Epoch: 424 [200000/697932 (29%)]\tLoss: 158.989234\n",
      "Train Epoch: 424 [300000/697932 (43%)]\tLoss: 160.747000\n",
      "Train Epoch: 424 [400000/697932 (57%)]\tLoss: 163.524266\n",
      "Train Epoch: 424 [500000/697932 (72%)]\tLoss: 162.302406\n",
      "Train Epoch: 424 [600000/697932 (86%)]\tLoss: 162.784766\n",
      "====> Epoch: 424 Average loss: 161.7455\n",
      "====> Test set loss: 164.5021\n",
      "Train Epoch: 425 [0/697932 (0%)]\tLoss: 158.841234\n",
      "Train Epoch: 425 [100000/697932 (14%)]\tLoss: 161.951469\n",
      "Train Epoch: 425 [200000/697932 (29%)]\tLoss: 161.592625\n",
      "Train Epoch: 425 [300000/697932 (43%)]\tLoss: 160.056656\n",
      "Train Epoch: 425 [400000/697932 (57%)]\tLoss: 163.236766\n",
      "Train Epoch: 425 [500000/697932 (72%)]\tLoss: 160.807375\n",
      "Train Epoch: 425 [600000/697932 (86%)]\tLoss: 161.911766\n",
      "====> Epoch: 425 Average loss: 161.6776\n",
      "====> Test set loss: 164.3827\n",
      "Train Epoch: 426 [0/697932 (0%)]\tLoss: 162.622641\n",
      "Train Epoch: 426 [100000/697932 (14%)]\tLoss: 159.809703\n",
      "Train Epoch: 426 [200000/697932 (29%)]\tLoss: 163.902047\n",
      "Train Epoch: 426 [300000/697932 (43%)]\tLoss: 162.059844\n",
      "Train Epoch: 426 [400000/697932 (57%)]\tLoss: 163.278422\n",
      "Train Epoch: 426 [500000/697932 (72%)]\tLoss: 160.542188\n",
      "Train Epoch: 426 [600000/697932 (86%)]\tLoss: 162.114062\n",
      "====> Epoch: 426 Average loss: 161.7482\n",
      "====> Test set loss: 164.4703\n",
      "Train Epoch: 427 [0/697932 (0%)]\tLoss: 161.698875\n",
      "Train Epoch: 427 [100000/697932 (14%)]\tLoss: 161.980922\n",
      "Train Epoch: 427 [200000/697932 (29%)]\tLoss: 163.603563\n",
      "Train Epoch: 427 [300000/697932 (43%)]\tLoss: 159.884719\n",
      "Train Epoch: 427 [400000/697932 (57%)]\tLoss: 160.021062\n",
      "Train Epoch: 427 [500000/697932 (72%)]\tLoss: 160.670234\n",
      "Train Epoch: 427 [600000/697932 (86%)]\tLoss: 160.276031\n",
      "====> Epoch: 427 Average loss: 161.7322\n",
      "====> Test set loss: 164.3944\n",
      "Train Epoch: 428 [0/697932 (0%)]\tLoss: 160.275187\n",
      "Train Epoch: 428 [100000/697932 (14%)]\tLoss: 162.819609\n",
      "Train Epoch: 428 [200000/697932 (29%)]\tLoss: 160.623406\n",
      "Train Epoch: 428 [300000/697932 (43%)]\tLoss: 161.642937\n",
      "Train Epoch: 428 [400000/697932 (57%)]\tLoss: 162.583531\n",
      "Train Epoch: 428 [500000/697932 (72%)]\tLoss: 158.359594\n",
      "Train Epoch: 428 [600000/697932 (86%)]\tLoss: 160.190281\n",
      "====> Epoch: 428 Average loss: 161.8012\n",
      "====> Test set loss: 164.4974\n",
      "Train Epoch: 429 [0/697932 (0%)]\tLoss: 160.332797\n",
      "Train Epoch: 429 [100000/697932 (14%)]\tLoss: 161.267281\n",
      "Train Epoch: 429 [200000/697932 (29%)]\tLoss: 163.082125\n",
      "Train Epoch: 429 [300000/697932 (43%)]\tLoss: 162.499391\n",
      "Train Epoch: 429 [400000/697932 (57%)]\tLoss: 164.398969\n",
      "Train Epoch: 429 [500000/697932 (72%)]\tLoss: 162.022406\n",
      "Train Epoch: 429 [600000/697932 (86%)]\tLoss: 160.357531\n",
      "====> Epoch: 429 Average loss: 161.7186\n",
      "====> Test set loss: 164.4353\n",
      "Train Epoch: 430 [0/697932 (0%)]\tLoss: 162.144172\n",
      "Train Epoch: 430 [100000/697932 (14%)]\tLoss: 162.140297\n",
      "Train Epoch: 430 [200000/697932 (29%)]\tLoss: 161.013844\n",
      "Train Epoch: 430 [300000/697932 (43%)]\tLoss: 157.644891\n",
      "Train Epoch: 430 [400000/697932 (57%)]\tLoss: 161.140922\n",
      "Train Epoch: 430 [500000/697932 (72%)]\tLoss: 162.160031\n",
      "Train Epoch: 430 [600000/697932 (86%)]\tLoss: 160.329391\n",
      "====> Epoch: 430 Average loss: 161.7229\n",
      "====> Test set loss: 164.3755\n",
      "Train Epoch: 431 [0/697932 (0%)]\tLoss: 162.724500\n",
      "Train Epoch: 431 [100000/697932 (14%)]\tLoss: 160.960641\n",
      "Train Epoch: 431 [200000/697932 (29%)]\tLoss: 164.853844\n",
      "Train Epoch: 431 [300000/697932 (43%)]\tLoss: 162.756906\n",
      "Train Epoch: 431 [400000/697932 (57%)]\tLoss: 160.255594\n",
      "Train Epoch: 431 [500000/697932 (72%)]\tLoss: 161.719641\n",
      "Train Epoch: 431 [600000/697932 (86%)]\tLoss: 160.234484\n",
      "====> Epoch: 431 Average loss: 161.6961\n",
      "====> Test set loss: 164.3104\n",
      "Train Epoch: 432 [0/697932 (0%)]\tLoss: 163.561281\n",
      "Train Epoch: 432 [100000/697932 (14%)]\tLoss: 165.170359\n",
      "Train Epoch: 432 [200000/697932 (29%)]\tLoss: 161.205203\n",
      "Train Epoch: 432 [300000/697932 (43%)]\tLoss: 163.115219\n",
      "Train Epoch: 432 [400000/697932 (57%)]\tLoss: 159.640328\n",
      "Train Epoch: 432 [500000/697932 (72%)]\tLoss: 162.949469\n",
      "Train Epoch: 432 [600000/697932 (86%)]\tLoss: 161.962000\n",
      "====> Epoch: 432 Average loss: 161.7240\n",
      "====> Test set loss: 164.3222\n",
      "Train Epoch: 433 [0/697932 (0%)]\tLoss: 162.822297\n",
      "Train Epoch: 433 [100000/697932 (14%)]\tLoss: 160.075375\n",
      "Train Epoch: 433 [200000/697932 (29%)]\tLoss: 163.863063\n",
      "Train Epoch: 433 [300000/697932 (43%)]\tLoss: 162.317891\n",
      "Train Epoch: 433 [400000/697932 (57%)]\tLoss: 163.079359\n",
      "Train Epoch: 433 [500000/697932 (72%)]\tLoss: 160.265469\n",
      "Train Epoch: 433 [600000/697932 (86%)]\tLoss: 161.414703\n",
      "====> Epoch: 433 Average loss: 161.6884\n",
      "====> Test set loss: 164.4665\n",
      "Train Epoch: 434 [0/697932 (0%)]\tLoss: 160.838297\n",
      "Train Epoch: 434 [100000/697932 (14%)]\tLoss: 160.905375\n",
      "Train Epoch: 434 [200000/697932 (29%)]\tLoss: 160.816000\n",
      "Train Epoch: 434 [300000/697932 (43%)]\tLoss: 162.948578\n",
      "Train Epoch: 434 [400000/697932 (57%)]\tLoss: 161.514484\n",
      "Train Epoch: 434 [500000/697932 (72%)]\tLoss: 162.732812\n",
      "Train Epoch: 434 [600000/697932 (86%)]\tLoss: 163.851812\n",
      "====> Epoch: 434 Average loss: 161.6711\n",
      "====> Test set loss: 164.4844\n",
      "Train Epoch: 435 [0/697932 (0%)]\tLoss: 163.386094\n",
      "Train Epoch: 435 [100000/697932 (14%)]\tLoss: 162.151000\n",
      "Train Epoch: 435 [200000/697932 (29%)]\tLoss: 162.302562\n",
      "Train Epoch: 435 [300000/697932 (43%)]\tLoss: 159.632609\n",
      "Train Epoch: 435 [400000/697932 (57%)]\tLoss: 161.708297\n",
      "Train Epoch: 435 [500000/697932 (72%)]\tLoss: 162.308312\n",
      "Train Epoch: 435 [600000/697932 (86%)]\tLoss: 160.820437\n",
      "====> Epoch: 435 Average loss: 161.6430\n",
      "====> Test set loss: 164.4881\n",
      "Train Epoch: 436 [0/697932 (0%)]\tLoss: 161.854578\n",
      "Train Epoch: 436 [100000/697932 (14%)]\tLoss: 161.066000\n",
      "Train Epoch: 436 [200000/697932 (29%)]\tLoss: 163.981500\n",
      "Train Epoch: 436 [300000/697932 (43%)]\tLoss: 160.926656\n",
      "Train Epoch: 436 [400000/697932 (57%)]\tLoss: 162.027344\n",
      "Train Epoch: 436 [500000/697932 (72%)]\tLoss: 163.070172\n",
      "Train Epoch: 436 [600000/697932 (86%)]\tLoss: 160.005625\n",
      "====> Epoch: 436 Average loss: 161.7897\n",
      "====> Test set loss: 164.3707\n",
      "Train Epoch: 437 [0/697932 (0%)]\tLoss: 162.473813\n",
      "Train Epoch: 437 [100000/697932 (14%)]\tLoss: 164.192828\n",
      "Train Epoch: 437 [200000/697932 (29%)]\tLoss: 163.190469\n",
      "Train Epoch: 437 [300000/697932 (43%)]\tLoss: 163.193938\n",
      "Train Epoch: 437 [400000/697932 (57%)]\tLoss: 163.261297\n",
      "Train Epoch: 437 [500000/697932 (72%)]\tLoss: 159.833125\n",
      "Train Epoch: 437 [600000/697932 (86%)]\tLoss: 161.399188\n",
      "====> Epoch: 437 Average loss: 161.7309\n",
      "====> Test set loss: 164.4948\n",
      "Train Epoch: 438 [0/697932 (0%)]\tLoss: 164.953750\n",
      "Train Epoch: 438 [100000/697932 (14%)]\tLoss: 163.622781\n",
      "Train Epoch: 438 [200000/697932 (29%)]\tLoss: 162.673094\n",
      "Train Epoch: 438 [300000/697932 (43%)]\tLoss: 159.232859\n",
      "Train Epoch: 438 [400000/697932 (57%)]\tLoss: 161.546797\n",
      "Train Epoch: 438 [500000/697932 (72%)]\tLoss: 160.802734\n",
      "Train Epoch: 438 [600000/697932 (86%)]\tLoss: 162.504000\n",
      "====> Epoch: 438 Average loss: 161.7433\n",
      "====> Test set loss: 164.4120\n",
      "Train Epoch: 439 [0/697932 (0%)]\tLoss: 163.235813\n",
      "Train Epoch: 439 [100000/697932 (14%)]\tLoss: 162.366172\n",
      "Train Epoch: 439 [200000/697932 (29%)]\tLoss: 163.535969\n",
      "Train Epoch: 439 [300000/697932 (43%)]\tLoss: 163.386250\n",
      "Train Epoch: 439 [400000/697932 (57%)]\tLoss: 161.043141\n",
      "Train Epoch: 439 [500000/697932 (72%)]\tLoss: 159.708750\n",
      "Train Epoch: 439 [600000/697932 (86%)]\tLoss: 161.377250\n",
      "====> Epoch: 439 Average loss: 161.7118\n",
      "====> Test set loss: 164.4469\n",
      "Train Epoch: 440 [0/697932 (0%)]\tLoss: 159.730609\n",
      "Train Epoch: 440 [100000/697932 (14%)]\tLoss: 162.721672\n",
      "Train Epoch: 440 [200000/697932 (29%)]\tLoss: 161.581500\n",
      "Train Epoch: 440 [300000/697932 (43%)]\tLoss: 161.544766\n",
      "Train Epoch: 440 [400000/697932 (57%)]\tLoss: 159.593734\n",
      "Train Epoch: 440 [500000/697932 (72%)]\tLoss: 161.944922\n",
      "Train Epoch: 440 [600000/697932 (86%)]\tLoss: 160.381516\n",
      "====> Epoch: 440 Average loss: 161.7065\n",
      "====> Test set loss: 164.4164\n",
      "Train Epoch: 441 [0/697932 (0%)]\tLoss: 162.788219\n",
      "Train Epoch: 441 [100000/697932 (14%)]\tLoss: 160.530625\n",
      "Train Epoch: 441 [200000/697932 (29%)]\tLoss: 162.113547\n",
      "Train Epoch: 441 [300000/697932 (43%)]\tLoss: 159.727703\n",
      "Train Epoch: 441 [400000/697932 (57%)]\tLoss: 159.989234\n",
      "Train Epoch: 441 [500000/697932 (72%)]\tLoss: 164.262703\n",
      "Train Epoch: 441 [600000/697932 (86%)]\tLoss: 160.979312\n",
      "====> Epoch: 441 Average loss: 161.6619\n",
      "====> Test set loss: 164.3586\n",
      "Train Epoch: 442 [0/697932 (0%)]\tLoss: 162.775281\n",
      "Train Epoch: 442 [100000/697932 (14%)]\tLoss: 159.136281\n",
      "Train Epoch: 442 [200000/697932 (29%)]\tLoss: 162.464156\n",
      "Train Epoch: 442 [300000/697932 (43%)]\tLoss: 162.008203\n",
      "Train Epoch: 442 [400000/697932 (57%)]\tLoss: 160.525625\n",
      "Train Epoch: 442 [500000/697932 (72%)]\tLoss: 161.125984\n",
      "Train Epoch: 442 [600000/697932 (86%)]\tLoss: 161.666531\n",
      "====> Epoch: 442 Average loss: 161.6392\n",
      "====> Test set loss: 164.3841\n",
      "Train Epoch: 443 [0/697932 (0%)]\tLoss: 159.052047\n",
      "Train Epoch: 443 [100000/697932 (14%)]\tLoss: 163.784938\n",
      "Train Epoch: 443 [200000/697932 (29%)]\tLoss: 158.639656\n",
      "Train Epoch: 443 [300000/697932 (43%)]\tLoss: 161.112078\n",
      "Train Epoch: 443 [400000/697932 (57%)]\tLoss: 162.684891\n",
      "Train Epoch: 443 [500000/697932 (72%)]\tLoss: 163.458953\n",
      "Train Epoch: 443 [600000/697932 (86%)]\tLoss: 159.738359\n",
      "====> Epoch: 443 Average loss: 161.6726\n",
      "====> Test set loss: 164.4016\n",
      "Train Epoch: 444 [0/697932 (0%)]\tLoss: 160.637375\n",
      "Train Epoch: 444 [100000/697932 (14%)]\tLoss: 161.472281\n",
      "Train Epoch: 444 [200000/697932 (29%)]\tLoss: 161.715750\n",
      "Train Epoch: 444 [300000/697932 (43%)]\tLoss: 158.184875\n",
      "Train Epoch: 444 [400000/697932 (57%)]\tLoss: 160.723609\n",
      "Train Epoch: 444 [500000/697932 (72%)]\tLoss: 162.789938\n",
      "Train Epoch: 444 [600000/697932 (86%)]\tLoss: 158.961344\n",
      "====> Epoch: 444 Average loss: 161.6757\n",
      "====> Test set loss: 164.4859\n",
      "Train Epoch: 445 [0/697932 (0%)]\tLoss: 162.340688\n",
      "Train Epoch: 445 [100000/697932 (14%)]\tLoss: 162.580734\n",
      "Train Epoch: 445 [200000/697932 (29%)]\tLoss: 162.316531\n",
      "Train Epoch: 445 [300000/697932 (43%)]\tLoss: 162.803609\n",
      "Train Epoch: 445 [400000/697932 (57%)]\tLoss: 162.494953\n",
      "Train Epoch: 445 [500000/697932 (72%)]\tLoss: 160.262672\n",
      "Train Epoch: 445 [600000/697932 (86%)]\tLoss: 159.966813\n",
      "====> Epoch: 445 Average loss: 161.5956\n",
      "====> Test set loss: 164.4271\n",
      "Train Epoch: 446 [0/697932 (0%)]\tLoss: 159.169359\n",
      "Train Epoch: 446 [100000/697932 (14%)]\tLoss: 162.094016\n",
      "Train Epoch: 446 [200000/697932 (29%)]\tLoss: 161.780625\n",
      "Train Epoch: 446 [300000/697932 (43%)]\tLoss: 163.107500\n",
      "Train Epoch: 446 [400000/697932 (57%)]\tLoss: 158.054203\n",
      "Train Epoch: 446 [500000/697932 (72%)]\tLoss: 162.131937\n",
      "Train Epoch: 446 [600000/697932 (86%)]\tLoss: 159.555156\n",
      "====> Epoch: 446 Average loss: 161.6140\n",
      "====> Test set loss: 164.2980\n",
      "Train Epoch: 447 [0/697932 (0%)]\tLoss: 163.097422\n",
      "Train Epoch: 447 [100000/697932 (14%)]\tLoss: 161.330906\n",
      "Train Epoch: 447 [200000/697932 (29%)]\tLoss: 161.327203\n",
      "Train Epoch: 447 [300000/697932 (43%)]\tLoss: 163.390359\n",
      "Train Epoch: 447 [400000/697932 (57%)]\tLoss: 160.821969\n",
      "Train Epoch: 447 [500000/697932 (72%)]\tLoss: 162.210500\n",
      "Train Epoch: 447 [600000/697932 (86%)]\tLoss: 161.903016\n",
      "====> Epoch: 447 Average loss: 161.6328\n",
      "====> Test set loss: 164.6034\n",
      "Train Epoch: 448 [0/697932 (0%)]\tLoss: 162.561797\n",
      "Train Epoch: 448 [100000/697932 (14%)]\tLoss: 162.365297\n",
      "Train Epoch: 448 [200000/697932 (29%)]\tLoss: 159.251297\n",
      "Train Epoch: 448 [300000/697932 (43%)]\tLoss: 160.968797\n",
      "Train Epoch: 448 [400000/697932 (57%)]\tLoss: 159.410844\n",
      "Train Epoch: 448 [500000/697932 (72%)]\tLoss: 161.376750\n",
      "Train Epoch: 448 [600000/697932 (86%)]\tLoss: 158.486297\n",
      "====> Epoch: 448 Average loss: 161.6558\n",
      "====> Test set loss: 164.4259\n",
      "Train Epoch: 449 [0/697932 (0%)]\tLoss: 161.833859\n",
      "Train Epoch: 449 [100000/697932 (14%)]\tLoss: 162.543156\n",
      "Train Epoch: 449 [200000/697932 (29%)]\tLoss: 159.139125\n",
      "Train Epoch: 449 [300000/697932 (43%)]\tLoss: 158.824344\n",
      "Train Epoch: 449 [400000/697932 (57%)]\tLoss: 158.500156\n",
      "Train Epoch: 449 [500000/697932 (72%)]\tLoss: 161.005719\n",
      "Train Epoch: 449 [600000/697932 (86%)]\tLoss: 162.935688\n",
      "====> Epoch: 449 Average loss: 161.6230\n",
      "====> Test set loss: 164.4218\n",
      "Train Epoch: 450 [0/697932 (0%)]\tLoss: 159.200562\n",
      "Train Epoch: 450 [100000/697932 (14%)]\tLoss: 160.745594\n",
      "Train Epoch: 450 [200000/697932 (29%)]\tLoss: 160.901328\n",
      "Train Epoch: 450 [300000/697932 (43%)]\tLoss: 159.542563\n",
      "Train Epoch: 450 [400000/697932 (57%)]\tLoss: 161.846797\n",
      "Train Epoch: 450 [500000/697932 (72%)]\tLoss: 162.988313\n",
      "Train Epoch: 450 [600000/697932 (86%)]\tLoss: 164.412750\n",
      "====> Epoch: 450 Average loss: 161.6833\n",
      "====> Test set loss: 164.3397\n",
      "Train Epoch: 451 [0/697932 (0%)]\tLoss: 162.048250\n",
      "Train Epoch: 451 [100000/697932 (14%)]\tLoss: 161.516016\n",
      "Train Epoch: 451 [200000/697932 (29%)]\tLoss: 161.937625\n",
      "Train Epoch: 451 [300000/697932 (43%)]\tLoss: 161.505188\n",
      "Train Epoch: 451 [400000/697932 (57%)]\tLoss: 159.276766\n",
      "Train Epoch: 451 [500000/697932 (72%)]\tLoss: 161.934359\n",
      "Train Epoch: 451 [600000/697932 (86%)]\tLoss: 160.786719\n",
      "====> Epoch: 451 Average loss: 161.5979\n",
      "====> Test set loss: 164.2668\n",
      "Train Epoch: 452 [0/697932 (0%)]\tLoss: 159.611578\n",
      "Train Epoch: 452 [100000/697932 (14%)]\tLoss: 162.621984\n",
      "Train Epoch: 452 [200000/697932 (29%)]\tLoss: 161.045203\n",
      "Train Epoch: 452 [300000/697932 (43%)]\tLoss: 159.993578\n",
      "Train Epoch: 452 [400000/697932 (57%)]\tLoss: 162.139484\n",
      "Train Epoch: 452 [500000/697932 (72%)]\tLoss: 164.768297\n",
      "Train Epoch: 452 [600000/697932 (86%)]\tLoss: 160.902172\n",
      "====> Epoch: 452 Average loss: 161.6813\n",
      "====> Test set loss: 164.5118\n",
      "Train Epoch: 453 [0/697932 (0%)]\tLoss: 161.140656\n",
      "Train Epoch: 453 [100000/697932 (14%)]\tLoss: 161.287531\n",
      "Train Epoch: 453 [200000/697932 (29%)]\tLoss: 158.977656\n",
      "Train Epoch: 453 [300000/697932 (43%)]\tLoss: 161.642484\n",
      "Train Epoch: 453 [400000/697932 (57%)]\tLoss: 160.302172\n",
      "Train Epoch: 453 [500000/697932 (72%)]\tLoss: 161.425016\n",
      "Train Epoch: 453 [600000/697932 (86%)]\tLoss: 162.499562\n",
      "====> Epoch: 453 Average loss: 161.7147\n",
      "====> Test set loss: 164.6320\n",
      "Train Epoch: 454 [0/697932 (0%)]\tLoss: 156.608391\n",
      "Train Epoch: 454 [100000/697932 (14%)]\tLoss: 160.417125\n",
      "Train Epoch: 454 [200000/697932 (29%)]\tLoss: 162.870062\n",
      "Train Epoch: 454 [300000/697932 (43%)]\tLoss: 162.152500\n",
      "Train Epoch: 454 [400000/697932 (57%)]\tLoss: 164.231797\n",
      "Train Epoch: 454 [500000/697932 (72%)]\tLoss: 164.653562\n",
      "Train Epoch: 454 [600000/697932 (86%)]\tLoss: 158.682625\n",
      "====> Epoch: 454 Average loss: 161.6884\n",
      "====> Test set loss: 164.3856\n",
      "Train Epoch: 455 [0/697932 (0%)]\tLoss: 160.769703\n",
      "Train Epoch: 455 [100000/697932 (14%)]\tLoss: 162.351031\n",
      "Train Epoch: 455 [200000/697932 (29%)]\tLoss: 161.577922\n",
      "Train Epoch: 455 [300000/697932 (43%)]\tLoss: 162.285750\n",
      "Train Epoch: 455 [400000/697932 (57%)]\tLoss: 161.540938\n",
      "Train Epoch: 455 [500000/697932 (72%)]\tLoss: 162.033734\n",
      "Train Epoch: 455 [600000/697932 (86%)]\tLoss: 161.515000\n",
      "====> Epoch: 455 Average loss: 161.6514\n",
      "====> Test set loss: 164.6569\n",
      "Train Epoch: 456 [0/697932 (0%)]\tLoss: 164.465609\n",
      "Train Epoch: 456 [100000/697932 (14%)]\tLoss: 162.810859\n",
      "Train Epoch: 456 [200000/697932 (29%)]\tLoss: 161.005844\n",
      "Train Epoch: 456 [300000/697932 (43%)]\tLoss: 163.782703\n",
      "Train Epoch: 456 [400000/697932 (57%)]\tLoss: 161.342453\n",
      "Train Epoch: 456 [500000/697932 (72%)]\tLoss: 161.756297\n",
      "Train Epoch: 456 [600000/697932 (86%)]\tLoss: 160.736016\n",
      "====> Epoch: 456 Average loss: 161.6226\n",
      "====> Test set loss: 164.3493\n",
      "Train Epoch: 457 [0/697932 (0%)]\tLoss: 160.898828\n",
      "Train Epoch: 457 [100000/697932 (14%)]\tLoss: 157.916422\n",
      "Train Epoch: 457 [200000/697932 (29%)]\tLoss: 161.914016\n",
      "Train Epoch: 457 [300000/697932 (43%)]\tLoss: 161.950562\n",
      "Train Epoch: 457 [400000/697932 (57%)]\tLoss: 166.528391\n",
      "Train Epoch: 457 [500000/697932 (72%)]\tLoss: 162.717703\n",
      "Train Epoch: 457 [600000/697932 (86%)]\tLoss: 158.438953\n",
      "====> Epoch: 457 Average loss: 161.7278\n",
      "====> Test set loss: 164.5487\n",
      "Train Epoch: 458 [0/697932 (0%)]\tLoss: 161.450703\n",
      "Train Epoch: 458 [100000/697932 (14%)]\tLoss: 159.771562\n",
      "Train Epoch: 458 [200000/697932 (29%)]\tLoss: 160.734000\n",
      "Train Epoch: 458 [300000/697932 (43%)]\tLoss: 163.564891\n",
      "Train Epoch: 458 [400000/697932 (57%)]\tLoss: 161.845828\n",
      "Train Epoch: 458 [500000/697932 (72%)]\tLoss: 163.873594\n",
      "Train Epoch: 458 [600000/697932 (86%)]\tLoss: 162.964547\n",
      "====> Epoch: 458 Average loss: 161.7102\n",
      "====> Test set loss: 164.4628\n",
      "Train Epoch: 459 [0/697932 (0%)]\tLoss: 160.117578\n",
      "Train Epoch: 459 [100000/697932 (14%)]\tLoss: 162.083000\n",
      "Train Epoch: 459 [200000/697932 (29%)]\tLoss: 162.691797\n",
      "Train Epoch: 459 [300000/697932 (43%)]\tLoss: 163.653984\n",
      "Train Epoch: 459 [400000/697932 (57%)]\tLoss: 164.313516\n",
      "Train Epoch: 459 [500000/697932 (72%)]\tLoss: 160.554422\n",
      "Train Epoch: 459 [600000/697932 (86%)]\tLoss: 157.698359\n",
      "====> Epoch: 459 Average loss: 161.6252\n",
      "====> Test set loss: 164.2883\n",
      "Train Epoch: 460 [0/697932 (0%)]\tLoss: 162.384844\n",
      "Train Epoch: 460 [100000/697932 (14%)]\tLoss: 158.735609\n",
      "Train Epoch: 460 [200000/697932 (29%)]\tLoss: 161.747531\n",
      "Train Epoch: 460 [300000/697932 (43%)]\tLoss: 162.731625\n",
      "Train Epoch: 460 [400000/697932 (57%)]\tLoss: 163.331250\n",
      "Train Epoch: 460 [500000/697932 (72%)]\tLoss: 162.867953\n",
      "Train Epoch: 460 [600000/697932 (86%)]\tLoss: 161.192234\n",
      "====> Epoch: 460 Average loss: 161.6123\n",
      "====> Test set loss: 164.5962\n",
      "Train Epoch: 461 [0/697932 (0%)]\tLoss: 161.410469\n",
      "Train Epoch: 461 [100000/697932 (14%)]\tLoss: 159.456813\n",
      "Train Epoch: 461 [200000/697932 (29%)]\tLoss: 161.886297\n",
      "Train Epoch: 461 [300000/697932 (43%)]\tLoss: 164.318641\n",
      "Train Epoch: 461 [400000/697932 (57%)]\tLoss: 163.087531\n",
      "Train Epoch: 461 [500000/697932 (72%)]\tLoss: 162.161500\n",
      "Train Epoch: 461 [600000/697932 (86%)]\tLoss: 159.982531\n",
      "====> Epoch: 461 Average loss: 161.6220\n",
      "====> Test set loss: 164.5218\n",
      "Train Epoch: 462 [0/697932 (0%)]\tLoss: 161.810250\n",
      "Train Epoch: 462 [100000/697932 (14%)]\tLoss: 162.471625\n",
      "Train Epoch: 462 [200000/697932 (29%)]\tLoss: 164.295109\n",
      "Train Epoch: 462 [300000/697932 (43%)]\tLoss: 161.474000\n",
      "Train Epoch: 462 [400000/697932 (57%)]\tLoss: 161.939016\n",
      "Train Epoch: 462 [500000/697932 (72%)]\tLoss: 160.703625\n",
      "Train Epoch: 462 [600000/697932 (86%)]\tLoss: 160.537047\n",
      "====> Epoch: 462 Average loss: 161.6436\n",
      "====> Test set loss: 164.3584\n",
      "Train Epoch: 463 [0/697932 (0%)]\tLoss: 161.138750\n",
      "Train Epoch: 463 [100000/697932 (14%)]\tLoss: 163.703984\n",
      "Train Epoch: 463 [200000/697932 (29%)]\tLoss: 161.582109\n",
      "Train Epoch: 463 [300000/697932 (43%)]\tLoss: 163.326781\n",
      "Train Epoch: 463 [400000/697932 (57%)]\tLoss: 161.887266\n",
      "Train Epoch: 463 [500000/697932 (72%)]\tLoss: 163.021656\n",
      "Train Epoch: 463 [600000/697932 (86%)]\tLoss: 159.086875\n",
      "====> Epoch: 463 Average loss: 161.5841\n",
      "====> Test set loss: 164.2327\n",
      "Train Epoch: 464 [0/697932 (0%)]\tLoss: 161.580062\n",
      "Train Epoch: 464 [100000/697932 (14%)]\tLoss: 159.228781\n",
      "Train Epoch: 464 [200000/697932 (29%)]\tLoss: 161.968578\n",
      "Train Epoch: 464 [300000/697932 (43%)]\tLoss: 159.553687\n",
      "Train Epoch: 464 [400000/697932 (57%)]\tLoss: 160.066125\n",
      "Train Epoch: 464 [500000/697932 (72%)]\tLoss: 162.423375\n",
      "Train Epoch: 464 [600000/697932 (86%)]\tLoss: 161.888375\n",
      "====> Epoch: 464 Average loss: 161.5598\n",
      "====> Test set loss: 164.4563\n",
      "Train Epoch: 465 [0/697932 (0%)]\tLoss: 162.746547\n",
      "Train Epoch: 465 [100000/697932 (14%)]\tLoss: 162.111031\n",
      "Train Epoch: 465 [200000/697932 (29%)]\tLoss: 162.592688\n",
      "Train Epoch: 465 [300000/697932 (43%)]\tLoss: 160.250344\n",
      "Train Epoch: 465 [400000/697932 (57%)]\tLoss: 162.942625\n",
      "Train Epoch: 465 [500000/697932 (72%)]\tLoss: 162.396828\n",
      "Train Epoch: 465 [600000/697932 (86%)]\tLoss: 161.783813\n",
      "====> Epoch: 465 Average loss: 161.6055\n",
      "====> Test set loss: 164.3301\n",
      "Train Epoch: 466 [0/697932 (0%)]\tLoss: 162.529453\n",
      "Train Epoch: 466 [100000/697932 (14%)]\tLoss: 164.707687\n",
      "Train Epoch: 466 [200000/697932 (29%)]\tLoss: 163.412125\n",
      "Train Epoch: 466 [300000/697932 (43%)]\tLoss: 160.197578\n",
      "Train Epoch: 466 [400000/697932 (57%)]\tLoss: 157.926453\n",
      "Train Epoch: 466 [500000/697932 (72%)]\tLoss: 161.872547\n",
      "Train Epoch: 466 [600000/697932 (86%)]\tLoss: 161.001844\n",
      "====> Epoch: 466 Average loss: 161.5892\n",
      "====> Test set loss: 164.3765\n",
      "Train Epoch: 467 [0/697932 (0%)]\tLoss: 164.259531\n",
      "Train Epoch: 467 [100000/697932 (14%)]\tLoss: 162.479281\n",
      "Train Epoch: 467 [200000/697932 (29%)]\tLoss: 159.725922\n",
      "Train Epoch: 467 [300000/697932 (43%)]\tLoss: 161.287031\n",
      "Train Epoch: 467 [400000/697932 (57%)]\tLoss: 163.097641\n",
      "Train Epoch: 467 [500000/697932 (72%)]\tLoss: 162.037703\n",
      "Train Epoch: 467 [600000/697932 (86%)]\tLoss: 157.877797\n",
      "====> Epoch: 467 Average loss: 161.5755\n",
      "====> Test set loss: 164.3419\n",
      "Train Epoch: 468 [0/697932 (0%)]\tLoss: 159.704750\n",
      "Train Epoch: 468 [100000/697932 (14%)]\tLoss: 159.918531\n",
      "Train Epoch: 468 [200000/697932 (29%)]\tLoss: 161.994656\n",
      "Train Epoch: 468 [300000/697932 (43%)]\tLoss: 161.266547\n",
      "Train Epoch: 468 [400000/697932 (57%)]\tLoss: 162.982844\n",
      "Train Epoch: 468 [500000/697932 (72%)]\tLoss: 160.336094\n",
      "Train Epoch: 468 [600000/697932 (86%)]\tLoss: 161.257203\n",
      "====> Epoch: 468 Average loss: 161.6363\n",
      "====> Test set loss: 164.2064\n",
      "Train Epoch: 469 [0/697932 (0%)]\tLoss: 164.758813\n",
      "Train Epoch: 469 [100000/697932 (14%)]\tLoss: 162.113344\n",
      "Train Epoch: 469 [200000/697932 (29%)]\tLoss: 160.518891\n",
      "Train Epoch: 469 [300000/697932 (43%)]\tLoss: 160.530578\n",
      "Train Epoch: 469 [400000/697932 (57%)]\tLoss: 163.302531\n",
      "Train Epoch: 469 [500000/697932 (72%)]\tLoss: 158.932516\n",
      "Train Epoch: 469 [600000/697932 (86%)]\tLoss: 159.783047\n",
      "====> Epoch: 469 Average loss: 161.5546\n",
      "====> Test set loss: 164.2974\n",
      "Train Epoch: 470 [0/697932 (0%)]\tLoss: 164.656234\n",
      "Train Epoch: 470 [100000/697932 (14%)]\tLoss: 160.227688\n",
      "Train Epoch: 470 [200000/697932 (29%)]\tLoss: 162.555922\n",
      "Train Epoch: 470 [300000/697932 (43%)]\tLoss: 161.095531\n",
      "Train Epoch: 470 [400000/697932 (57%)]\tLoss: 161.073297\n",
      "Train Epoch: 470 [500000/697932 (72%)]\tLoss: 161.116953\n",
      "Train Epoch: 470 [600000/697932 (86%)]\tLoss: 161.536641\n",
      "====> Epoch: 470 Average loss: 161.5787\n",
      "====> Test set loss: 164.6071\n",
      "Train Epoch: 471 [0/697932 (0%)]\tLoss: 160.888156\n",
      "Train Epoch: 471 [100000/697932 (14%)]\tLoss: 164.295406\n",
      "Train Epoch: 471 [200000/697932 (29%)]\tLoss: 163.154063\n",
      "Train Epoch: 471 [300000/697932 (43%)]\tLoss: 159.876688\n",
      "Train Epoch: 471 [400000/697932 (57%)]\tLoss: 160.143313\n",
      "Train Epoch: 471 [500000/697932 (72%)]\tLoss: 162.847938\n",
      "Train Epoch: 471 [600000/697932 (86%)]\tLoss: 160.976438\n",
      "====> Epoch: 471 Average loss: 161.5713\n",
      "====> Test set loss: 164.2790\n",
      "Train Epoch: 472 [0/697932 (0%)]\tLoss: 158.657187\n",
      "Train Epoch: 472 [100000/697932 (14%)]\tLoss: 161.600313\n",
      "Train Epoch: 472 [200000/697932 (29%)]\tLoss: 160.509594\n",
      "Train Epoch: 472 [300000/697932 (43%)]\tLoss: 161.492969\n",
      "Train Epoch: 472 [400000/697932 (57%)]\tLoss: 163.914125\n",
      "Train Epoch: 472 [500000/697932 (72%)]\tLoss: 161.630141\n",
      "Train Epoch: 472 [600000/697932 (86%)]\tLoss: 161.673047\n",
      "====> Epoch: 472 Average loss: 161.6228\n",
      "====> Test set loss: 164.3690\n",
      "Train Epoch: 473 [0/697932 (0%)]\tLoss: 159.849172\n",
      "Train Epoch: 473 [100000/697932 (14%)]\tLoss: 160.692172\n",
      "Train Epoch: 473 [200000/697932 (29%)]\tLoss: 162.066844\n",
      "Train Epoch: 473 [300000/697932 (43%)]\tLoss: 165.013609\n",
      "Train Epoch: 473 [400000/697932 (57%)]\tLoss: 162.724219\n",
      "Train Epoch: 473 [500000/697932 (72%)]\tLoss: 161.188406\n",
      "Train Epoch: 473 [600000/697932 (86%)]\tLoss: 162.382656\n",
      "====> Epoch: 473 Average loss: 161.5804\n",
      "====> Test set loss: 164.3255\n",
      "Train Epoch: 474 [0/697932 (0%)]\tLoss: 158.307578\n",
      "Train Epoch: 474 [100000/697932 (14%)]\tLoss: 162.204844\n",
      "Train Epoch: 474 [200000/697932 (29%)]\tLoss: 163.037813\n",
      "Train Epoch: 474 [300000/697932 (43%)]\tLoss: 162.549062\n",
      "Train Epoch: 474 [400000/697932 (57%)]\tLoss: 160.641125\n",
      "Train Epoch: 474 [500000/697932 (72%)]\tLoss: 161.703922\n",
      "Train Epoch: 474 [600000/697932 (86%)]\tLoss: 163.899594\n",
      "====> Epoch: 474 Average loss: 161.5560\n",
      "====> Test set loss: 164.4535\n",
      "Train Epoch: 475 [0/697932 (0%)]\tLoss: 162.091500\n",
      "Train Epoch: 475 [100000/697932 (14%)]\tLoss: 164.473594\n",
      "Train Epoch: 475 [200000/697932 (29%)]\tLoss: 160.671188\n",
      "Train Epoch: 475 [300000/697932 (43%)]\tLoss: 162.420344\n",
      "Train Epoch: 475 [400000/697932 (57%)]\tLoss: 157.850500\n",
      "Train Epoch: 475 [500000/697932 (72%)]\tLoss: 161.857188\n",
      "Train Epoch: 475 [600000/697932 (86%)]\tLoss: 159.680625\n",
      "====> Epoch: 475 Average loss: 161.5271\n",
      "====> Test set loss: 164.7420\n",
      "Train Epoch: 476 [0/697932 (0%)]\tLoss: 163.068406\n",
      "Train Epoch: 476 [100000/697932 (14%)]\tLoss: 160.574734\n",
      "Train Epoch: 476 [200000/697932 (29%)]\tLoss: 161.150156\n",
      "Train Epoch: 476 [300000/697932 (43%)]\tLoss: 160.306469\n",
      "Train Epoch: 476 [400000/697932 (57%)]\tLoss: 159.803266\n",
      "Train Epoch: 476 [500000/697932 (72%)]\tLoss: 158.897547\n",
      "Train Epoch: 476 [600000/697932 (86%)]\tLoss: 161.801078\n",
      "====> Epoch: 476 Average loss: 161.5986\n",
      "====> Test set loss: 164.7677\n",
      "Train Epoch: 477 [0/697932 (0%)]\tLoss: 162.564500\n",
      "Train Epoch: 477 [100000/697932 (14%)]\tLoss: 162.907437\n",
      "Train Epoch: 477 [200000/697932 (29%)]\tLoss: 160.467578\n",
      "Train Epoch: 477 [300000/697932 (43%)]\tLoss: 164.106641\n",
      "Train Epoch: 477 [400000/697932 (57%)]\tLoss: 162.748578\n",
      "Train Epoch: 477 [500000/697932 (72%)]\tLoss: 160.371000\n",
      "Train Epoch: 477 [600000/697932 (86%)]\tLoss: 159.931813\n",
      "====> Epoch: 477 Average loss: 161.5718\n",
      "====> Test set loss: 164.5501\n",
      "Train Epoch: 478 [0/697932 (0%)]\tLoss: 161.200219\n",
      "Train Epoch: 478 [100000/697932 (14%)]\tLoss: 160.637641\n",
      "Train Epoch: 478 [200000/697932 (29%)]\tLoss: 161.401625\n",
      "Train Epoch: 478 [300000/697932 (43%)]\tLoss: 162.873203\n",
      "Train Epoch: 478 [400000/697932 (57%)]\tLoss: 163.017750\n",
      "Train Epoch: 478 [500000/697932 (72%)]\tLoss: 160.546219\n",
      "Train Epoch: 478 [600000/697932 (86%)]\tLoss: 163.757203\n",
      "====> Epoch: 478 Average loss: 161.5696\n",
      "====> Test set loss: 164.1509\n",
      "Train Epoch: 479 [0/697932 (0%)]\tLoss: 162.312125\n",
      "Train Epoch: 479 [100000/697932 (14%)]\tLoss: 159.582891\n",
      "Train Epoch: 479 [200000/697932 (29%)]\tLoss: 162.800391\n",
      "Train Epoch: 479 [300000/697932 (43%)]\tLoss: 161.808188\n",
      "Train Epoch: 479 [400000/697932 (57%)]\tLoss: 162.329875\n",
      "Train Epoch: 479 [500000/697932 (72%)]\tLoss: 159.627844\n",
      "Train Epoch: 479 [600000/697932 (86%)]\tLoss: 161.868984\n",
      "====> Epoch: 479 Average loss: 161.5394\n",
      "====> Test set loss: 164.6169\n",
      "Train Epoch: 480 [0/697932 (0%)]\tLoss: 164.405625\n",
      "Train Epoch: 480 [100000/697932 (14%)]\tLoss: 160.091594\n",
      "Train Epoch: 480 [200000/697932 (29%)]\tLoss: 159.371781\n",
      "Train Epoch: 480 [300000/697932 (43%)]\tLoss: 160.946453\n",
      "Train Epoch: 480 [400000/697932 (57%)]\tLoss: 163.231750\n",
      "Train Epoch: 480 [500000/697932 (72%)]\tLoss: 160.498766\n",
      "Train Epoch: 480 [600000/697932 (86%)]\tLoss: 161.794078\n",
      "====> Epoch: 480 Average loss: 161.5554\n",
      "====> Test set loss: 164.5166\n",
      "Train Epoch: 481 [0/697932 (0%)]\tLoss: 163.384172\n",
      "Train Epoch: 481 [100000/697932 (14%)]\tLoss: 159.042797\n",
      "Train Epoch: 481 [200000/697932 (29%)]\tLoss: 162.172953\n",
      "Train Epoch: 481 [300000/697932 (43%)]\tLoss: 161.148016\n",
      "Train Epoch: 481 [400000/697932 (57%)]\tLoss: 161.049984\n",
      "Train Epoch: 481 [500000/697932 (72%)]\tLoss: 160.790266\n",
      "Train Epoch: 481 [600000/697932 (86%)]\tLoss: 159.956859\n",
      "====> Epoch: 481 Average loss: 161.6611\n",
      "====> Test set loss: 164.4107\n",
      "Train Epoch: 482 [0/697932 (0%)]\tLoss: 163.255937\n",
      "Train Epoch: 482 [100000/697932 (14%)]\tLoss: 165.639438\n",
      "Train Epoch: 482 [200000/697932 (29%)]\tLoss: 159.879375\n",
      "Train Epoch: 482 [300000/697932 (43%)]\tLoss: 161.813813\n",
      "Train Epoch: 482 [400000/697932 (57%)]\tLoss: 159.481844\n",
      "Train Epoch: 482 [500000/697932 (72%)]\tLoss: 164.138062\n",
      "Train Epoch: 482 [600000/697932 (86%)]\tLoss: 160.242906\n",
      "====> Epoch: 482 Average loss: 161.5934\n",
      "====> Test set loss: 164.1703\n",
      "Train Epoch: 483 [0/697932 (0%)]\tLoss: 166.314125\n",
      "Train Epoch: 483 [100000/697932 (14%)]\tLoss: 159.088719\n",
      "Train Epoch: 483 [200000/697932 (29%)]\tLoss: 160.700625\n",
      "Train Epoch: 483 [300000/697932 (43%)]\tLoss: 159.381859\n",
      "Train Epoch: 483 [400000/697932 (57%)]\tLoss: 164.686250\n",
      "Train Epoch: 483 [500000/697932 (72%)]\tLoss: 162.482313\n",
      "Train Epoch: 483 [600000/697932 (86%)]\tLoss: 160.381109\n",
      "====> Epoch: 483 Average loss: 161.5740\n",
      "====> Test set loss: 164.6426\n",
      "Train Epoch: 484 [0/697932 (0%)]\tLoss: 162.730453\n",
      "Train Epoch: 484 [100000/697932 (14%)]\tLoss: 160.624781\n",
      "Train Epoch: 484 [200000/697932 (29%)]\tLoss: 165.259156\n",
      "Train Epoch: 484 [300000/697932 (43%)]\tLoss: 161.275156\n",
      "Train Epoch: 484 [400000/697932 (57%)]\tLoss: 164.714016\n",
      "Train Epoch: 484 [500000/697932 (72%)]\tLoss: 163.380500\n",
      "Train Epoch: 484 [600000/697932 (86%)]\tLoss: 158.958812\n",
      "====> Epoch: 484 Average loss: 161.5380\n",
      "====> Test set loss: 164.3270\n",
      "Train Epoch: 485 [0/697932 (0%)]\tLoss: 162.002875\n",
      "Train Epoch: 485 [100000/697932 (14%)]\tLoss: 162.196406\n",
      "Train Epoch: 485 [200000/697932 (29%)]\tLoss: 161.656641\n",
      "Train Epoch: 485 [300000/697932 (43%)]\tLoss: 156.190484\n",
      "Train Epoch: 485 [400000/697932 (57%)]\tLoss: 159.958156\n",
      "Train Epoch: 485 [500000/697932 (72%)]\tLoss: 159.318813\n",
      "Train Epoch: 485 [600000/697932 (86%)]\tLoss: 163.930531\n",
      "====> Epoch: 485 Average loss: 161.5595\n",
      "====> Test set loss: 164.3304\n",
      "Train Epoch: 486 [0/697932 (0%)]\tLoss: 160.194562\n",
      "Train Epoch: 486 [100000/697932 (14%)]\tLoss: 161.754125\n",
      "Train Epoch: 486 [200000/697932 (29%)]\tLoss: 158.282219\n",
      "Train Epoch: 486 [300000/697932 (43%)]\tLoss: 160.641000\n",
      "Train Epoch: 486 [400000/697932 (57%)]\tLoss: 162.113094\n",
      "Train Epoch: 486 [500000/697932 (72%)]\tLoss: 161.984391\n",
      "Train Epoch: 486 [600000/697932 (86%)]\tLoss: 160.250859\n",
      "====> Epoch: 486 Average loss: 161.4819\n",
      "====> Test set loss: 164.6752\n",
      "Train Epoch: 487 [0/697932 (0%)]\tLoss: 160.978016\n",
      "Train Epoch: 487 [100000/697932 (14%)]\tLoss: 161.998406\n",
      "Train Epoch: 487 [200000/697932 (29%)]\tLoss: 160.929953\n",
      "Train Epoch: 487 [300000/697932 (43%)]\tLoss: 161.979531\n",
      "Train Epoch: 487 [400000/697932 (57%)]\tLoss: 161.396938\n",
      "Train Epoch: 487 [500000/697932 (72%)]\tLoss: 163.246453\n",
      "Train Epoch: 487 [600000/697932 (86%)]\tLoss: 163.657328\n",
      "====> Epoch: 487 Average loss: 161.5798\n",
      "====> Test set loss: 164.3932\n",
      "Train Epoch: 488 [0/697932 (0%)]\tLoss: 160.478312\n",
      "Train Epoch: 488 [100000/697932 (14%)]\tLoss: 162.477547\n",
      "Train Epoch: 488 [200000/697932 (29%)]\tLoss: 160.089578\n",
      "Train Epoch: 488 [300000/697932 (43%)]\tLoss: 161.178047\n",
      "Train Epoch: 488 [400000/697932 (57%)]\tLoss: 159.554859\n",
      "Train Epoch: 488 [500000/697932 (72%)]\tLoss: 163.293375\n",
      "Train Epoch: 488 [600000/697932 (86%)]\tLoss: 159.765844\n",
      "====> Epoch: 488 Average loss: 161.5327\n",
      "====> Test set loss: 164.2460\n",
      "Train Epoch: 489 [0/697932 (0%)]\tLoss: 159.536156\n",
      "Train Epoch: 489 [100000/697932 (14%)]\tLoss: 158.731109\n",
      "Train Epoch: 489 [200000/697932 (29%)]\tLoss: 160.690125\n",
      "Train Epoch: 489 [300000/697932 (43%)]\tLoss: 161.762672\n",
      "Train Epoch: 489 [400000/697932 (57%)]\tLoss: 163.170938\n",
      "Train Epoch: 489 [500000/697932 (72%)]\tLoss: 159.726641\n",
      "Train Epoch: 489 [600000/697932 (86%)]\tLoss: 160.019563\n",
      "====> Epoch: 489 Average loss: 161.4915\n",
      "====> Test set loss: 164.3281\n",
      "Train Epoch: 490 [0/697932 (0%)]\tLoss: 159.060969\n",
      "Train Epoch: 490 [100000/697932 (14%)]\tLoss: 161.835281\n",
      "Train Epoch: 490 [200000/697932 (29%)]\tLoss: 161.870797\n",
      "Train Epoch: 490 [300000/697932 (43%)]\tLoss: 164.226766\n",
      "Train Epoch: 490 [400000/697932 (57%)]\tLoss: 160.805625\n",
      "Train Epoch: 490 [500000/697932 (72%)]\tLoss: 160.053812\n",
      "Train Epoch: 490 [600000/697932 (86%)]\tLoss: 162.416328\n",
      "====> Epoch: 490 Average loss: 161.5431\n",
      "====> Test set loss: 164.4754\n",
      "Train Epoch: 491 [0/697932 (0%)]\tLoss: 160.885063\n",
      "Train Epoch: 491 [100000/697932 (14%)]\tLoss: 159.706437\n",
      "Train Epoch: 491 [200000/697932 (29%)]\tLoss: 159.813656\n",
      "Train Epoch: 491 [300000/697932 (43%)]\tLoss: 159.257828\n",
      "Train Epoch: 491 [400000/697932 (57%)]\tLoss: 161.168359\n",
      "Train Epoch: 491 [500000/697932 (72%)]\tLoss: 161.777547\n",
      "Train Epoch: 491 [600000/697932 (86%)]\tLoss: 162.800625\n",
      "====> Epoch: 491 Average loss: 161.6929\n",
      "====> Test set loss: 164.4413\n",
      "Train Epoch: 492 [0/697932 (0%)]\tLoss: 164.728391\n",
      "Train Epoch: 492 [100000/697932 (14%)]\tLoss: 161.960438\n",
      "Train Epoch: 492 [200000/697932 (29%)]\tLoss: 163.156609\n",
      "Train Epoch: 492 [300000/697932 (43%)]\tLoss: 163.527328\n",
      "Train Epoch: 492 [400000/697932 (57%)]\tLoss: 161.812172\n",
      "Train Epoch: 492 [500000/697932 (72%)]\tLoss: 159.452313\n",
      "Train Epoch: 492 [600000/697932 (86%)]\tLoss: 160.663328\n",
      "====> Epoch: 492 Average loss: 161.6193\n",
      "====> Test set loss: 164.3500\n",
      "Train Epoch: 493 [0/697932 (0%)]\tLoss: 163.259703\n",
      "Train Epoch: 493 [100000/697932 (14%)]\tLoss: 160.966578\n",
      "Train Epoch: 493 [200000/697932 (29%)]\tLoss: 160.520312\n",
      "Train Epoch: 493 [300000/697932 (43%)]\tLoss: 158.556219\n",
      "Train Epoch: 493 [400000/697932 (57%)]\tLoss: 161.923156\n",
      "Train Epoch: 493 [500000/697932 (72%)]\tLoss: 159.379125\n",
      "Train Epoch: 493 [600000/697932 (86%)]\tLoss: 163.423281\n",
      "====> Epoch: 493 Average loss: 161.5681\n",
      "====> Test set loss: 164.6471\n",
      "Train Epoch: 494 [0/697932 (0%)]\tLoss: 163.886250\n",
      "Train Epoch: 494 [100000/697932 (14%)]\tLoss: 160.168453\n",
      "Train Epoch: 494 [200000/697932 (29%)]\tLoss: 161.216391\n",
      "Train Epoch: 494 [300000/697932 (43%)]\tLoss: 163.108781\n",
      "Train Epoch: 494 [400000/697932 (57%)]\tLoss: 162.842844\n",
      "Train Epoch: 494 [500000/697932 (72%)]\tLoss: 158.604562\n",
      "Train Epoch: 494 [600000/697932 (86%)]\tLoss: 160.717016\n",
      "====> Epoch: 494 Average loss: 161.6394\n",
      "====> Test set loss: 164.5219\n",
      "Train Epoch: 495 [0/697932 (0%)]\tLoss: 160.285750\n",
      "Train Epoch: 495 [100000/697932 (14%)]\tLoss: 159.696672\n",
      "Train Epoch: 495 [200000/697932 (29%)]\tLoss: 162.103156\n",
      "Train Epoch: 495 [300000/697932 (43%)]\tLoss: 162.191281\n",
      "Train Epoch: 495 [400000/697932 (57%)]\tLoss: 159.810328\n",
      "Train Epoch: 495 [500000/697932 (72%)]\tLoss: 161.252281\n",
      "Train Epoch: 495 [600000/697932 (86%)]\tLoss: 160.243594\n",
      "====> Epoch: 495 Average loss: 161.6226\n",
      "====> Test set loss: 164.4092\n",
      "Train Epoch: 496 [0/697932 (0%)]\tLoss: 159.154719\n",
      "Train Epoch: 496 [100000/697932 (14%)]\tLoss: 163.409922\n",
      "Train Epoch: 496 [200000/697932 (29%)]\tLoss: 160.979469\n",
      "Train Epoch: 496 [300000/697932 (43%)]\tLoss: 160.699813\n",
      "Train Epoch: 496 [400000/697932 (57%)]\tLoss: 159.145688\n",
      "Train Epoch: 496 [500000/697932 (72%)]\tLoss: 160.098047\n",
      "Train Epoch: 496 [600000/697932 (86%)]\tLoss: 160.658203\n",
      "====> Epoch: 496 Average loss: 161.5662\n",
      "====> Test set loss: 164.4624\n",
      "Train Epoch: 497 [0/697932 (0%)]\tLoss: 163.290828\n",
      "Train Epoch: 497 [100000/697932 (14%)]\tLoss: 163.760672\n",
      "Train Epoch: 497 [200000/697932 (29%)]\tLoss: 162.603422\n",
      "Train Epoch: 497 [300000/697932 (43%)]\tLoss: 159.963953\n",
      "Train Epoch: 497 [400000/697932 (57%)]\tLoss: 164.393281\n",
      "Train Epoch: 497 [500000/697932 (72%)]\tLoss: 162.913609\n",
      "Train Epoch: 497 [600000/697932 (86%)]\tLoss: 162.988469\n",
      "====> Epoch: 497 Average loss: 161.6829\n",
      "====> Test set loss: 164.4061\n",
      "Train Epoch: 498 [0/697932 (0%)]\tLoss: 162.275219\n",
      "Train Epoch: 498 [100000/697932 (14%)]\tLoss: 162.975328\n",
      "Train Epoch: 498 [200000/697932 (29%)]\tLoss: 163.793750\n",
      "Train Epoch: 498 [300000/697932 (43%)]\tLoss: 163.009031\n",
      "Train Epoch: 498 [400000/697932 (57%)]\tLoss: 161.258594\n",
      "Train Epoch: 498 [500000/697932 (72%)]\tLoss: 161.164625\n",
      "Train Epoch: 498 [600000/697932 (86%)]\tLoss: 161.192172\n",
      "====> Epoch: 498 Average loss: 161.6123\n",
      "====> Test set loss: 164.6104\n",
      "Train Epoch: 499 [0/697932 (0%)]\tLoss: 162.916344\n",
      "Train Epoch: 499 [100000/697932 (14%)]\tLoss: 159.668172\n",
      "Train Epoch: 499 [200000/697932 (29%)]\tLoss: 161.937359\n",
      "Train Epoch: 499 [300000/697932 (43%)]\tLoss: 161.808156\n",
      "Train Epoch: 499 [400000/697932 (57%)]\tLoss: 162.933922\n",
      "Train Epoch: 499 [500000/697932 (72%)]\tLoss: 160.984625\n",
      "Train Epoch: 499 [600000/697932 (86%)]\tLoss: 161.637172\n",
      "====> Epoch: 499 Average loss: 161.5303\n",
      "====> Test set loss: 164.2771\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "for epoch in range(0, num_epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    #Make many examples of latent space stuff\n",
    "    for i in range(10):\n",
    "        z = torch.randn( 64, Z_dim).cuda()\n",
    "        sample = vae.decoder(z).cuda()\n",
    "\n",
    "        save_image(sample.view(64, 1, 28, 28), './samples/sample_' +str(i) +'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae,'./trained_models/latest_model_500_epochs.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interum cell can be ignored\n",
    "with torch.no_grad():\n",
    "\n",
    "    #What does a typical A and a encoded and decoded look like\n",
    "    #for i in range(10):\n",
    "\n",
    "\n",
    "        test_image_A, test_target_A = test_dataset[79]\n",
    "\n",
    "        z = torch.randn( 64, Z_dim).cuda()\n",
    "        \n",
    "        sample = vae.decoder(z).cuda()\n",
    "        \n",
    "        sample.view(64, 1, 28, 28).show()\n",
    "        #save_image(sample.view(64, 1, 28, 28), './samples/sample_' +str(i) +'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 40\n",
      "1 32\n",
      "2 4\n",
      "3 8\n",
      "4 2\n",
      "5 1\n",
      "6 6\n",
      "7 39\n",
      "8 9\n",
      "9 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/813719249.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "val = 79\n",
    "num_examples = 10\n",
    "for x in range(num_examples):\n",
    "    test_image_A, test_target_A = test_dataset[val + x]\n",
    "    #(test_image_A.view(1,1,28,28))\n",
    "    print(x, test_target_A)\n",
    "    #save_image(test_image_A.view(1, 1, 28, 28), './examples/example_input_'+str(val)+',' +str(x)+'.png')\n",
    "\n",
    "    #Shape input into what the network wants\n",
    "    z = torch.flatten(test_image_A)#.cuda()\n",
    "    z = torch.tensor(z)\n",
    "    z = z.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    latent_sample =  vae.encoder(z)[0]\n",
    "    sample = vae.forward(z)[0].cuda()\n",
    "    direct_sample = vae.decoder(latent_sample)#[0]\n",
    "\n",
    "    #save_image(sample.view(1, 1, 28, 28), './examples/example_decoded_'+str(val)+',' +str(x)+'.png')\n",
    "\n",
    "    comb_tensor = torch.cat((z, sample[0], direct_sample ) ).cuda()\n",
    "    save_image(comb_tensor.view(3, 1, 28, 28), './examples/example_endecoded_'+str(val)+',' +str(x)+'.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/3543351417.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "#Capital A label is 10\n",
    "#lower case a label is 36\n",
    "val = 0\n",
    "test_target_A  =1\n",
    "while test_target_A != 36:\n",
    "    test_image_A, test_target_A = test_dataset[val]\n",
    "    val += 1\n",
    "    if val%100 == 0:\n",
    "        print(val)\n",
    "\n",
    "\n",
    "print(x, test_target_A)\n",
    "#save_image(test_image_A.view(1, 1, 28, 28), './examples/example_input_'+str(val)+',' +str(x)+'.png')\n",
    "\n",
    "#Shape input into what the network wants\n",
    "z = torch.flatten(test_image_A)#.cuda()\n",
    "z = torch.tensor(z)\n",
    "z = z.cuda()\n",
    "\n",
    "#Map through the model to the latent space and the whole way through\n",
    "latent_sample =  vae.encoder(z)[0]\n",
    "sample = vae.forward(z)[0].cuda()\n",
    "direct_sample = vae.decoder(latent_sample)#[0]\n",
    "\n",
    "#save_image(sample.view(1, 1, 28, 28), './examples/example_decoded_'+str(val)+',' +str(x)+'.png')\n",
    "\n",
    "comb_tensor = torch.cat((z, sample[0], direct_sample ) ).cuda()\n",
    "save_image(comb_tensor.view(3, 1, 28, 28), './examples/example_endecoded_'+str(val)+',' +str(x)+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capital A label is 10\n",
    "#lower case a label is 36\n",
    "def all_test_images_indexes_for_label(target_label):\n",
    "    indexes = []\n",
    "    for index in range(len(test_dataset)): \n",
    "        image, label = test_dataset[index]\n",
    "        if label == target_label:\n",
    "            indexes.append(index)\n",
    "    return indexes\n",
    "\n",
    "def master_image_label_map():\n",
    "    master_label = []\n",
    "    for index in range(len(test_dataset)): \n",
    "        image, label = test_dataset[index]\n",
    "        master_label.append(label)\n",
    "    return master_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 32, 4, 8, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "Label_Map = master_image_label_map()\n",
    "print(Label_Map[79:85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[195, 303, 497, 738, 791, 798, 862, 880, 950, 1091, 1274, 1332, 1535, 1839, 1928, 1951, 2130, 2393, 2402, 3009, 3153, 3198, 3506, 3763, 4116, 4204, 4231, 4233, 4380, 4381, 4442, 4534, 4574, 4650, 4763, 5035, 5118, 5203, 5556, 5571, 5693, 5803, 5904, 5907, 6048, 6058, 6088, 6128, 6271, 6445, 6519, 6626, 6653, 6655, 6660, 6739, 6833, 6913, 7040, 7092, 7131, 7407, 7435, 7440, 7512, 7879, 7992, 8046, 8087, 8179, 8471, 8491, 8556, 8673, 8818, 8867, 8938, 9022, 9040, 9125, 9194, 9291, 9490, 9546, 9687, 9756, 9844, 9914, 9933, 9955, 10050, 10108, 10149, 10323, 10351, 10428, 10572, 10578, 10671, 10700, 10735, 11015, 11345, 11346, 11347, 11610, 11742, 11874, 11878, 12019, 12029, 12036, 12179, 12294, 12295, 12304, 12621, 12777, 13163, 13194, 13199, 13350, 13635, 13670, 13820, 13846, 13864, 13915, 14055, 14101, 14162, 14350, 14387, 14443, 14696, 14917, 15077, 15136, 15175, 15248, 15287, 15364, 15370, 15495, 15513, 15612, 15701, 15734, 15739, 15750, 15862, 16001, 16198, 16266, 16376, 16406, 16433, 16468, 16621, 16628, 16642, 16951, 17053, 17215, 17237, 17322, 17394, 17477, 17543, 17557, 17603, 17649, 17692, 17909, 17982, 18281, 18299, 18308, 18367, 18502, 18523, 18721, 18799, 18809, 18853, 18864, 18881, 19025, 19203, 19343, 19565, 19641, 19939, 19946, 20033, 20124, 20170, 20199, 20438, 20448, 20479, 20583, 20586, 20755, 21178, 21548, 22236, 22238, 22261, 22304, 22380, 22470, 22669, 22671, 22858, 22918, 23032, 23169, 23526, 23605, 23974, 24350, 24385, 24404, 24452, 24483, 24580, 24943, 24955, 24994, 25113, 25226, 25280, 25335, 25455, 25471, 25497, 25498, 25526, 25636, 25653, 25662, 25664, 25742, 25973, 26000, 26057, 26143, 26156, 26238, 26288, 26320, 26663, 26871, 26910, 27128, 27191, 27328, 27394, 27500, 27526, 27563, 27629, 27676, 27702, 27754, 27758, 28066, 28080, 28561, 28629, 28718, 28775, 28896, 29016, 29144, 29277, 29352, 29392, 29429, 29498, 29600, 29608, 29933, 30161, 30277, 30637, 30726, 30806, 31007, 31231, 31338, 31371, 31442, 31454, 31473, 31866, 31990, 32047, 32071, 32073, 32166, 32210, 32445, 32587, 32599, 32826, 32863, 32935, 32964, 33366, 33636, 33717, 33806, 33890, 34015, 34093, 34276, 34296, 34314, 34383, 34583, 34679, 34789, 34870, 35188, 35305, 35325, 35428, 35595, 35735, 35803, 35875, 35948, 35961, 36001, 36131, 36198, 36411, 36495, 36709, 36725, 37266, 37343, 37520, 37624, 37659, 37663, 37667, 37864, 38276, 38392, 38526, 38712, 38730, 38880, 38962, 39128, 39338, 39396, 39435, 39782, 39855, 39947, 40175, 40221, 40313, 40490, 40667, 40675, 40755, 40897, 41096, 41148, 41421, 41658, 41782, 41879, 41958, 41959, 42010, 42020, 42118, 42122, 42648, 42750, 42821, 42861, 43287, 43374, 43460, 43520, 43557, 43703, 43770, 43975, 44065, 44082, 44084, 44255, 44284, 44399, 44484, 44593, 45138, 45394, 45474, 45701, 45722, 45902, 45931, 46142, 46206, 46248, 46327, 46337, 46438, 46530, 46632, 46649, 46845, 46912, 46982, 46991, 47136, 47195, 47210, 47226, 47271, 47294, 47323, 47553, 48058, 48150, 48153, 48479, 48504, 48543, 48626, 48676, 48824, 48958, 48961, 49082, 49184, 49336, 49391, 49617, 49632, 49647, 49718, 49724, 49728, 49971, 50085, 50164, 50334, 50362, 50401, 50504, 50606, 50731, 50991, 51007, 51200, 51278, 51292, 51411, 51433, 51508, 51763, 51840, 51900, 51948, 51953, 52182, 52317, 52686, 52760, 52786, 52790, 52960, 53028, 53119, 53387, 53415, 53445, 53487, 53525, 53584, 53595, 53597, 53793, 53797, 53851, 53901, 53974, 53979, 54084, 54152, 54373, 54374, 54910, 54929, 54933, 55030, 55046, 55088, 55226, 55236, 55496, 55550, 55798, 55804, 55829, 55865, 55926, 55989, 56036, 56125, 56284, 56340, 56369, 56412, 56441, 56901, 57094, 57197, 57297, 57586, 57609, 57620, 57651, 57699, 57757, 57759, 57828, 57882, 57930, 58025, 58063, 58091, 58354, 58576, 58844, 58925, 59124, 59189, 59292, 59361, 59534, 59589, 59664, 59712, 59750, 60017, 60478, 60504, 60529, 60541, 60610, 60749, 61284, 61407, 61494, 61704, 61771, 61827, 62022, 62089, 62127, 62300, 62388, 62415, 62515, 62596, 62668, 62682, 62833, 62939, 62992, 63049, 63212, 63224, 63382, 63711, 63789, 63815, 63897, 63928, 64037, 64051, 64091, 64164, 64179, 64216, 64342, 64530, 64833, 65175, 65205, 65282, 65632, 65732, 65787, 65855, 66058, 66223, 66234, 66289, 66346, 66479, 66512, 66526, 66535, 66698, 66947, 67024, 67135, 67186, 67360, 67578, 67827, 67906, 68007, 68012, 68130, 68202, 68215, 68422, 68755, 68766, 68818, 68872, 68959, 69034, 69061, 69106, 69166, 69305, 69319, 69320, 69531, 69580, 69649, 69719, 69953, 70066, 70226, 70383, 70417, 70464, 70619, 70668, 70697, 70858, 70960, 70986, 71258, 71279, 71457, 71468, 71648, 71785, 71860, 71905, 71928, 71955, 72213, 72225, 72463, 72548, 72601, 72668, 72824, 72981, 73027, 73147, 73212, 73573, 73821, 73838, 73996, 74174, 74187, 74575, 74659, 74763, 74830, 74847, 75181, 75282, 75385, 75411, 75510, 75580, 75709, 75814, 75842, 75947, 76143, 76315, 76645, 76710, 76750, 76845, 76878, 77099, 77153, 77180, 77653, 77901, 78198, 78328, 78449, 78483, 78830, 78933, 79045, 79059, 79067, 79238, 79306, 79384, 79415, 79649, 79958, 79997, 80050, 80138, 80159, 80163, 80306, 80364, 80449, 80612, 80769, 80831, 80832, 80956, 81114, 81271, 81285, 81314, 81333, 81428, 81519, 81566, 81642, 81915, 82134, 82162, 82439, 82530, 82542, 82670, 82812, 82855, 82970, 83423, 83495, 83772, 83868, 83869, 84034, 84679, 84684, 84922, 84984, 85021, 85416, 85443, 85640, 85918, 86108, 86131, 86356, 86438, 86617, 86698, 86704, 87004, 87118, 87298, 87395, 87434, 87454, 87577, 87581, 87812, 87948, 87977, 88008, 88029, 88239, 88305, 88357, 88362, 88389, 88545, 88744, 88817, 89200, 89482, 89542, 89605, 89606, 89628, 89661, 90057, 90190, 90355, 90443, 90448, 90485, 90634, 90764, 90921, 90974, 91001, 91176, 91278, 91508, 91866, 91890, 91898, 92209, 92248, 92266, 92399, 92458, 92532, 92767, 92919, 93014, 93072, 93247, 93287, 93543, 93831, 93850, 93916, 93966, 94184, 94220, 94240, 94322, 94497, 94533, 94650, 94667, 94672, 94735, 95222, 95362, 95686, 95766, 95777, 95778, 95798, 95808, 95960, 96001, 96026, 96156, 96639, 96741, 96941, 97023, 97143, 97250, 97327, 97330, 97423, 97862, 97901, 97919, 98255, 98288, 98341, 98384, 98395, 98485, 98725, 98748, 98953, 99089, 99142, 99296, 99449, 99482, 99587, 99621, 99641, 99697, 99703, 99792, 99845, 99943, 100070, 100074, 100129, 100204, 100238, 100285, 100350, 100380, 100386, 100415, 100554, 100831, 100888, 100988, 101047, 101195, 101252, 101379, 101384, 101799, 101961, 102025, 102040, 102072, 102439, 102825, 102854, 103021, 103028, 103044, 103341, 103354, 103416, 103622, 103778, 103796, 103981, 104074, 104103, 104128, 104185, 104277, 104370, 104626, 105062, 105095, 105173, 105212, 105214, 105224, 105361, 105458, 105657, 105658, 105738, 105795, 105809, 105906, 106158, 106241, 106324, 106399, 106464, 106524, 106615, 106802, 107037, 107182, 107186, 107264, 107342, 107351, 107520, 107828, 107835, 107870, 107926, 107991, 108103, 108123, 108357, 108376, 108404, 108425, 108513, 108541, 108689, 108811, 108927, 108931, 108986, 109133, 109240, 109446, 109448, 109612, 109617, 109868, 109907, 110127, 110304, 110359, 110429, 110472, 110875, 110995, 111012, 111074, 111115, 111131, 111142, 111398, 111577, 111641, 111697, 111830, 111872, 111951, 112176, 112187, 112307, 112430, 112491, 112504, 112539, 112554, 112684, 112712, 112780, 112810, 112884, 112919, 112937, 113004, 113088, 113442, 113462, 113504, 113536, 113602, 113685, 113902, 114019, 114030, 114306, 114480, 114575, 114679, 114727, 114967, 114988, 115150, 115338, 115346, 115356, 115550, 115666, 115728, 115744, 115756, 115772, 116078, 116181]\n"
     ]
    }
   ],
   "source": [
    "print( all_test_images_indexes_for_label(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "labels_to_indexes = {}\n",
    "for label in range(62):\n",
    "    labels_to_indexes[label] = [index for index in range(len(Label_Map)) if Label_Map[index]==label]\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/2319603254.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "A_indexes = labels_to_indexes[10]\n",
    "\n",
    "for val in A_indexes[:10]:\n",
    "    test_image_A, test_target_A = test_dataset[val]\n",
    "\n",
    "    #Shape input into what the network wants\n",
    "    z = torch.flatten(test_image_A)#.cuda()\n",
    "    z = torch.tensor(z)\n",
    "    z = z.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    latent_sample =  vae.encoder(z)[0]\n",
    "    sample = vae.forward(z)[0].cuda()\n",
    "    direct_sample = vae.decoder(latent_sample)#[0]\n",
    "\n",
    "    #save_image(sample.view(1, 1, 28, 28), './examples/example_decoded_'+str(val)+',' +str(x)+'.png')\n",
    "\n",
    "    comb_tensor = torch.cat((z, sample[0], direct_sample ) ).cuda()\n",
    "    save_image(comb_tensor.view(3, 1, 28, 28), './examples/A/example_endecoded_'+str(val)+',' +str(x)+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/1352081597.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A_indexes = labels_to_indexes[36]\n",
    "\n",
    "for val in A_indexes[:10]:\n",
    "    test_image_A, test_target_A = test_dataset[val]\n",
    "\n",
    "    #Shape input into what the network wants\n",
    "    z = torch.flatten(test_image_A)#.cuda()\n",
    "    z = torch.tensor(z)\n",
    "    z = z.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    latent_sample =  vae.encoder(z)[0]\n",
    "    sample = vae.forward(z)[0].cuda()\n",
    "    direct_sample = vae.decoder(latent_sample)#[0]\n",
    "\n",
    "    comb_tensor = torch.cat((z, sample[0], direct_sample ) ).cuda()\n",
    "    save_image(comb_tensor.view(3, 1, 28, 28), './examples/a/example_endecoded_'+str(val)+',' +str(x)+'.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_latent_space_vector(Indexes):\n",
    "    total_sample = torch.zeros(Z_dim).cuda()\n",
    "    for index in Indexes:\n",
    "        test_image_A, test_target_A = test_dataset[index]\n",
    "\n",
    "        #Shape input into what the network wants\n",
    "        z = torch.flatten(test_image_A)\n",
    "        z = torch.tensor(z)\n",
    "        z = z.cuda()\n",
    "\n",
    "        #Map through the model to the latent space \n",
    "        latent_sample =  vae.encoder(z)[0]\n",
    "        total_sample = torch.add(latent_sample, total_sample)\n",
    "    \n",
    "    return total_sample/len(Indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/2705388341.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.4356, -0.9968, -0.4837,  0.5196], device='cuda:0',\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "A_indexes = labels_to_indexes[10]\n",
    "a_indexes = labels_to_indexes[36]\n",
    "\n",
    "A_mean = mean_latent_space_vector(A_indexes)\n",
    "a_mean = mean_latent_space_vector(a_indexes)\n",
    "\n",
    "lower_to_upper = A_mean-a_mean\n",
    "lower_to_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/612106745.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "#using lower to upper to change from a to A\n",
    "a_indexes = labels_to_indexes[36]\n",
    "\n",
    "for val in a_indexes[:30]:\n",
    "    test_image_A, test_target_A = test_dataset[val]\n",
    "\n",
    "    #Shape input into what the network wants\n",
    "    z = torch.flatten(test_image_A)#.cuda()\n",
    "    z = torch.tensor(z)\n",
    "    z = z.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    latent_sample =  vae.encoder(z)[0]\n",
    "\n",
    "    sample = vae.forward(z)[0].cuda()\n",
    "\n",
    "    adjusted_latent = torch.add(latent_sample , lower_to_upper)\n",
    "    direct_sample = vae.decoder(adjusted_latent)\n",
    "\n",
    "    comb_tensor = torch.cat((z, sample[0], direct_sample ) ).cuda()\n",
    "    save_image(comb_tensor.view(3, 1, 28, 28), './examples/lower_to_upper/example_a'+str(val)+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/3369991721.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#using lower to upper to go backwards from A to a\n",
    "A_indexes = labels_to_indexes[10]\n",
    "\n",
    "for val in A_indexes[:30]:\n",
    "    test_image_A, test_target_A = test_dataset[val]\n",
    "\n",
    "    #Shape input into what the network wants\n",
    "    z = torch.flatten(test_image_A)#.cuda(k\n",
    "    z = torch.tensor(z)\n",
    "    z = z.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    latent_sample =  vae.encoder(z)[0]\n",
    "\n",
    "    sample = vae.forward(z)[0].cuda()\n",
    "\n",
    "    adjusted_latent = torch.add(latent_sample , -lower_to_upper)\n",
    "    direct_sample = vae.decoder(adjusted_latent)\n",
    "\n",
    "    comb_tensor = torch.cat((z, sample[0], direct_sample ) ).cuda()\n",
    "    save_image(comb_tensor.view(3, 1, 28, 28), './examples/upper_to_lower/example_A'+str(val)+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/2509097139.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "#BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB\n",
    "#using lower to upper to go backwards from B to b\n",
    "A_indexes = labels_to_indexes[11]\n",
    "\n",
    "for val in A_indexes[:30]:\n",
    "    test_image_A, test_target_A = test_dataset[val]\n",
    "\n",
    "    #Shape input into what the network wants\n",
    "    z = torch.flatten(test_image_A)#.cuda(k\n",
    "    z = torch.tensor(z)\n",
    "    z = z.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    latent_sample =  vae.encoder(z)[0]\n",
    "\n",
    "    sample = vae.forward(z)[0].cuda()\n",
    "\n",
    "    adjusted_latent = torch.add(latent_sample , -lower_to_upper)\n",
    "    direct_sample = vae.decoder(adjusted_latent)\n",
    "\n",
    "    comb_tensor = torch.cat((z, sample[0], direct_sample ) ).cuda()\n",
    "    save_image(comb_tensor.view(3, 1, 28, 28), './examples/upper_to_lower/example_B'+str(val)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/577651449.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "#Average A Image is the last of the triple, first 2 images are the first example of letter in dataset\n",
    "letter_index = 10\n",
    "A_indexes = labels_to_indexes[letter_index]\n",
    "\n",
    "test_image_A, test_target_A = test_dataset[A_indexes[0]]\n",
    "\n",
    "#Shape input into what the network wants\n",
    "z = torch.flatten(test_image_A)#.cuda(k\n",
    "z = torch.tensor(z)\n",
    "z = z.cuda()\n",
    "\n",
    "#Map through the model to the latent space and the whole way through\n",
    "latent_sample =  A_mean\n",
    "\n",
    "sample = vae.forward(z)[0].cuda()\n",
    "\n",
    "direct_sample = vae.decoder(latent_sample)\n",
    "\n",
    "comb_tensor = torch.cat((z, sample[0], direct_sample ) ).cuda()\n",
    "save_image(comb_tensor.view(3, 1, 28, 28), './examples/average_letter/example_A'+str(val)+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/1103046019.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "#MIDDLE CASE A to a\n",
    "#using lower to upper to go backwards from A to middle case a\n",
    "A_indexes = labels_to_indexes[10]\n",
    "\n",
    "for val in A_indexes[:30]:\n",
    "    test_image_A, test_target_A = test_dataset[val]\n",
    "\n",
    "    #Shape input into what the network wants\n",
    "    z = torch.flatten(test_image_A)#.cuda(k\n",
    "    z = torch.tensor(z)\n",
    "    z = z.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    latent_sample =  vae.encoder(z)[0]\n",
    "\n",
    "    sample = vae.forward(z)[0].cuda()\n",
    "\n",
    "    adjusted_latent = torch.add(latent_sample , ((-1/2)*lower_to_upper) )\n",
    "    direct_sample = vae.decoder(adjusted_latent)\n",
    "\n",
    "    comb_tensor = torch.cat((z, sample[0], direct_sample ) ).cuda()\n",
    "    save_image(comb_tensor.view(3, 1, 28, 28), './examples/middle_case/example_A'+str(val)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/1158315434.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Average A Image is the middle of the triple, last is middle case A\n",
    "letter_index = 10\n",
    "A_indexes = labels_to_indexes[letter_index]\n",
    "\n",
    "test_image_A, test_target_A = test_dataset[A_indexes[0]]\n",
    "\n",
    "#Shape input into what the network wants\n",
    "z = torch.flatten(test_image_A)#.cuda(k\n",
    "z = torch.tensor(z)\n",
    "z = z.cuda()\n",
    "\n",
    "#Map through the model to the latent space and the whole way through\n",
    "latent_sample =  A_mean\n",
    "\n",
    "sample = vae.decoder(latent_sample)\n",
    "\n",
    "adjusted_latent = torch.add(latent_sample , ((-1/2)*lower_to_upper) )\n",
    "direct_sample = vae.decoder(adjusted_latent).cuda()\n",
    "\n",
    "comb_tensor = torch.cat( (z, sample, direct_sample ) ).cuda()\n",
    "save_image(comb_tensor.view(3, 1, 28, 28), './examples/middle_case/example_mean_A'+str(val)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/2705388341.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "#Generate dict of the mean differences between lowercase and uppercase letters one at a time\n",
    "#uses indexes of the capital letters\n",
    "lower_to_upper_dict = {}\n",
    "Upper_means = {}\n",
    "Lower_means = {}\n",
    "\n",
    "for offset in range(26):\n",
    "    A_indexes = labels_to_indexes[10 + offset]\n",
    "    a_indexes = labels_to_indexes[36 + offset]\n",
    "\n",
    "    A_mean = mean_latent_space_vector(A_indexes)\n",
    "    a_mean = mean_latent_space_vector(a_indexes)\n",
    "\n",
    "    lower_to_upper = A_mean-a_mean\n",
    "    lower_to_upper_dict[10+offset] = lower_to_upper\n",
    "    Upper_means[10+offset] = A_mean\n",
    "    Lower_means[36+offset] = a_mean\n",
    "\n",
    "    print(offset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{10: tensor([-0.4356, -0.9968, -0.4837,  0.5196], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 11: tensor([1.5060, 0.3430, 0.1606, 0.8291], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 12: tensor([ 0.0560, -0.1615, -0.0261,  0.2118], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 13: tensor([ 0.8656,  1.2986,  1.3437, -0.5851], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 14: tensor([ 0.6133, -0.7716, -0.1098,  0.8886], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 15: tensor([ 0.3895,  0.1158, -0.2041,  0.0749], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 16: tensor([-0.0719,  1.0673,  0.0767, -0.0458], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 17: tensor([ 1.1480,  0.4086, -0.6208,  0.1472], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 18: tensor([0.2524, 0.2773, 0.0160, 0.2414], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 19: tensor([ 2.1362,  1.0899, -0.0610, -0.2231], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 20: tensor([ 0.2955,  0.1250, -0.1250, -0.0835], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 21: tensor([-0.2122,  1.4493,  1.8807,  0.6714], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 22: tensor([-0.0492, -0.0775,  0.1392,  0.2276], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 23: tensor([ 0.2613,  0.0564, -0.4131,  0.7787], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 24: tensor([-0.1731, -0.0082, -0.0774,  0.0230], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 25: tensor([ 0.2098, -0.2489, -0.1840, -0.1001], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 26: tensor([ 0.1177,  1.3480, -0.2297,  0.3152], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 27: tensor([-1.2928,  0.3077,  1.6347,  0.5076], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 28: tensor([-0.0438, -0.0083, -0.1158,  0.0702], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 29: tensor([ 2.2085, -0.0355, -0.1120, -0.7399], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 30: tensor([-0.0081, -0.1410,  0.0531,  0.3975], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 31: tensor([-0.0256, -0.0378, -0.0645,  0.0433], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 32: tensor([ 0.0312, -0.1779,  0.2833,  0.1247], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 33: tensor([-0.1065, -0.1168,  0.1406,  0.1315], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 34: tensor([ 0.0812, -0.2539, -0.3601, -0.0153], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>), 35: tensor([ 0.3718, -0.0768,  0.5089,  0.5337], device='cuda:0',\n",
      "       grad_fn=<SubBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "print(lower_to_upper_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_uppercase_dict ={\n",
    "10:'A',\n",
    "11:'B',\n",
    "12:'C',\n",
    "13:'D',\n",
    "14:'E',\n",
    "15:'F',\n",
    "16:'G',\n",
    "17:'H',\n",
    "18:'I',\n",
    "19:'J',\n",
    "20:'K',\n",
    "21:'L',\n",
    "22:'M',\n",
    "23:'N',\n",
    "24:'O',\n",
    "25:'P',\n",
    "26:'Q',\n",
    "27:'R',\n",
    "28:'S',\n",
    "29:'T',\n",
    "30:'U',\n",
    "31:'V',\n",
    "32:'W',\n",
    "33:'X',\n",
    "34:'Y',\n",
    "35:'Z',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/4272264550.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n",
      "/tmp/ipykernel_66026/4272264550.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_lower = torch.tensor(z_lower)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Average A Image is the middle of the triple, last is middle case A\n",
    "for offset in range(26):\n",
    "    letter_index = 10 +offset\n",
    "    lower_index = 36 + offset\n",
    "\n",
    "    A_indexes = labels_to_indexes[letter_index]\n",
    "    a_indexes = labels_to_indexes[lower_index]\n",
    "\n",
    "    test_image_A, test_target_A = test_dataset[A_indexes[0]]\n",
    "    test_image_a, test_target_a = test_dataset[a_indexes[0]]\n",
    "\n",
    "    #Shape input into what the network wants\n",
    "    z = torch.flatten(test_image_A)#.cuda(k\n",
    "    z = torch.tensor(z)\n",
    "    z = z.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    latent_sample = Upper_means[letter_index] \n",
    "\n",
    "    sample = vae.decoder(latent_sample)\n",
    "    adjusted_latent = torch.add(latent_sample , ((-1/2)*lower_to_upper_dict[letter_index]) )\n",
    "    direct_sample = vae.decoder(adjusted_latent).cuda()\n",
    "\n",
    "\n",
    "    #repeat above with the lower letter [refactor into funciton?]\n",
    "    #Shape input into what the network wants\n",
    "    z_lower = torch.flatten(test_image_a)#.cuda(k\n",
    "    z_lower = torch.tensor(z_lower)\n",
    "    z_lower = z_lower.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    lower_latent_sample = Lower_means[lower_index] \n",
    "\n",
    "    lower_sample = vae.decoder(lower_latent_sample)\n",
    "\n",
    "    lower_adjusted_latent = torch.add(lower_latent_sample , (lower_to_upper_dict[letter_index]) )\n",
    "    lower_direct_sample = vae.decoder(lower_adjusted_latent).cuda()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    comb_tensor = torch.cat( (z, sample, direct_sample, z_lower, lower_sample,lower_direct_sample ) ).cuda()\n",
    "    comb_view = comb_tensor.view(6, 1, 28, 28)\n",
    "    comb_view = fix_tensor_for_image(comb_view)\n",
    "    save_image(comb_view, './examples/average_letter/first_mean_and_lower_'+str(index_to_uppercase_dict[letter_index])+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_tensor_for_image(comb_view):\n",
    "    comb_view = torch.flip(comb_view,dims=[2])\n",
    "    comb_view = torch.rot90(comb_view,3,[2,3])\n",
    "    return comb_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/2077675637.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n",
      "/tmp/ipykernel_66026/2077675637.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z_lower = torch.tensor(z_lower)\n"
     ]
    }
   ],
   "source": [
    "#Mega image\n",
    "#Average A Image is the middle of the triple, last is middle case A\n",
    "Combined_View = torch.empty(0, 1, 28, 28).cuda()\n",
    "for offset in range(26):\n",
    "    letter_index = 10 +offset\n",
    "    lower_index = 36 + offset\n",
    "\n",
    "    A_indexes = labels_to_indexes[letter_index]\n",
    "    a_indexes = labels_to_indexes[lower_index]\n",
    "\n",
    "    test_image_A, test_target_A = test_dataset[A_indexes[0]]\n",
    "    test_image_a, test_target_a = test_dataset[a_indexes[0]]\n",
    "\n",
    "    #Shape input into what the network wants\n",
    "    z = torch.flatten(test_image_A)#.cuda(k\n",
    "    z = torch.tensor(z)\n",
    "    z = z.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    latent_sample = Upper_means[letter_index] \n",
    "\n",
    "    sample = vae.decoder(latent_sample)\n",
    "    adjusted_latent = torch.add(latent_sample , ((-1/2)*lower_to_upper_dict[letter_index]) )\n",
    "    direct_sample = vae.decoder(adjusted_latent).cuda()\n",
    "\n",
    "\n",
    "    #repeat above with the lower letter [refactor into funciton?]\n",
    "    #Shape input into what the network wants\n",
    "    z_lower = torch.flatten(test_image_a)#.cuda(k\n",
    "    z_lower = torch.tensor(z_lower)\n",
    "    z_lower = z_lower.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    lower_latent_sample = Lower_means[lower_index] \n",
    "\n",
    "    lower_sample = vae.decoder(lower_latent_sample)\n",
    "\n",
    "    lower_adjusted_latent = torch.add(lower_latent_sample , (lower_to_upper_dict[letter_index]) )\n",
    "    lower_direct_sample = vae.decoder(lower_adjusted_latent).cuda()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    comb_tensor = torch.cat( (z, sample, direct_sample, z_lower, lower_sample,lower_direct_sample ) ).cuda()\n",
    "    comb_view = comb_tensor.view(6, 1, 28, 28)\n",
    "    comb_view = fix_tensor_for_image(comb_view)\n",
    "    Combined_View = torch.cat((Combined_View, comb_view))\n",
    "\n",
    "save_image(Combined_View, './examples/average_letter/first_mean_and_lower_master.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/4096834.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "#Mega image of all middle case letters and first and mean\n",
    "Combined_View = torch.empty(0, 1, 28, 28).cuda()\n",
    "for offset in range(26):\n",
    "    letter_index = 10 +offset\n",
    "    A_indexes = labels_to_indexes[letter_index]\n",
    "    test_image_A, test_target_A = test_dataset[A_indexes[0]]\n",
    "\n",
    "    #Shape input into what the network wants\n",
    "    z = torch.flatten(test_image_A)#.cuda(k\n",
    "    z = torch.tensor(z)\n",
    "    z = z.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    latent_sample = Upper_means[letter_index] \n",
    "\n",
    "    sample = vae.decoder(latent_sample)\n",
    "    adjusted_latent = torch.add(latent_sample , ((-1/2)*lower_to_upper_dict[letter_index]) )\n",
    "    direct_sample = vae.decoder(adjusted_latent).cuda()\n",
    "\n",
    "    #comb_tensor = torch.cat( (z, sample, direct_sample) ).cuda()\n",
    "    comb_view = direct_sample.view(1, 1, 28, 28)\n",
    "    comb_view = fix_tensor_for_image(comb_view)\n",
    "    Combined_View = torch.cat((Combined_View, comb_view))\n",
    "\n",
    "save_image(Combined_View, './examples/average_letter/middle_master.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/3239536680.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "#Mega image of only middle case letters\n",
    "Combined_View = torch.empty(0, 1, 28, 28).cuda()\n",
    "for offset in range(26):\n",
    "    letter_index = 10 +offset\n",
    "    A_indexes = labels_to_indexes[letter_index]\n",
    "    test_image_A, test_target_A = test_dataset[A_indexes[0]]\n",
    "\n",
    "    #Shape input into what the network wants\n",
    "    z = torch.flatten(test_image_A)#.cuda(k\n",
    "    z = torch.tensor(z)\n",
    "    z = z.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    latent_sample = Upper_means[letter_index] \n",
    "\n",
    "    sample = vae.decoder(latent_sample)\n",
    "    adjusted_latent = torch.add(latent_sample , ((-1/2)*lower_to_upper_dict[letter_index]) )\n",
    "    direct_sample = vae.decoder(adjusted_latent).cuda()\n",
    "\n",
    "    comb_tensor = torch.cat( (z, sample, direct_sample) ).cuda()\n",
    "    comb_view = comb_tensor.view(3, 1, 28, 28)\n",
    "    comb_view = fix_tensor_for_image(comb_view)\n",
    "    Combined_View = torch.cat((Combined_View, comb_view))\n",
    "\n",
    "save_image(Combined_View, './examples/average_letter/first_mean_and_middle_master.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function to make only the Mega image of all fractional case letters and first and mean\n",
    "def generate_fractional_composite(fractional_value, file_name):\n",
    "    Combined_View = torch.empty(0, 1, 28, 28).cuda()\n",
    "    for offset in range(26):\n",
    "        letter_index = 10 +offset\n",
    "        lower_index = 36 + offset\n",
    "\n",
    "        #Map through the model to the latent space and the whole way through\n",
    "        latent_sample = Lower_means[lower_index] \n",
    "        adjusted_latent = torch.add(latent_sample , ((fractional_value)*lower_to_upper_dict[letter_index]) )\n",
    "        direct_sample = vae.decoder(adjusted_latent).cuda()\n",
    "\n",
    "        comb_view = direct_sample.view(1, 1, 28, 28)\n",
    "        comb_view = fix_tensor_for_image(comb_view)\n",
    "        Combined_View = torch.cat((Combined_View, comb_view))\n",
    "\n",
    "    save_image(Combined_View, './examples/average_letter/'+file_name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mega_image_fractional_composite(fractional_value, file_name):\n",
    "    Combined_View = torch.empty(0, 1, 28, 28).cuda()\n",
    "    for offset in range(26):\n",
    "        letter_index = 10 +offset\n",
    "        lower_index = 36 + offset\n",
    "        A_indexes = labels_to_indexes[letter_index]\n",
    "        test_image_A, test_target_A = test_dataset[A_indexes[0]]\n",
    "\n",
    "        #Shape input into what the network wants\n",
    "        z = torch.flatten(test_image_A)#.cuda(k\n",
    "        z = torch.tensor(z)\n",
    "        z = z.cuda()\n",
    "\n",
    "        #Map through the model to the latent space and the whole way through\n",
    "        latent_sample = Lower_means[lower_index] \n",
    "\n",
    "        sample = vae.decoder(latent_sample)\n",
    "        adjusted_latent = torch.add(latent_sample , ((fractional_value)*lower_to_upper_dict[letter_index]) )\n",
    "        direct_sample = vae.decoder(adjusted_latent).cuda()\n",
    "\n",
    "        comb_tensor = torch.cat( (z, sample, direct_sample) ).cuda()\n",
    "        comb_view = comb_tensor.view(3, 1, 28, 28)\n",
    "        comb_view = fix_tensor_for_image(comb_view)\n",
    "        Combined_View = torch.cat((Combined_View, comb_view))\n",
    "\n",
    "    save_image(Combined_View, './examples/average_letter/'+file_name+'_mega_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_fractional_composite(0,'lower_case')\n",
    "generate_fractional_composite(1/2,'middle')\n",
    "generate_fractional_composite(1/4,'one_quarter')\n",
    "generate_fractional_composite(3/4,'three_quarter')\n",
    "generate_fractional_composite(1,'back_to_upper_from_lower')\n",
    "generate_fractional_composite(-1,'double_lower')\n",
    "generate_fractional_composite(2,'double_upper_from_lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 1.1828147172927856\n",
      "B 2.0622119903564453\n",
      "D 2.577026844024658\n",
      "E 1.2735296487808228\n",
      "G 1.12784743309021\n",
      "H 1.6361483335494995\n",
      "J 2.5664310455322266\n",
      "L 2.7141246795654297\n",
      "N 1.011705994606018\n",
      "Q 1.3242154121398926\n",
      "R 2.105133533477783\n",
      "T 2.4109084606170654\n",
      "[1.1828147172927856, 2.0622119903564453, 0.26304057240486145, 2.577026844024658, 1.2735296487808228, 0.38430047035217285, 1.12784743309021, 1.6361483335494995, 0.5150911808013916, 2.5664310455322266, 0.31175297498703003, 2.7141246795654297, 0.2677946984767914, 1.011705994606018, 0.16333363950252533, 0.23474393784999847, 1.3242154121398926, 2.105133533477783, 0.15040652453899384, 2.4109084606170654, 0.4012865126132965, 0.08240438997745514, 0.3601233661174774, 0.20464889705181122, 0.3758935332298279, 0.7144733667373657]\n"
     ]
    }
   ],
   "source": [
    "#Which letters are actually different from upper to lower\n",
    "Norms = []\n",
    "Upper_Lower_distinct_indexes = []\n",
    "for offset in range(26):\n",
    "    letter_index = 10 +offset\n",
    "    captalization_vect = lower_to_upper_dict[letter_index]\n",
    "    vect_norm = torch.norm(captalization_vect).item()\n",
    "    letter = index_to_uppercase_dict[letter_index]\n",
    "    Norms.append(vect_norm)\n",
    "    if vect_norm>1:\n",
    "        print(letter, vect_norm)\n",
    "        Upper_Lower_distinct_indexes.append(10+offset)\n",
    "print(Norms)\n",
    "\n",
    "#J and L are distinct wiht this process but F is not, in the paper they remove J and L and keep F as its own class when compressin in the By_Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(Norms, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([norm for norm in Norms if norm >=1 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2368e+15, 4.6183e-01, 7.5212e-02, 1.8753e-01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Make average capitalization vector\n",
    "mean_distinct_lower_upper_vector =  torch.empty(1, 4).cuda()\n",
    "for index in Upper_Lower_distinct_indexes:\n",
    "    mean_distinct_lower_upper_vector = torch.add( mean_distinct_lower_upper_vector, lower_to_upper_dict[index])\n",
    "\n",
    "mean_distinct_upper_lower_vector = mean_distinct_lower_upper_vector/len(Upper_Lower_distinct_indexes)\n",
    "print(mean_distinct_upper_lower_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/2705388341.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "{0: tensor([-0.0486,  1.7586, -0.3592,  0.3226], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 1: tensor([-1.3171, -1.4150, -0.3829, -0.1228], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 2: tensor([ 0.9367,  0.3439,  0.6909, -0.5600], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 3: tensor([-0.5568,  0.3207,  1.0250,  1.5168], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 4: tensor([ 0.0646, -0.7411,  0.1578, -0.2515], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 5: tensor([ 0.8945,  0.4823, -0.7192,  1.3620], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 6: tensor([-1.5097,  0.7385,  0.9170, -0.5531], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 7: tensor([ 1.1069, -1.3633,  1.1155,  0.3314], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 8: tensor([ 0.3109, -0.2116,  0.1776,  1.0464], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 9: tensor([ 0.1287, -0.9480,  0.8000,  0.4248], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "#Generate dict of the mean number\n",
    "Number_means = {}\n",
    "\n",
    "for offset in range(10):\n",
    "    num_indexes = labels_to_indexes[offset]\n",
    "    num_mean = mean_latent_space_vector(num_indexes)\n",
    "    Number_means[offset] = num_mean\n",
    "\n",
    "    print(offset)\n",
    "print(Number_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_mega_image_fractional_composite_mean_vector(fractional_value, file_name, include_numbers):\n",
    "    Combined_View = torch.empty(0, 1, 28, 28).cuda()\n",
    "    num_characters = 26\n",
    "    if include_numbers:\n",
    "        num_characters = 36\n",
    "    for offset in range(num_characters):\n",
    "\n",
    "        letter_index = 10 +offset\n",
    "        lower_index  = 36 +offset \n",
    "        if include_numbers:\n",
    "            letter_index = offset\n",
    "            lower_index  = 26 + offset \n",
    "\n",
    "\n",
    "        A_indexes = labels_to_indexes[letter_index]\n",
    "        test_image_A, test_target_A = test_dataset[A_indexes[0]]\n",
    "\n",
    "        #Shape input into what the network wants\n",
    "        z = torch.flatten(test_image_A)#.cuda(k\n",
    "        z = torch.tensor(z)\n",
    "        z = z.cuda()\n",
    "\n",
    "        #Map through the model to the latent space and the whole way through\n",
    "        if letter_index< 10:\n",
    "            latent_sample = Number_means[letter_index]\n",
    "        else:\n",
    "            latent_sample = Lower_means[lower_index] \n",
    "\n",
    "        print(offset)\n",
    "        sample = vae.decoder(latent_sample)\n",
    "        adjusted_latent = torch.add(latent_sample , ((fractional_value)*mean_distinct_lower_upper_vector) )\n",
    "        direct_sample = vae.decoder(adjusted_latent).cuda()\n",
    "\n",
    "        #comb_tensor = torch.cat( (z, sample, direct_sample) ).cuda()\n",
    "        comb_tensor = torch.cat( (z, sample) ).cuda()\n",
    "        comb_view = comb_tensor.view(2, 1, 28, 28)\n",
    "        comb_view = fix_tensor_for_image(comb_view)\n",
    "        Combined_View = torch.cat((Combined_View, comb_view))\n",
    "\n",
    "    save_image(Combined_View, './examples/average_figure/'+file_name+'_mega_image_mean_vector.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/571993546.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "62",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[315]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgenerate_mega_image_fractional_composite_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlower_case\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m generate_mega_image_fractional_composite_mean_vector(\u001b[32m1\u001b[39m/\u001b[32m2\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mmiddle\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      3\u001b[39m generate_mega_image_fractional_composite_mean_vector(\u001b[32m1\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mback_to_upper_from_lower\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[314]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mgenerate_mega_image_fractional_composite_mean_vector\u001b[39m\u001b[34m(fractional_value, file_name, include_numbers)\u001b[39m\n\u001b[32m     24\u001b[39m     latent_sample = Number_means[letter_index]\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     latent_sample = \u001b[43mLower_means\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlower_index\u001b[49m\u001b[43m]\u001b[49m \n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(offset)\n\u001b[32m     29\u001b[39m sample = vae.decoder(latent_sample)\n",
      "\u001b[31mKeyError\u001b[39m: 62"
     ]
    }
   ],
   "source": [
    "generate_mega_image_fractional_composite_mean_vector(0,'lower_case',True)\n",
    "generate_mega_image_fractional_composite_mean_vector(1/2,'middle',True)\n",
    "generate_mega_image_fractional_composite_mean_vector(1,'back_to_upper_from_lower',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{36: tensor([ 0.1856,  0.7281, -0.2798, -1.0024], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 37: tensor([-1.4995,  0.2027,  0.7548, -0.9796], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 38: tensor([-0.2445,  1.5988,  0.3852, -1.1751], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 39: tensor([-1.5024,  0.1439, -0.4773,  0.9843], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 40: tensor([-0.3494,  1.0476, -0.1597, -1.2066], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 41: tensor([-0.2382, -0.7197, -0.6926, -0.2018], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 42: tensor([-0.0321, -0.1642,  0.6155,  0.5226], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 43: tensor([-1.1384, -0.1401,  0.6610, -0.7424], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 44: tensor([-1.0407, -1.3558,  0.0464, -0.4731], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 45: tensor([-1.2858, -0.2215,  0.4535,  0.9710], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 46: tensor([-0.2608,  0.0884,  0.2777, -0.5468], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 47: tensor([-1.2319, -1.4429, -0.2108, -0.3082], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 48: tensor([ 0.4279,  0.1032, -1.3926, -1.5738], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 49: tensor([ 0.0457, -0.1065, -0.4506, -1.2121], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 50: tensor([ 0.0781,  1.8343, -0.6366,  0.0873], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 51: tensor([ 0.2023, -0.6074, -1.1596, -0.1385], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 52: tensor([ 0.0452, -0.4622,  0.8456,  0.2369], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 53: tensor([ 0.7153, -0.7937, -1.7607, -0.6038], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 54: tensor([0.6461, 0.7161, 0.1702, 1.6475], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 55: tensor([-0.5079, -0.7462, -0.6271, -0.9641], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 56: tensor([ 0.6660,  0.5178, -0.2423, -0.6130], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 57: tensor([ 0.7507, -0.3700, -0.2104, -0.0319], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 58: tensor([ 1.1364,  0.3201, -1.0545, -0.8010], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 59: tensor([ 0.2775, -0.0839,  0.3080, -0.6839], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 60: tensor([ 0.4032, -0.8943,  0.1453, -0.0654], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 61: tensor([ 1.9559,  0.3475,  0.9998, -1.6964], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "print(Lower_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
