{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prerequisites\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "batch_size = 10**3\n",
    "Z_dim = 3\n",
    "# Extended MNIST Dataset\n",
    "train_dataset = datasets.EMNIST(root='./emnist_data/', split= 'byclass', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.EMNIST(root='./emnist_data/', split= 'byclass', train=False, transform=transforms.ToTensor(), download=False)\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, x_dim, h_dim1, h_dim2, z_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # encoder part\n",
    "        self.fc1 = nn.Linear(x_dim, h_dim1)\n",
    "        self.fc2 = nn.Linear(h_dim1, h_dim2)\n",
    "        self.fc31 = nn.Linear(h_dim2, z_dim)\n",
    "        self.fc32 = nn.Linear(h_dim2, z_dim)\n",
    "        # decoder part\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim2)\n",
    "        self.fc5 = nn.Linear(h_dim2, h_dim1)\n",
    "        self.fc6 = nn.Linear(h_dim1, x_dim)\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        return self.fc31(h), self.fc32(h) # mu, log_var\n",
    "    \n",
    "    def sampling(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps.mul(std).add_(mu) # return z sample\n",
    "        \n",
    "    def decoder(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        h = F.relu(self.fc5(h))\n",
    "        return F.sigmoid(self.fc6(h)) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encoder(x.view(-1, 784))\n",
    "        z = self.sampling(mu, log_var)\n",
    "        return self.decoder(z), mu, log_var\n",
    "\n",
    "# build model\n",
    "vae = VAE(x_dim=784, h_dim1= 512, h_dim2=256, z_dim=Z_dim)\n",
    "if torch.cuda.is_available():\n",
    "    vae.cuda()\n",
    "    model = vae.to('cuda')\n",
    "    print('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc31): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (fc32): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (fc4): Linear(in_features=3, out_features=256, bias=True)\n",
       "  (fc5): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (fc6): Linear(in_features=512, out_features=784, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(vae.parameters())\n",
    "# return reconstruction error + KL divergence losses\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    vae.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_batch, mu, log_var = vae(data)\n",
    "        loss = loss_function(recon_batch, data, mu, log_var)\n",
    "        \n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    vae.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for data, _ in test_loader:\n",
    "            data = data.cuda()\n",
    "            recon, mu, log_var = vae(data)\n",
    "            \n",
    "            # sum up batch loss\n",
    "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/697932 (0%)]\tLoss: 545.326500\n",
      "Train Epoch: 0 [100000/697932 (14%)]\tLoss: 243.339359\n",
      "Train Epoch: 0 [200000/697932 (29%)]\tLoss: 222.329859\n",
      "Train Epoch: 0 [300000/697932 (43%)]\tLoss: 216.709953\n",
      "Train Epoch: 0 [400000/697932 (57%)]\tLoss: 214.012734\n",
      "Train Epoch: 0 [500000/697932 (72%)]\tLoss: 206.931172\n",
      "Train Epoch: 0 [600000/697932 (86%)]\tLoss: 202.480047\n",
      "====> Epoch: 0 Average loss: 223.5106\n",
      "====> Test set loss: 204.5342\n",
      "Train Epoch: 1 [0/697932 (0%)]\tLoss: 203.609000\n",
      "Train Epoch: 1 [100000/697932 (14%)]\tLoss: 204.191641\n",
      "Train Epoch: 1 [200000/697932 (29%)]\tLoss: 200.528469\n",
      "Train Epoch: 1 [300000/697932 (43%)]\tLoss: 201.959656\n",
      "Train Epoch: 1 [400000/697932 (57%)]\tLoss: 196.306719\n",
      "Train Epoch: 1 [500000/697932 (72%)]\tLoss: 201.166438\n",
      "Train Epoch: 1 [600000/697932 (86%)]\tLoss: 202.148781\n",
      "====> Epoch: 1 Average loss: 200.6304\n",
      "====> Test set loss: 197.3325\n",
      "Train Epoch: 2 [0/697932 (0%)]\tLoss: 197.475719\n",
      "Train Epoch: 2 [100000/697932 (14%)]\tLoss: 195.622391\n",
      "Train Epoch: 2 [200000/697932 (29%)]\tLoss: 192.972859\n",
      "Train Epoch: 2 [300000/697932 (43%)]\tLoss: 195.725500\n",
      "Train Epoch: 2 [400000/697932 (57%)]\tLoss: 194.579875\n",
      "Train Epoch: 2 [500000/697932 (72%)]\tLoss: 195.942719\n",
      "Train Epoch: 2 [600000/697932 (86%)]\tLoss: 194.230687\n",
      "====> Epoch: 2 Average loss: 195.3121\n",
      "====> Test set loss: 193.5940\n",
      "Train Epoch: 3 [0/697932 (0%)]\tLoss: 194.102516\n",
      "Train Epoch: 3 [100000/697932 (14%)]\tLoss: 192.030078\n",
      "Train Epoch: 3 [200000/697932 (29%)]\tLoss: 193.389922\n",
      "Train Epoch: 3 [300000/697932 (43%)]\tLoss: 189.836437\n",
      "Train Epoch: 3 [400000/697932 (57%)]\tLoss: 191.980906\n",
      "Train Epoch: 3 [500000/697932 (72%)]\tLoss: 192.631563\n",
      "Train Epoch: 3 [600000/697932 (86%)]\tLoss: 188.419875\n",
      "====> Epoch: 3 Average loss: 192.1212\n",
      "====> Test set loss: 191.1293\n",
      "Train Epoch: 4 [0/697932 (0%)]\tLoss: 190.232359\n",
      "Train Epoch: 4 [100000/697932 (14%)]\tLoss: 189.794375\n",
      "Train Epoch: 4 [200000/697932 (29%)]\tLoss: 188.584078\n",
      "Train Epoch: 4 [300000/697932 (43%)]\tLoss: 187.744297\n",
      "Train Epoch: 4 [400000/697932 (57%)]\tLoss: 190.638937\n",
      "Train Epoch: 4 [500000/697932 (72%)]\tLoss: 191.074641\n",
      "Train Epoch: 4 [600000/697932 (86%)]\tLoss: 188.007828\n",
      "====> Epoch: 4 Average loss: 190.0010\n",
      "====> Test set loss: 189.2134\n",
      "Train Epoch: 5 [0/697932 (0%)]\tLoss: 189.767219\n",
      "Train Epoch: 5 [100000/697932 (14%)]\tLoss: 188.512219\n",
      "Train Epoch: 5 [200000/697932 (29%)]\tLoss: 187.569281\n",
      "Train Epoch: 5 [300000/697932 (43%)]\tLoss: 187.546422\n",
      "Train Epoch: 5 [400000/697932 (57%)]\tLoss: 190.761687\n",
      "Train Epoch: 5 [500000/697932 (72%)]\tLoss: 185.637828\n",
      "Train Epoch: 5 [600000/697932 (86%)]\tLoss: 188.380828\n",
      "====> Epoch: 5 Average loss: 188.3208\n",
      "====> Test set loss: 187.9061\n",
      "Train Epoch: 6 [0/697932 (0%)]\tLoss: 188.196188\n",
      "Train Epoch: 6 [100000/697932 (14%)]\tLoss: 187.495531\n",
      "Train Epoch: 6 [200000/697932 (29%)]\tLoss: 188.426375\n",
      "Train Epoch: 6 [300000/697932 (43%)]\tLoss: 185.338422\n",
      "Train Epoch: 6 [400000/697932 (57%)]\tLoss: 186.961312\n",
      "Train Epoch: 6 [500000/697932 (72%)]\tLoss: 187.837766\n",
      "Train Epoch: 6 [600000/697932 (86%)]\tLoss: 185.236234\n",
      "====> Epoch: 6 Average loss: 187.1187\n",
      "====> Test set loss: 186.6763\n",
      "Train Epoch: 7 [0/697932 (0%)]\tLoss: 186.686344\n",
      "Train Epoch: 7 [100000/697932 (14%)]\tLoss: 186.393313\n",
      "Train Epoch: 7 [200000/697932 (29%)]\tLoss: 189.132406\n",
      "Train Epoch: 7 [300000/697932 (43%)]\tLoss: 186.065047\n",
      "Train Epoch: 7 [400000/697932 (57%)]\tLoss: 187.897437\n",
      "Train Epoch: 7 [500000/697932 (72%)]\tLoss: 189.155188\n",
      "Train Epoch: 7 [600000/697932 (86%)]\tLoss: 183.307328\n",
      "====> Epoch: 7 Average loss: 186.1210\n",
      "====> Test set loss: 186.1932\n",
      "Train Epoch: 8 [0/697932 (0%)]\tLoss: 185.374750\n",
      "Train Epoch: 8 [100000/697932 (14%)]\tLoss: 183.963922\n",
      "Train Epoch: 8 [200000/697932 (29%)]\tLoss: 183.377953\n",
      "Train Epoch: 8 [300000/697932 (43%)]\tLoss: 184.839422\n",
      "Train Epoch: 8 [400000/697932 (57%)]\tLoss: 184.904641\n",
      "Train Epoch: 8 [500000/697932 (72%)]\tLoss: 182.091344\n",
      "Train Epoch: 8 [600000/697932 (86%)]\tLoss: 183.192375\n",
      "====> Epoch: 8 Average loss: 185.3581\n",
      "====> Test set loss: 185.3014\n",
      "Train Epoch: 9 [0/697932 (0%)]\tLoss: 186.148609\n",
      "Train Epoch: 9 [100000/697932 (14%)]\tLoss: 185.086766\n",
      "Train Epoch: 9 [200000/697932 (29%)]\tLoss: 185.244812\n",
      "Train Epoch: 9 [300000/697932 (43%)]\tLoss: 190.381844\n",
      "Train Epoch: 9 [400000/697932 (57%)]\tLoss: 182.129109\n",
      "Train Epoch: 9 [500000/697932 (72%)]\tLoss: 184.888609\n",
      "Train Epoch: 9 [600000/697932 (86%)]\tLoss: 185.060969\n",
      "====> Epoch: 9 Average loss: 184.6354\n",
      "====> Test set loss: 184.7356\n",
      "Train Epoch: 10 [0/697932 (0%)]\tLoss: 185.029547\n",
      "Train Epoch: 10 [100000/697932 (14%)]\tLoss: 188.010938\n",
      "Train Epoch: 10 [200000/697932 (29%)]\tLoss: 181.986031\n",
      "Train Epoch: 10 [300000/697932 (43%)]\tLoss: 184.547484\n",
      "Train Epoch: 10 [400000/697932 (57%)]\tLoss: 185.448109\n",
      "Train Epoch: 10 [500000/697932 (72%)]\tLoss: 182.793937\n",
      "Train Epoch: 10 [600000/697932 (86%)]\tLoss: 182.381953\n",
      "====> Epoch: 10 Average loss: 184.0729\n",
      "====> Test set loss: 184.5049\n",
      "Train Epoch: 11 [0/697932 (0%)]\tLoss: 184.745016\n",
      "Train Epoch: 11 [100000/697932 (14%)]\tLoss: 183.767953\n",
      "Train Epoch: 11 [200000/697932 (29%)]\tLoss: 182.151125\n",
      "Train Epoch: 11 [300000/697932 (43%)]\tLoss: 183.974156\n",
      "Train Epoch: 11 [400000/697932 (57%)]\tLoss: 181.173594\n",
      "Train Epoch: 11 [500000/697932 (72%)]\tLoss: 183.313187\n",
      "Train Epoch: 11 [600000/697932 (86%)]\tLoss: 181.785063\n",
      "====> Epoch: 11 Average loss: 183.5087\n",
      "====> Test set loss: 183.6309\n",
      "Train Epoch: 12 [0/697932 (0%)]\tLoss: 186.419906\n",
      "Train Epoch: 12 [100000/697932 (14%)]\tLoss: 181.529297\n",
      "Train Epoch: 12 [200000/697932 (29%)]\tLoss: 182.972563\n",
      "Train Epoch: 12 [300000/697932 (43%)]\tLoss: 183.452625\n",
      "Train Epoch: 12 [400000/697932 (57%)]\tLoss: 186.482812\n",
      "Train Epoch: 12 [500000/697932 (72%)]\tLoss: 180.970250\n",
      "Train Epoch: 12 [600000/697932 (86%)]\tLoss: 185.111234\n",
      "====> Epoch: 12 Average loss: 183.1097\n",
      "====> Test set loss: 183.2632\n",
      "Train Epoch: 13 [0/697932 (0%)]\tLoss: 180.223766\n",
      "Train Epoch: 13 [100000/697932 (14%)]\tLoss: 187.871047\n",
      "Train Epoch: 13 [200000/697932 (29%)]\tLoss: 183.935937\n",
      "Train Epoch: 13 [300000/697932 (43%)]\tLoss: 180.600891\n",
      "Train Epoch: 13 [400000/697932 (57%)]\tLoss: 181.486141\n",
      "Train Epoch: 13 [500000/697932 (72%)]\tLoss: 183.295250\n",
      "Train Epoch: 13 [600000/697932 (86%)]\tLoss: 182.009531\n",
      "====> Epoch: 13 Average loss: 182.6471\n",
      "====> Test set loss: 183.0642\n",
      "Train Epoch: 14 [0/697932 (0%)]\tLoss: 185.099703\n",
      "Train Epoch: 14 [100000/697932 (14%)]\tLoss: 180.235625\n",
      "Train Epoch: 14 [200000/697932 (29%)]\tLoss: 181.282594\n",
      "Train Epoch: 14 [300000/697932 (43%)]\tLoss: 182.543125\n",
      "Train Epoch: 14 [400000/697932 (57%)]\tLoss: 180.962219\n",
      "Train Epoch: 14 [500000/697932 (72%)]\tLoss: 181.519531\n",
      "Train Epoch: 14 [600000/697932 (86%)]\tLoss: 182.968938\n",
      "====> Epoch: 14 Average loss: 182.2576\n",
      "====> Test set loss: 182.7752\n",
      "Train Epoch: 15 [0/697932 (0%)]\tLoss: 183.257062\n",
      "Train Epoch: 15 [100000/697932 (14%)]\tLoss: 184.508250\n",
      "Train Epoch: 15 [200000/697932 (29%)]\tLoss: 181.301703\n",
      "Train Epoch: 15 [300000/697932 (43%)]\tLoss: 185.300062\n",
      "Train Epoch: 15 [400000/697932 (57%)]\tLoss: 180.926172\n",
      "Train Epoch: 15 [500000/697932 (72%)]\tLoss: 182.531031\n",
      "Train Epoch: 15 [600000/697932 (86%)]\tLoss: 180.436156\n",
      "====> Epoch: 15 Average loss: 181.8950\n",
      "====> Test set loss: 182.2238\n",
      "Train Epoch: 16 [0/697932 (0%)]\tLoss: 183.070938\n",
      "Train Epoch: 16 [100000/697932 (14%)]\tLoss: 184.836953\n",
      "Train Epoch: 16 [200000/697932 (29%)]\tLoss: 182.445219\n",
      "Train Epoch: 16 [300000/697932 (43%)]\tLoss: 181.713687\n",
      "Train Epoch: 16 [400000/697932 (57%)]\tLoss: 183.719391\n",
      "Train Epoch: 16 [500000/697932 (72%)]\tLoss: 182.651938\n",
      "Train Epoch: 16 [600000/697932 (86%)]\tLoss: 183.515656\n",
      "====> Epoch: 16 Average loss: 181.5704\n",
      "====> Test set loss: 182.0294\n",
      "Train Epoch: 17 [0/697932 (0%)]\tLoss: 180.922562\n",
      "Train Epoch: 17 [100000/697932 (14%)]\tLoss: 180.195578\n",
      "Train Epoch: 17 [200000/697932 (29%)]\tLoss: 180.622141\n",
      "Train Epoch: 17 [300000/697932 (43%)]\tLoss: 182.686688\n",
      "Train Epoch: 17 [400000/697932 (57%)]\tLoss: 181.272687\n",
      "Train Epoch: 17 [500000/697932 (72%)]\tLoss: 183.339203\n",
      "Train Epoch: 17 [600000/697932 (86%)]\tLoss: 180.650203\n",
      "====> Epoch: 17 Average loss: 181.1615\n",
      "====> Test set loss: 181.6409\n",
      "Train Epoch: 18 [0/697932 (0%)]\tLoss: 178.516047\n",
      "Train Epoch: 18 [100000/697932 (14%)]\tLoss: 180.121766\n",
      "Train Epoch: 18 [200000/697932 (29%)]\tLoss: 178.843453\n",
      "Train Epoch: 18 [300000/697932 (43%)]\tLoss: 179.716297\n",
      "Train Epoch: 18 [400000/697932 (57%)]\tLoss: 181.932969\n",
      "Train Epoch: 18 [500000/697932 (72%)]\tLoss: 179.145672\n",
      "Train Epoch: 18 [600000/697932 (86%)]\tLoss: 177.620156\n",
      "====> Epoch: 18 Average loss: 180.9707\n",
      "====> Test set loss: 181.1399\n",
      "Train Epoch: 19 [0/697932 (0%)]\tLoss: 179.985594\n",
      "Train Epoch: 19 [100000/697932 (14%)]\tLoss: 178.494578\n",
      "Train Epoch: 19 [200000/697932 (29%)]\tLoss: 179.206125\n",
      "Train Epoch: 19 [300000/697932 (43%)]\tLoss: 178.711000\n",
      "Train Epoch: 19 [400000/697932 (57%)]\tLoss: 180.118156\n",
      "Train Epoch: 19 [500000/697932 (72%)]\tLoss: 177.428375\n",
      "Train Epoch: 19 [600000/697932 (86%)]\tLoss: 181.245422\n",
      "====> Epoch: 19 Average loss: 180.6793\n",
      "====> Test set loss: 181.2144\n",
      "Train Epoch: 20 [0/697932 (0%)]\tLoss: 178.184875\n",
      "Train Epoch: 20 [100000/697932 (14%)]\tLoss: 180.450609\n",
      "Train Epoch: 20 [200000/697932 (29%)]\tLoss: 182.170344\n",
      "Train Epoch: 20 [300000/697932 (43%)]\tLoss: 180.501453\n",
      "Train Epoch: 20 [400000/697932 (57%)]\tLoss: 176.738500\n",
      "Train Epoch: 20 [500000/697932 (72%)]\tLoss: 178.762672\n",
      "Train Epoch: 20 [600000/697932 (86%)]\tLoss: 178.645453\n",
      "====> Epoch: 20 Average loss: 180.4741\n",
      "====> Test set loss: 180.9570\n",
      "Train Epoch: 21 [0/697932 (0%)]\tLoss: 180.156516\n",
      "Train Epoch: 21 [100000/697932 (14%)]\tLoss: 181.837187\n",
      "Train Epoch: 21 [200000/697932 (29%)]\tLoss: 180.841891\n",
      "Train Epoch: 21 [300000/697932 (43%)]\tLoss: 182.977953\n",
      "Train Epoch: 21 [400000/697932 (57%)]\tLoss: 184.171359\n",
      "Train Epoch: 21 [500000/697932 (72%)]\tLoss: 182.416594\n",
      "Train Epoch: 21 [600000/697932 (86%)]\tLoss: 180.773438\n",
      "====> Epoch: 21 Average loss: 180.4314\n",
      "====> Test set loss: 180.7933\n",
      "Train Epoch: 22 [0/697932 (0%)]\tLoss: 179.475500\n",
      "Train Epoch: 22 [100000/697932 (14%)]\tLoss: 179.760844\n",
      "Train Epoch: 22 [200000/697932 (29%)]\tLoss: 177.893203\n",
      "Train Epoch: 22 [300000/697932 (43%)]\tLoss: 179.394375\n",
      "Train Epoch: 22 [400000/697932 (57%)]\tLoss: 179.802812\n",
      "Train Epoch: 22 [500000/697932 (72%)]\tLoss: 181.639281\n",
      "Train Epoch: 22 [600000/697932 (86%)]\tLoss: 179.110922\n",
      "====> Epoch: 22 Average loss: 180.0710\n",
      "====> Test set loss: 180.4552\n",
      "Train Epoch: 23 [0/697932 (0%)]\tLoss: 180.321531\n",
      "Train Epoch: 23 [100000/697932 (14%)]\tLoss: 182.544078\n",
      "Train Epoch: 23 [200000/697932 (29%)]\tLoss: 179.526750\n",
      "Train Epoch: 23 [300000/697932 (43%)]\tLoss: 179.675344\n",
      "Train Epoch: 23 [400000/697932 (57%)]\tLoss: 175.993781\n",
      "Train Epoch: 23 [500000/697932 (72%)]\tLoss: 180.512812\n",
      "Train Epoch: 23 [600000/697932 (86%)]\tLoss: 179.795969\n",
      "====> Epoch: 23 Average loss: 179.8822\n",
      "====> Test set loss: 180.6718\n",
      "Train Epoch: 24 [0/697932 (0%)]\tLoss: 182.427578\n",
      "Train Epoch: 24 [100000/697932 (14%)]\tLoss: 175.773344\n",
      "Train Epoch: 24 [200000/697932 (29%)]\tLoss: 176.276484\n",
      "Train Epoch: 24 [300000/697932 (43%)]\tLoss: 177.317391\n",
      "Train Epoch: 24 [400000/697932 (57%)]\tLoss: 180.595156\n",
      "Train Epoch: 24 [500000/697932 (72%)]\tLoss: 180.570578\n",
      "Train Epoch: 24 [600000/697932 (86%)]\tLoss: 180.438359\n",
      "====> Epoch: 24 Average loss: 179.6866\n",
      "====> Test set loss: 180.1493\n",
      "Train Epoch: 25 [0/697932 (0%)]\tLoss: 179.598703\n",
      "Train Epoch: 25 [100000/697932 (14%)]\tLoss: 179.922094\n",
      "Train Epoch: 25 [200000/697932 (29%)]\tLoss: 177.739438\n",
      "Train Epoch: 25 [300000/697932 (43%)]\tLoss: 182.436922\n",
      "Train Epoch: 25 [400000/697932 (57%)]\tLoss: 180.955922\n",
      "Train Epoch: 25 [500000/697932 (72%)]\tLoss: 181.159328\n",
      "Train Epoch: 25 [600000/697932 (86%)]\tLoss: 178.177703\n",
      "====> Epoch: 25 Average loss: 179.5394\n",
      "====> Test set loss: 179.9704\n",
      "Train Epoch: 26 [0/697932 (0%)]\tLoss: 180.891719\n",
      "Train Epoch: 26 [100000/697932 (14%)]\tLoss: 180.037438\n",
      "Train Epoch: 26 [200000/697932 (29%)]\tLoss: 179.363469\n",
      "Train Epoch: 26 [300000/697932 (43%)]\tLoss: 180.506984\n",
      "Train Epoch: 26 [400000/697932 (57%)]\tLoss: 177.885437\n",
      "Train Epoch: 26 [500000/697932 (72%)]\tLoss: 179.640953\n",
      "Train Epoch: 26 [600000/697932 (86%)]\tLoss: 177.428219\n",
      "====> Epoch: 26 Average loss: 179.3703\n",
      "====> Test set loss: 179.9918\n",
      "Train Epoch: 27 [0/697932 (0%)]\tLoss: 180.920719\n",
      "Train Epoch: 27 [100000/697932 (14%)]\tLoss: 181.524234\n",
      "Train Epoch: 27 [200000/697932 (29%)]\tLoss: 179.376594\n",
      "Train Epoch: 27 [300000/697932 (43%)]\tLoss: 181.036234\n",
      "Train Epoch: 27 [400000/697932 (57%)]\tLoss: 180.914625\n",
      "Train Epoch: 27 [500000/697932 (72%)]\tLoss: 178.542266\n",
      "Train Epoch: 27 [600000/697932 (86%)]\tLoss: 178.453484\n",
      "====> Epoch: 27 Average loss: 179.2652\n",
      "====> Test set loss: 179.6958\n",
      "Train Epoch: 28 [0/697932 (0%)]\tLoss: 181.574156\n",
      "Train Epoch: 28 [100000/697932 (14%)]\tLoss: 179.918406\n",
      "Train Epoch: 28 [200000/697932 (29%)]\tLoss: 179.105234\n",
      "Train Epoch: 28 [300000/697932 (43%)]\tLoss: 176.851281\n",
      "Train Epoch: 28 [400000/697932 (57%)]\tLoss: 177.450859\n",
      "Train Epoch: 28 [500000/697932 (72%)]\tLoss: 179.286234\n",
      "Train Epoch: 28 [600000/697932 (86%)]\tLoss: 177.584656\n",
      "====> Epoch: 28 Average loss: 179.0566\n",
      "====> Test set loss: 179.6519\n",
      "Train Epoch: 29 [0/697932 (0%)]\tLoss: 178.654328\n",
      "Train Epoch: 29 [100000/697932 (14%)]\tLoss: 179.391281\n",
      "Train Epoch: 29 [200000/697932 (29%)]\tLoss: 180.440562\n",
      "Train Epoch: 29 [300000/697932 (43%)]\tLoss: 176.489750\n",
      "Train Epoch: 29 [400000/697932 (57%)]\tLoss: 178.397953\n",
      "Train Epoch: 29 [500000/697932 (72%)]\tLoss: 178.319109\n",
      "Train Epoch: 29 [600000/697932 (86%)]\tLoss: 181.728453\n",
      "====> Epoch: 29 Average loss: 178.8147\n",
      "====> Test set loss: 179.2999\n",
      "Train Epoch: 30 [0/697932 (0%)]\tLoss: 178.577047\n",
      "Train Epoch: 30 [100000/697932 (14%)]\tLoss: 176.874812\n",
      "Train Epoch: 30 [200000/697932 (29%)]\tLoss: 176.839109\n",
      "Train Epoch: 30 [300000/697932 (43%)]\tLoss: 177.879266\n",
      "Train Epoch: 30 [400000/697932 (57%)]\tLoss: 177.759125\n",
      "Train Epoch: 30 [500000/697932 (72%)]\tLoss: 182.250234\n",
      "Train Epoch: 30 [600000/697932 (86%)]\tLoss: 181.653938\n",
      "====> Epoch: 30 Average loss: 178.7675\n",
      "====> Test set loss: 179.1583\n",
      "Train Epoch: 31 [0/697932 (0%)]\tLoss: 177.558469\n",
      "Train Epoch: 31 [100000/697932 (14%)]\tLoss: 178.261719\n",
      "Train Epoch: 31 [200000/697932 (29%)]\tLoss: 178.651797\n",
      "Train Epoch: 31 [300000/697932 (43%)]\tLoss: 178.928937\n",
      "Train Epoch: 31 [400000/697932 (57%)]\tLoss: 179.669953\n",
      "Train Epoch: 31 [500000/697932 (72%)]\tLoss: 181.694641\n",
      "Train Epoch: 31 [600000/697932 (86%)]\tLoss: 181.839063\n",
      "====> Epoch: 31 Average loss: 178.6754\n",
      "====> Test set loss: 179.4454\n",
      "Train Epoch: 32 [0/697932 (0%)]\tLoss: 179.034469\n",
      "Train Epoch: 32 [100000/697932 (14%)]\tLoss: 177.278266\n",
      "Train Epoch: 32 [200000/697932 (29%)]\tLoss: 179.348203\n",
      "Train Epoch: 32 [300000/697932 (43%)]\tLoss: 179.498734\n",
      "Train Epoch: 32 [400000/697932 (57%)]\tLoss: 178.880078\n",
      "Train Epoch: 32 [500000/697932 (72%)]\tLoss: 173.441578\n",
      "Train Epoch: 32 [600000/697932 (86%)]\tLoss: 176.046844\n",
      "====> Epoch: 32 Average loss: 178.6456\n",
      "====> Test set loss: 179.3387\n",
      "Train Epoch: 33 [0/697932 (0%)]\tLoss: 175.987750\n",
      "Train Epoch: 33 [100000/697932 (14%)]\tLoss: 178.981188\n",
      "Train Epoch: 33 [200000/697932 (29%)]\tLoss: 179.478187\n",
      "Train Epoch: 33 [300000/697932 (43%)]\tLoss: 182.668578\n",
      "Train Epoch: 33 [400000/697932 (57%)]\tLoss: 178.784531\n",
      "Train Epoch: 33 [500000/697932 (72%)]\tLoss: 181.268563\n",
      "Train Epoch: 33 [600000/697932 (86%)]\tLoss: 178.650781\n",
      "====> Epoch: 33 Average loss: 178.3158\n",
      "====> Test set loss: 178.9530\n",
      "Train Epoch: 34 [0/697932 (0%)]\tLoss: 177.631672\n",
      "Train Epoch: 34 [100000/697932 (14%)]\tLoss: 178.369891\n",
      "Train Epoch: 34 [200000/697932 (29%)]\tLoss: 175.172047\n",
      "Train Epoch: 34 [300000/697932 (43%)]\tLoss: 179.201797\n",
      "Train Epoch: 34 [400000/697932 (57%)]\tLoss: 176.334172\n",
      "Train Epoch: 34 [500000/697932 (72%)]\tLoss: 183.615578\n",
      "Train Epoch: 34 [600000/697932 (86%)]\tLoss: 178.471125\n",
      "====> Epoch: 34 Average loss: 178.2640\n",
      "====> Test set loss: 179.0341\n",
      "Train Epoch: 35 [0/697932 (0%)]\tLoss: 180.195656\n",
      "Train Epoch: 35 [100000/697932 (14%)]\tLoss: 177.750922\n",
      "Train Epoch: 35 [200000/697932 (29%)]\tLoss: 177.590250\n",
      "Train Epoch: 35 [300000/697932 (43%)]\tLoss: 177.403328\n",
      "Train Epoch: 35 [400000/697932 (57%)]\tLoss: 177.889344\n",
      "Train Epoch: 35 [500000/697932 (72%)]\tLoss: 180.502438\n",
      "Train Epoch: 35 [600000/697932 (86%)]\tLoss: 178.072297\n",
      "====> Epoch: 35 Average loss: 178.2733\n",
      "====> Test set loss: 178.7460\n",
      "Train Epoch: 36 [0/697932 (0%)]\tLoss: 176.000531\n",
      "Train Epoch: 36 [100000/697932 (14%)]\tLoss: 182.238438\n",
      "Train Epoch: 36 [200000/697932 (29%)]\tLoss: 179.245266\n",
      "Train Epoch: 36 [300000/697932 (43%)]\tLoss: 179.119922\n",
      "Train Epoch: 36 [400000/697932 (57%)]\tLoss: 178.259906\n",
      "Train Epoch: 36 [500000/697932 (72%)]\tLoss: 180.119328\n",
      "Train Epoch: 36 [600000/697932 (86%)]\tLoss: 177.653641\n",
      "====> Epoch: 36 Average loss: 178.0012\n",
      "====> Test set loss: 178.6170\n",
      "Train Epoch: 37 [0/697932 (0%)]\tLoss: 175.955703\n",
      "Train Epoch: 37 [100000/697932 (14%)]\tLoss: 176.126906\n",
      "Train Epoch: 37 [200000/697932 (29%)]\tLoss: 179.619313\n",
      "Train Epoch: 37 [300000/697932 (43%)]\tLoss: 175.857094\n",
      "Train Epoch: 37 [400000/697932 (57%)]\tLoss: 178.024156\n",
      "Train Epoch: 37 [500000/697932 (72%)]\tLoss: 178.879828\n",
      "Train Epoch: 37 [600000/697932 (86%)]\tLoss: 177.988016\n",
      "====> Epoch: 37 Average loss: 177.9298\n",
      "====> Test set loss: 178.7230\n",
      "Train Epoch: 38 [0/697932 (0%)]\tLoss: 178.911875\n",
      "Train Epoch: 38 [100000/697932 (14%)]\tLoss: 179.212844\n",
      "Train Epoch: 38 [200000/697932 (29%)]\tLoss: 175.574828\n",
      "Train Epoch: 38 [300000/697932 (43%)]\tLoss: 179.102047\n",
      "Train Epoch: 38 [400000/697932 (57%)]\tLoss: 175.596563\n",
      "Train Epoch: 38 [500000/697932 (72%)]\tLoss: 178.104578\n",
      "Train Epoch: 38 [600000/697932 (86%)]\tLoss: 178.368453\n",
      "====> Epoch: 38 Average loss: 177.8187\n",
      "====> Test set loss: 178.6637\n",
      "Train Epoch: 39 [0/697932 (0%)]\tLoss: 178.660297\n",
      "Train Epoch: 39 [100000/697932 (14%)]\tLoss: 179.371594\n",
      "Train Epoch: 39 [200000/697932 (29%)]\tLoss: 178.403781\n",
      "Train Epoch: 39 [300000/697932 (43%)]\tLoss: 177.722938\n",
      "Train Epoch: 39 [400000/697932 (57%)]\tLoss: 177.298188\n",
      "Train Epoch: 39 [500000/697932 (72%)]\tLoss: 177.231719\n",
      "Train Epoch: 39 [600000/697932 (86%)]\tLoss: 176.448422\n",
      "====> Epoch: 39 Average loss: 177.7617\n",
      "====> Test set loss: 178.5250\n",
      "Train Epoch: 40 [0/697932 (0%)]\tLoss: 179.159500\n",
      "Train Epoch: 40 [100000/697932 (14%)]\tLoss: 176.568047\n",
      "Train Epoch: 40 [200000/697932 (29%)]\tLoss: 181.586719\n",
      "Train Epoch: 40 [300000/697932 (43%)]\tLoss: 178.172125\n",
      "Train Epoch: 40 [400000/697932 (57%)]\tLoss: 176.100437\n",
      "Train Epoch: 40 [500000/697932 (72%)]\tLoss: 177.699359\n",
      "Train Epoch: 40 [600000/697932 (86%)]\tLoss: 179.377031\n",
      "====> Epoch: 40 Average loss: 177.7075\n",
      "====> Test set loss: 178.9591\n",
      "Train Epoch: 41 [0/697932 (0%)]\tLoss: 177.667906\n",
      "Train Epoch: 41 [100000/697932 (14%)]\tLoss: 178.799578\n",
      "Train Epoch: 41 [200000/697932 (29%)]\tLoss: 179.017719\n",
      "Train Epoch: 41 [300000/697932 (43%)]\tLoss: 177.233234\n",
      "Train Epoch: 41 [400000/697932 (57%)]\tLoss: 175.770531\n",
      "Train Epoch: 41 [500000/697932 (72%)]\tLoss: 177.582109\n",
      "Train Epoch: 41 [600000/697932 (86%)]\tLoss: 176.700266\n",
      "====> Epoch: 41 Average loss: 177.7491\n",
      "====> Test set loss: 178.4228\n",
      "Train Epoch: 42 [0/697932 (0%)]\tLoss: 175.907609\n",
      "Train Epoch: 42 [100000/697932 (14%)]\tLoss: 176.740172\n",
      "Train Epoch: 42 [200000/697932 (29%)]\tLoss: 175.350734\n",
      "Train Epoch: 42 [300000/697932 (43%)]\tLoss: 176.089562\n",
      "Train Epoch: 42 [400000/697932 (57%)]\tLoss: 177.837625\n",
      "Train Epoch: 42 [500000/697932 (72%)]\tLoss: 176.706578\n",
      "Train Epoch: 42 [600000/697932 (86%)]\tLoss: 178.092469\n",
      "====> Epoch: 42 Average loss: 177.5524\n",
      "====> Test set loss: 178.3621\n",
      "Train Epoch: 43 [0/697932 (0%)]\tLoss: 176.352969\n",
      "Train Epoch: 43 [100000/697932 (14%)]\tLoss: 175.509828\n",
      "Train Epoch: 43 [200000/697932 (29%)]\tLoss: 179.227000\n",
      "Train Epoch: 43 [300000/697932 (43%)]\tLoss: 177.137969\n",
      "Train Epoch: 43 [400000/697932 (57%)]\tLoss: 179.670156\n",
      "Train Epoch: 43 [500000/697932 (72%)]\tLoss: 176.414750\n",
      "Train Epoch: 43 [600000/697932 (86%)]\tLoss: 177.551922\n",
      "====> Epoch: 43 Average loss: 177.5366\n",
      "====> Test set loss: 178.1740\n",
      "Train Epoch: 44 [0/697932 (0%)]\tLoss: 178.894563\n",
      "Train Epoch: 44 [100000/697932 (14%)]\tLoss: 176.607047\n",
      "Train Epoch: 44 [200000/697932 (29%)]\tLoss: 176.729500\n",
      "Train Epoch: 44 [300000/697932 (43%)]\tLoss: 175.706516\n",
      "Train Epoch: 44 [400000/697932 (57%)]\tLoss: 178.445906\n",
      "Train Epoch: 44 [500000/697932 (72%)]\tLoss: 180.525078\n",
      "Train Epoch: 44 [600000/697932 (86%)]\tLoss: 176.327344\n",
      "====> Epoch: 44 Average loss: 177.4259\n",
      "====> Test set loss: 177.9818\n",
      "Train Epoch: 45 [0/697932 (0%)]\tLoss: 174.977266\n",
      "Train Epoch: 45 [100000/697932 (14%)]\tLoss: 180.570531\n",
      "Train Epoch: 45 [200000/697932 (29%)]\tLoss: 179.315438\n",
      "Train Epoch: 45 [300000/697932 (43%)]\tLoss: 177.618000\n",
      "Train Epoch: 45 [400000/697932 (57%)]\tLoss: 177.703344\n",
      "Train Epoch: 45 [500000/697932 (72%)]\tLoss: 177.782406\n",
      "Train Epoch: 45 [600000/697932 (86%)]\tLoss: 179.782641\n",
      "====> Epoch: 45 Average loss: 177.2761\n",
      "====> Test set loss: 177.9214\n",
      "Train Epoch: 46 [0/697932 (0%)]\tLoss: 177.314109\n",
      "Train Epoch: 46 [100000/697932 (14%)]\tLoss: 175.875219\n",
      "Train Epoch: 46 [200000/697932 (29%)]\tLoss: 177.685750\n",
      "Train Epoch: 46 [300000/697932 (43%)]\tLoss: 176.271734\n",
      "Train Epoch: 46 [400000/697932 (57%)]\tLoss: 175.787031\n",
      "Train Epoch: 46 [500000/697932 (72%)]\tLoss: 181.178969\n",
      "Train Epoch: 46 [600000/697932 (86%)]\tLoss: 177.212344\n",
      "====> Epoch: 46 Average loss: 177.1251\n",
      "====> Test set loss: 178.0669\n",
      "Train Epoch: 47 [0/697932 (0%)]\tLoss: 176.261250\n",
      "Train Epoch: 47 [100000/697932 (14%)]\tLoss: 176.574719\n",
      "Train Epoch: 47 [200000/697932 (29%)]\tLoss: 177.102625\n",
      "Train Epoch: 47 [300000/697932 (43%)]\tLoss: 180.039266\n",
      "Train Epoch: 47 [400000/697932 (57%)]\tLoss: 177.522109\n",
      "Train Epoch: 47 [500000/697932 (72%)]\tLoss: 180.972281\n",
      "Train Epoch: 47 [600000/697932 (86%)]\tLoss: 177.558703\n",
      "====> Epoch: 47 Average loss: 177.1391\n",
      "====> Test set loss: 177.9806\n",
      "Train Epoch: 48 [0/697932 (0%)]\tLoss: 178.430344\n",
      "Train Epoch: 48 [100000/697932 (14%)]\tLoss: 174.177766\n",
      "Train Epoch: 48 [200000/697932 (29%)]\tLoss: 176.797125\n",
      "Train Epoch: 48 [300000/697932 (43%)]\tLoss: 175.480844\n",
      "Train Epoch: 48 [400000/697932 (57%)]\tLoss: 173.538781\n",
      "Train Epoch: 48 [500000/697932 (72%)]\tLoss: 176.715281\n",
      "Train Epoch: 48 [600000/697932 (86%)]\tLoss: 179.415687\n",
      "====> Epoch: 48 Average loss: 176.9876\n",
      "====> Test set loss: 177.8149\n",
      "Train Epoch: 49 [0/697932 (0%)]\tLoss: 175.387219\n",
      "Train Epoch: 49 [100000/697932 (14%)]\tLoss: 179.898547\n",
      "Train Epoch: 49 [200000/697932 (29%)]\tLoss: 177.200687\n",
      "Train Epoch: 49 [300000/697932 (43%)]\tLoss: 177.266859\n",
      "Train Epoch: 49 [400000/697932 (57%)]\tLoss: 180.565891\n",
      "Train Epoch: 49 [500000/697932 (72%)]\tLoss: 176.780828\n",
      "Train Epoch: 49 [600000/697932 (86%)]\tLoss: 177.759391\n",
      "====> Epoch: 49 Average loss: 176.9900\n",
      "====> Test set loss: 178.1636\n",
      "Train Epoch: 50 [0/697932 (0%)]\tLoss: 178.052828\n",
      "Train Epoch: 50 [100000/697932 (14%)]\tLoss: 176.112719\n",
      "Train Epoch: 50 [200000/697932 (29%)]\tLoss: 177.607234\n",
      "Train Epoch: 50 [300000/697932 (43%)]\tLoss: 175.089812\n",
      "Train Epoch: 50 [400000/697932 (57%)]\tLoss: 173.704672\n",
      "Train Epoch: 50 [500000/697932 (72%)]\tLoss: 176.251563\n",
      "Train Epoch: 50 [600000/697932 (86%)]\tLoss: 177.605766\n",
      "====> Epoch: 50 Average loss: 176.9965\n",
      "====> Test set loss: 178.0116\n",
      "Train Epoch: 51 [0/697932 (0%)]\tLoss: 173.448813\n",
      "Train Epoch: 51 [100000/697932 (14%)]\tLoss: 176.497813\n",
      "Train Epoch: 51 [200000/697932 (29%)]\tLoss: 173.508078\n",
      "Train Epoch: 51 [300000/697932 (43%)]\tLoss: 180.983406\n",
      "Train Epoch: 51 [400000/697932 (57%)]\tLoss: 177.179469\n",
      "Train Epoch: 51 [500000/697932 (72%)]\tLoss: 173.419891\n",
      "Train Epoch: 51 [600000/697932 (86%)]\tLoss: 176.829328\n",
      "====> Epoch: 51 Average loss: 176.8086\n",
      "====> Test set loss: 177.8926\n",
      "Train Epoch: 52 [0/697932 (0%)]\tLoss: 177.541156\n",
      "Train Epoch: 52 [100000/697932 (14%)]\tLoss: 175.750672\n",
      "Train Epoch: 52 [200000/697932 (29%)]\tLoss: 175.105344\n",
      "Train Epoch: 52 [300000/697932 (43%)]\tLoss: 174.871531\n",
      "Train Epoch: 52 [400000/697932 (57%)]\tLoss: 178.696641\n",
      "Train Epoch: 52 [500000/697932 (72%)]\tLoss: 175.679703\n",
      "Train Epoch: 52 [600000/697932 (86%)]\tLoss: 178.018766\n",
      "====> Epoch: 52 Average loss: 176.8213\n",
      "====> Test set loss: 177.7999\n",
      "Train Epoch: 53 [0/697932 (0%)]\tLoss: 176.688000\n",
      "Train Epoch: 53 [100000/697932 (14%)]\tLoss: 176.091891\n",
      "Train Epoch: 53 [200000/697932 (29%)]\tLoss: 174.205047\n",
      "Train Epoch: 53 [300000/697932 (43%)]\tLoss: 176.621406\n",
      "Train Epoch: 53 [400000/697932 (57%)]\tLoss: 173.137063\n",
      "Train Epoch: 53 [500000/697932 (72%)]\tLoss: 176.795938\n",
      "Train Epoch: 53 [600000/697932 (86%)]\tLoss: 176.702594\n",
      "====> Epoch: 53 Average loss: 176.6761\n",
      "====> Test set loss: 177.5010\n",
      "Train Epoch: 54 [0/697932 (0%)]\tLoss: 177.772797\n",
      "Train Epoch: 54 [100000/697932 (14%)]\tLoss: 175.201453\n",
      "Train Epoch: 54 [200000/697932 (29%)]\tLoss: 174.621859\n",
      "Train Epoch: 54 [300000/697932 (43%)]\tLoss: 174.596453\n",
      "Train Epoch: 54 [400000/697932 (57%)]\tLoss: 176.798062\n",
      "Train Epoch: 54 [500000/697932 (72%)]\tLoss: 177.259312\n",
      "Train Epoch: 54 [600000/697932 (86%)]\tLoss: 176.399625\n",
      "====> Epoch: 54 Average loss: 176.8698\n",
      "====> Test set loss: 177.7332\n",
      "Train Epoch: 55 [0/697932 (0%)]\tLoss: 179.487219\n",
      "Train Epoch: 55 [100000/697932 (14%)]\tLoss: 174.945516\n",
      "Train Epoch: 55 [200000/697932 (29%)]\tLoss: 177.245375\n",
      "Train Epoch: 55 [300000/697932 (43%)]\tLoss: 174.832250\n",
      "Train Epoch: 55 [400000/697932 (57%)]\tLoss: 178.874828\n",
      "Train Epoch: 55 [500000/697932 (72%)]\tLoss: 177.026453\n",
      "Train Epoch: 55 [600000/697932 (86%)]\tLoss: 178.817656\n",
      "====> Epoch: 55 Average loss: 176.7205\n",
      "====> Test set loss: 177.2480\n",
      "Train Epoch: 56 [0/697932 (0%)]\tLoss: 174.337953\n",
      "Train Epoch: 56 [100000/697932 (14%)]\tLoss: 178.356937\n",
      "Train Epoch: 56 [200000/697932 (29%)]\tLoss: 175.017953\n",
      "Train Epoch: 56 [300000/697932 (43%)]\tLoss: 176.410219\n",
      "Train Epoch: 56 [400000/697932 (57%)]\tLoss: 177.810875\n",
      "Train Epoch: 56 [500000/697932 (72%)]\tLoss: 177.738078\n",
      "Train Epoch: 56 [600000/697932 (86%)]\tLoss: 176.038375\n",
      "====> Epoch: 56 Average loss: 176.7977\n",
      "====> Test set loss: 177.7806\n",
      "Train Epoch: 57 [0/697932 (0%)]\tLoss: 177.259641\n",
      "Train Epoch: 57 [100000/697932 (14%)]\tLoss: 176.850328\n",
      "Train Epoch: 57 [200000/697932 (29%)]\tLoss: 176.277344\n",
      "Train Epoch: 57 [300000/697932 (43%)]\tLoss: 180.426891\n",
      "Train Epoch: 57 [400000/697932 (57%)]\tLoss: 180.265688\n",
      "Train Epoch: 57 [500000/697932 (72%)]\tLoss: 175.753203\n",
      "Train Epoch: 57 [600000/697932 (86%)]\tLoss: 177.059234\n",
      "====> Epoch: 57 Average loss: 176.6579\n",
      "====> Test set loss: 177.7669\n",
      "Train Epoch: 58 [0/697932 (0%)]\tLoss: 180.428766\n",
      "Train Epoch: 58 [100000/697932 (14%)]\tLoss: 175.717766\n",
      "Train Epoch: 58 [200000/697932 (29%)]\tLoss: 175.588812\n",
      "Train Epoch: 58 [300000/697932 (43%)]\tLoss: 176.593750\n",
      "Train Epoch: 58 [400000/697932 (57%)]\tLoss: 176.463875\n",
      "Train Epoch: 58 [500000/697932 (72%)]\tLoss: 173.265938\n",
      "Train Epoch: 58 [600000/697932 (86%)]\tLoss: 175.580625\n",
      "====> Epoch: 58 Average loss: 176.6038\n",
      "====> Test set loss: 177.0730\n",
      "Train Epoch: 59 [0/697932 (0%)]\tLoss: 172.055531\n",
      "Train Epoch: 59 [100000/697932 (14%)]\tLoss: 173.628484\n",
      "Train Epoch: 59 [200000/697932 (29%)]\tLoss: 175.092312\n",
      "Train Epoch: 59 [300000/697932 (43%)]\tLoss: 179.152688\n",
      "Train Epoch: 59 [400000/697932 (57%)]\tLoss: 175.062328\n",
      "Train Epoch: 59 [500000/697932 (72%)]\tLoss: 177.361266\n",
      "Train Epoch: 59 [600000/697932 (86%)]\tLoss: 174.605734\n",
      "====> Epoch: 59 Average loss: 176.3641\n",
      "====> Test set loss: 177.2447\n",
      "Train Epoch: 60 [0/697932 (0%)]\tLoss: 173.660500\n",
      "Train Epoch: 60 [100000/697932 (14%)]\tLoss: 172.124031\n",
      "Train Epoch: 60 [200000/697932 (29%)]\tLoss: 176.971812\n",
      "Train Epoch: 60 [300000/697932 (43%)]\tLoss: 177.378422\n",
      "Train Epoch: 60 [400000/697932 (57%)]\tLoss: 176.597578\n",
      "Train Epoch: 60 [500000/697932 (72%)]\tLoss: 175.356812\n",
      "Train Epoch: 60 [600000/697932 (86%)]\tLoss: 180.056375\n",
      "====> Epoch: 60 Average loss: 176.4094\n",
      "====> Test set loss: 177.5074\n",
      "Train Epoch: 61 [0/697932 (0%)]\tLoss: 178.808188\n",
      "Train Epoch: 61 [100000/697932 (14%)]\tLoss: 174.899313\n",
      "Train Epoch: 61 [200000/697932 (29%)]\tLoss: 176.194344\n",
      "Train Epoch: 61 [300000/697932 (43%)]\tLoss: 177.225672\n",
      "Train Epoch: 61 [400000/697932 (57%)]\tLoss: 173.163141\n",
      "Train Epoch: 61 [500000/697932 (72%)]\tLoss: 179.464234\n",
      "Train Epoch: 61 [600000/697932 (86%)]\tLoss: 177.207188\n",
      "====> Epoch: 61 Average loss: 176.3522\n",
      "====> Test set loss: 177.0252\n",
      "Train Epoch: 62 [0/697932 (0%)]\tLoss: 176.622547\n",
      "Train Epoch: 62 [100000/697932 (14%)]\tLoss: 178.832687\n",
      "Train Epoch: 62 [200000/697932 (29%)]\tLoss: 177.140703\n",
      "Train Epoch: 62 [300000/697932 (43%)]\tLoss: 178.592891\n",
      "Train Epoch: 62 [400000/697932 (57%)]\tLoss: 176.983844\n",
      "Train Epoch: 62 [500000/697932 (72%)]\tLoss: 176.802672\n",
      "Train Epoch: 62 [600000/697932 (86%)]\tLoss: 175.525422\n",
      "====> Epoch: 62 Average loss: 176.1759\n",
      "====> Test set loss: 177.1292\n",
      "Train Epoch: 63 [0/697932 (0%)]\tLoss: 175.882250\n",
      "Train Epoch: 63 [100000/697932 (14%)]\tLoss: 173.658594\n",
      "Train Epoch: 63 [200000/697932 (29%)]\tLoss: 176.901016\n",
      "Train Epoch: 63 [300000/697932 (43%)]\tLoss: 176.492328\n",
      "Train Epoch: 63 [400000/697932 (57%)]\tLoss: 180.504188\n",
      "Train Epoch: 63 [500000/697932 (72%)]\tLoss: 175.917109\n",
      "Train Epoch: 63 [600000/697932 (86%)]\tLoss: 179.962734\n",
      "====> Epoch: 63 Average loss: 176.1306\n",
      "====> Test set loss: 176.9204\n",
      "Train Epoch: 64 [0/697932 (0%)]\tLoss: 173.300234\n",
      "Train Epoch: 64 [100000/697932 (14%)]\tLoss: 174.453313\n",
      "Train Epoch: 64 [200000/697932 (29%)]\tLoss: 176.945375\n",
      "Train Epoch: 64 [300000/697932 (43%)]\tLoss: 176.313312\n",
      "Train Epoch: 64 [400000/697932 (57%)]\tLoss: 176.157391\n",
      "Train Epoch: 64 [500000/697932 (72%)]\tLoss: 173.551172\n",
      "Train Epoch: 64 [600000/697932 (86%)]\tLoss: 179.224594\n",
      "====> Epoch: 64 Average loss: 176.0071\n",
      "====> Test set loss: 177.0801\n",
      "Train Epoch: 65 [0/697932 (0%)]\tLoss: 178.008000\n",
      "Train Epoch: 65 [100000/697932 (14%)]\tLoss: 176.220234\n",
      "Train Epoch: 65 [200000/697932 (29%)]\tLoss: 176.414000\n",
      "Train Epoch: 65 [300000/697932 (43%)]\tLoss: 175.238172\n",
      "Train Epoch: 65 [400000/697932 (57%)]\tLoss: 176.960547\n",
      "Train Epoch: 65 [500000/697932 (72%)]\tLoss: 180.396187\n",
      "Train Epoch: 65 [600000/697932 (86%)]\tLoss: 178.956859\n",
      "====> Epoch: 65 Average loss: 176.0480\n",
      "====> Test set loss: 177.2551\n",
      "Train Epoch: 66 [0/697932 (0%)]\tLoss: 176.361063\n",
      "Train Epoch: 66 [100000/697932 (14%)]\tLoss: 174.424344\n",
      "Train Epoch: 66 [200000/697932 (29%)]\tLoss: 173.874156\n",
      "Train Epoch: 66 [300000/697932 (43%)]\tLoss: 171.869172\n",
      "Train Epoch: 66 [400000/697932 (57%)]\tLoss: 172.173094\n",
      "Train Epoch: 66 [500000/697932 (72%)]\tLoss: 179.419969\n",
      "Train Epoch: 66 [600000/697932 (86%)]\tLoss: 174.530703\n",
      "====> Epoch: 66 Average loss: 175.9538\n",
      "====> Test set loss: 176.7123\n",
      "Train Epoch: 67 [0/697932 (0%)]\tLoss: 176.630969\n",
      "Train Epoch: 67 [100000/697932 (14%)]\tLoss: 173.474734\n",
      "Train Epoch: 67 [200000/697932 (29%)]\tLoss: 175.154125\n",
      "Train Epoch: 67 [300000/697932 (43%)]\tLoss: 177.830062\n",
      "Train Epoch: 67 [400000/697932 (57%)]\tLoss: 174.534766\n",
      "Train Epoch: 67 [500000/697932 (72%)]\tLoss: 177.035547\n",
      "Train Epoch: 67 [600000/697932 (86%)]\tLoss: 176.849406\n",
      "====> Epoch: 67 Average loss: 175.8589\n",
      "====> Test set loss: 176.6526\n",
      "Train Epoch: 68 [0/697932 (0%)]\tLoss: 172.616797\n",
      "Train Epoch: 68 [100000/697932 (14%)]\tLoss: 177.697188\n",
      "Train Epoch: 68 [200000/697932 (29%)]\tLoss: 175.076188\n",
      "Train Epoch: 68 [300000/697932 (43%)]\tLoss: 178.798438\n",
      "Train Epoch: 68 [400000/697932 (57%)]\tLoss: 173.745641\n",
      "Train Epoch: 68 [500000/697932 (72%)]\tLoss: 177.137359\n",
      "Train Epoch: 68 [600000/697932 (86%)]\tLoss: 179.818625\n",
      "====> Epoch: 68 Average loss: 175.8269\n",
      "====> Test set loss: 177.0469\n",
      "Train Epoch: 69 [0/697932 (0%)]\tLoss: 174.864031\n",
      "Train Epoch: 69 [100000/697932 (14%)]\tLoss: 176.475719\n",
      "Train Epoch: 69 [200000/697932 (29%)]\tLoss: 178.167953\n",
      "Train Epoch: 69 [300000/697932 (43%)]\tLoss: 178.334125\n",
      "Train Epoch: 69 [400000/697932 (57%)]\tLoss: 175.721531\n",
      "Train Epoch: 69 [500000/697932 (72%)]\tLoss: 177.424594\n",
      "Train Epoch: 69 [600000/697932 (86%)]\tLoss: 173.480422\n",
      "====> Epoch: 69 Average loss: 175.8566\n",
      "====> Test set loss: 176.8999\n",
      "Train Epoch: 70 [0/697932 (0%)]\tLoss: 177.898562\n",
      "Train Epoch: 70 [100000/697932 (14%)]\tLoss: 173.937906\n",
      "Train Epoch: 70 [200000/697932 (29%)]\tLoss: 175.077250\n",
      "Train Epoch: 70 [300000/697932 (43%)]\tLoss: 178.684188\n",
      "Train Epoch: 70 [400000/697932 (57%)]\tLoss: 176.486719\n",
      "Train Epoch: 70 [500000/697932 (72%)]\tLoss: 173.706719\n",
      "Train Epoch: 70 [600000/697932 (86%)]\tLoss: 176.751594\n",
      "====> Epoch: 70 Average loss: 175.9156\n",
      "====> Test set loss: 176.8874\n",
      "Train Epoch: 71 [0/697932 (0%)]\tLoss: 177.277750\n",
      "Train Epoch: 71 [100000/697932 (14%)]\tLoss: 175.814734\n",
      "Train Epoch: 71 [200000/697932 (29%)]\tLoss: 175.080406\n",
      "Train Epoch: 71 [300000/697932 (43%)]\tLoss: 174.645203\n",
      "Train Epoch: 71 [400000/697932 (57%)]\tLoss: 175.783031\n",
      "Train Epoch: 71 [500000/697932 (72%)]\tLoss: 173.796453\n",
      "Train Epoch: 71 [600000/697932 (86%)]\tLoss: 177.075000\n",
      "====> Epoch: 71 Average loss: 175.7388\n",
      "====> Test set loss: 176.4886\n",
      "Train Epoch: 72 [0/697932 (0%)]\tLoss: 176.240594\n",
      "Train Epoch: 72 [100000/697932 (14%)]\tLoss: 175.493219\n",
      "Train Epoch: 72 [200000/697932 (29%)]\tLoss: 173.378984\n",
      "Train Epoch: 72 [300000/697932 (43%)]\tLoss: 178.344453\n",
      "Train Epoch: 72 [400000/697932 (57%)]\tLoss: 175.561875\n",
      "Train Epoch: 72 [500000/697932 (72%)]\tLoss: 176.887359\n",
      "Train Epoch: 72 [600000/697932 (86%)]\tLoss: 175.807344\n",
      "====> Epoch: 72 Average loss: 175.8505\n",
      "====> Test set loss: 177.2670\n",
      "Train Epoch: 73 [0/697932 (0%)]\tLoss: 174.433375\n",
      "Train Epoch: 73 [100000/697932 (14%)]\tLoss: 176.089094\n",
      "Train Epoch: 73 [200000/697932 (29%)]\tLoss: 177.030812\n",
      "Train Epoch: 73 [300000/697932 (43%)]\tLoss: 173.849672\n",
      "Train Epoch: 73 [400000/697932 (57%)]\tLoss: 178.027406\n",
      "Train Epoch: 73 [500000/697932 (72%)]\tLoss: 175.378938\n",
      "Train Epoch: 73 [600000/697932 (86%)]\tLoss: 175.048266\n",
      "====> Epoch: 73 Average loss: 175.7768\n",
      "====> Test set loss: 177.0452\n",
      "Train Epoch: 74 [0/697932 (0%)]\tLoss: 175.762625\n",
      "Train Epoch: 74 [100000/697932 (14%)]\tLoss: 174.805609\n",
      "Train Epoch: 74 [200000/697932 (29%)]\tLoss: 175.858937\n",
      "Train Epoch: 74 [300000/697932 (43%)]\tLoss: 176.711281\n",
      "Train Epoch: 74 [400000/697932 (57%)]\tLoss: 175.016906\n",
      "Train Epoch: 74 [500000/697932 (72%)]\tLoss: 174.311578\n",
      "Train Epoch: 74 [600000/697932 (86%)]\tLoss: 176.758187\n",
      "====> Epoch: 74 Average loss: 175.6981\n",
      "====> Test set loss: 176.6012\n",
      "Train Epoch: 75 [0/697932 (0%)]\tLoss: 175.853766\n",
      "Train Epoch: 75 [100000/697932 (14%)]\tLoss: 174.011437\n",
      "Train Epoch: 75 [200000/697932 (29%)]\tLoss: 174.030844\n",
      "Train Epoch: 75 [300000/697932 (43%)]\tLoss: 174.635562\n",
      "Train Epoch: 75 [400000/697932 (57%)]\tLoss: 174.186531\n",
      "Train Epoch: 75 [500000/697932 (72%)]\tLoss: 173.892547\n",
      "Train Epoch: 75 [600000/697932 (86%)]\tLoss: 174.953328\n",
      "====> Epoch: 75 Average loss: 175.7483\n",
      "====> Test set loss: 176.8355\n",
      "Train Epoch: 76 [0/697932 (0%)]\tLoss: 177.807797\n",
      "Train Epoch: 76 [100000/697932 (14%)]\tLoss: 177.295422\n",
      "Train Epoch: 76 [200000/697932 (29%)]\tLoss: 177.667469\n",
      "Train Epoch: 76 [300000/697932 (43%)]\tLoss: 173.853141\n",
      "Train Epoch: 76 [400000/697932 (57%)]\tLoss: 176.218266\n",
      "Train Epoch: 76 [500000/697932 (72%)]\tLoss: 176.443000\n",
      "Train Epoch: 76 [600000/697932 (86%)]\tLoss: 178.957875\n",
      "====> Epoch: 76 Average loss: 175.5707\n",
      "====> Test set loss: 176.6008\n",
      "Train Epoch: 77 [0/697932 (0%)]\tLoss: 174.880250\n",
      "Train Epoch: 77 [100000/697932 (14%)]\tLoss: 173.018875\n",
      "Train Epoch: 77 [200000/697932 (29%)]\tLoss: 175.642125\n",
      "Train Epoch: 77 [300000/697932 (43%)]\tLoss: 174.253125\n",
      "Train Epoch: 77 [400000/697932 (57%)]\tLoss: 175.558063\n",
      "Train Epoch: 77 [500000/697932 (72%)]\tLoss: 176.756016\n",
      "Train Epoch: 77 [600000/697932 (86%)]\tLoss: 175.332375\n",
      "====> Epoch: 77 Average loss: 175.7604\n",
      "====> Test set loss: 177.0376\n",
      "Train Epoch: 78 [0/697932 (0%)]\tLoss: 175.851156\n",
      "Train Epoch: 78 [100000/697932 (14%)]\tLoss: 178.507734\n",
      "Train Epoch: 78 [200000/697932 (29%)]\tLoss: 176.021391\n",
      "Train Epoch: 78 [300000/697932 (43%)]\tLoss: 175.584563\n",
      "Train Epoch: 78 [400000/697932 (57%)]\tLoss: 175.360078\n",
      "Train Epoch: 78 [500000/697932 (72%)]\tLoss: 177.492734\n",
      "Train Epoch: 78 [600000/697932 (86%)]\tLoss: 180.525547\n",
      "====> Epoch: 78 Average loss: 175.6742\n",
      "====> Test set loss: 176.6714\n",
      "Train Epoch: 79 [0/697932 (0%)]\tLoss: 177.437984\n",
      "Train Epoch: 79 [100000/697932 (14%)]\tLoss: 176.038000\n",
      "Train Epoch: 79 [200000/697932 (29%)]\tLoss: 175.224187\n",
      "Train Epoch: 79 [300000/697932 (43%)]\tLoss: 176.576078\n",
      "Train Epoch: 79 [400000/697932 (57%)]\tLoss: 177.454719\n",
      "Train Epoch: 79 [500000/697932 (72%)]\tLoss: 177.046922\n",
      "Train Epoch: 79 [600000/697932 (86%)]\tLoss: 176.210141\n",
      "====> Epoch: 79 Average loss: 175.5386\n",
      "====> Test set loss: 176.5811\n",
      "Train Epoch: 80 [0/697932 (0%)]\tLoss: 176.278562\n",
      "Train Epoch: 80 [100000/697932 (14%)]\tLoss: 176.606781\n",
      "Train Epoch: 80 [200000/697932 (29%)]\tLoss: 173.418594\n",
      "Train Epoch: 80 [300000/697932 (43%)]\tLoss: 177.315531\n",
      "Train Epoch: 80 [400000/697932 (57%)]\tLoss: 176.255750\n",
      "Train Epoch: 80 [500000/697932 (72%)]\tLoss: 176.902813\n",
      "Train Epoch: 80 [600000/697932 (86%)]\tLoss: 172.898547\n",
      "====> Epoch: 80 Average loss: 175.4372\n",
      "====> Test set loss: 176.5115\n",
      "Train Epoch: 81 [0/697932 (0%)]\tLoss: 175.911859\n",
      "Train Epoch: 81 [100000/697932 (14%)]\tLoss: 173.768500\n",
      "Train Epoch: 81 [200000/697932 (29%)]\tLoss: 173.979391\n",
      "Train Epoch: 81 [300000/697932 (43%)]\tLoss: 175.214516\n",
      "Train Epoch: 81 [400000/697932 (57%)]\tLoss: 176.052391\n",
      "Train Epoch: 81 [500000/697932 (72%)]\tLoss: 175.953203\n",
      "Train Epoch: 81 [600000/697932 (86%)]\tLoss: 175.036250\n",
      "====> Epoch: 81 Average loss: 175.5985\n",
      "====> Test set loss: 176.4989\n",
      "Train Epoch: 82 [0/697932 (0%)]\tLoss: 175.180594\n",
      "Train Epoch: 82 [100000/697932 (14%)]\tLoss: 177.728922\n",
      "Train Epoch: 82 [200000/697932 (29%)]\tLoss: 174.709609\n",
      "Train Epoch: 82 [300000/697932 (43%)]\tLoss: 172.186469\n",
      "Train Epoch: 82 [400000/697932 (57%)]\tLoss: 175.778063\n",
      "Train Epoch: 82 [500000/697932 (72%)]\tLoss: 176.332875\n",
      "Train Epoch: 82 [600000/697932 (86%)]\tLoss: 173.693641\n",
      "====> Epoch: 82 Average loss: 175.5920\n",
      "====> Test set loss: 176.7639\n",
      "Train Epoch: 83 [0/697932 (0%)]\tLoss: 175.952609\n",
      "Train Epoch: 83 [100000/697932 (14%)]\tLoss: 177.604344\n",
      "Train Epoch: 83 [200000/697932 (29%)]\tLoss: 173.802391\n",
      "Train Epoch: 83 [300000/697932 (43%)]\tLoss: 173.664453\n",
      "Train Epoch: 83 [400000/697932 (57%)]\tLoss: 175.101422\n",
      "Train Epoch: 83 [500000/697932 (72%)]\tLoss: 174.416422\n",
      "Train Epoch: 83 [600000/697932 (86%)]\tLoss: 178.631797\n",
      "====> Epoch: 83 Average loss: 175.3795\n",
      "====> Test set loss: 176.5100\n",
      "Train Epoch: 84 [0/697932 (0%)]\tLoss: 174.203703\n",
      "Train Epoch: 84 [100000/697932 (14%)]\tLoss: 175.503766\n",
      "Train Epoch: 84 [200000/697932 (29%)]\tLoss: 175.805844\n",
      "Train Epoch: 84 [300000/697932 (43%)]\tLoss: 175.320609\n",
      "Train Epoch: 84 [400000/697932 (57%)]\tLoss: 176.652250\n",
      "Train Epoch: 84 [500000/697932 (72%)]\tLoss: 178.519313\n",
      "Train Epoch: 84 [600000/697932 (86%)]\tLoss: 176.287422\n",
      "====> Epoch: 84 Average loss: 175.3313\n",
      "====> Test set loss: 176.4536\n",
      "Train Epoch: 85 [0/697932 (0%)]\tLoss: 177.089844\n",
      "Train Epoch: 85 [100000/697932 (14%)]\tLoss: 175.029375\n",
      "Train Epoch: 85 [200000/697932 (29%)]\tLoss: 172.174313\n",
      "Train Epoch: 85 [300000/697932 (43%)]\tLoss: 172.029922\n",
      "Train Epoch: 85 [400000/697932 (57%)]\tLoss: 171.877375\n",
      "Train Epoch: 85 [500000/697932 (72%)]\tLoss: 175.308016\n",
      "Train Epoch: 85 [600000/697932 (86%)]\tLoss: 175.508547\n",
      "====> Epoch: 85 Average loss: 175.2091\n",
      "====> Test set loss: 176.3090\n",
      "Train Epoch: 86 [0/697932 (0%)]\tLoss: 175.659625\n",
      "Train Epoch: 86 [100000/697932 (14%)]\tLoss: 172.542516\n",
      "Train Epoch: 86 [200000/697932 (29%)]\tLoss: 179.496797\n",
      "Train Epoch: 86 [300000/697932 (43%)]\tLoss: 175.103984\n",
      "Train Epoch: 86 [400000/697932 (57%)]\tLoss: 178.390266\n",
      "Train Epoch: 86 [500000/697932 (72%)]\tLoss: 178.383813\n",
      "Train Epoch: 86 [600000/697932 (86%)]\tLoss: 173.909500\n",
      "====> Epoch: 86 Average loss: 175.3120\n",
      "====> Test set loss: 176.5637\n",
      "Train Epoch: 87 [0/697932 (0%)]\tLoss: 173.982078\n",
      "Train Epoch: 87 [100000/697932 (14%)]\tLoss: 176.205844\n",
      "Train Epoch: 87 [200000/697932 (29%)]\tLoss: 174.940656\n",
      "Train Epoch: 87 [300000/697932 (43%)]\tLoss: 173.417484\n",
      "Train Epoch: 87 [400000/697932 (57%)]\tLoss: 179.610719\n",
      "Train Epoch: 87 [500000/697932 (72%)]\tLoss: 172.927500\n",
      "Train Epoch: 87 [600000/697932 (86%)]\tLoss: 178.897047\n",
      "====> Epoch: 87 Average loss: 175.5168\n",
      "====> Test set loss: 176.8299\n",
      "Train Epoch: 88 [0/697932 (0%)]\tLoss: 173.905891\n",
      "Train Epoch: 88 [100000/697932 (14%)]\tLoss: 174.712531\n",
      "Train Epoch: 88 [200000/697932 (29%)]\tLoss: 177.677141\n",
      "Train Epoch: 88 [300000/697932 (43%)]\tLoss: 171.654812\n",
      "Train Epoch: 88 [400000/697932 (57%)]\tLoss: 176.530031\n",
      "Train Epoch: 88 [500000/697932 (72%)]\tLoss: 175.344875\n",
      "Train Epoch: 88 [600000/697932 (86%)]\tLoss: 174.837828\n",
      "====> Epoch: 88 Average loss: 175.2982\n",
      "====> Test set loss: 176.2400\n",
      "Train Epoch: 89 [0/697932 (0%)]\tLoss: 176.315547\n",
      "Train Epoch: 89 [100000/697932 (14%)]\tLoss: 175.125078\n",
      "Train Epoch: 89 [200000/697932 (29%)]\tLoss: 174.536219\n",
      "Train Epoch: 89 [300000/697932 (43%)]\tLoss: 171.405594\n",
      "Train Epoch: 89 [400000/697932 (57%)]\tLoss: 175.194406\n",
      "Train Epoch: 89 [500000/697932 (72%)]\tLoss: 177.908844\n",
      "Train Epoch: 89 [600000/697932 (86%)]\tLoss: 178.297906\n",
      "====> Epoch: 89 Average loss: 175.2388\n",
      "====> Test set loss: 176.1582\n",
      "Train Epoch: 90 [0/697932 (0%)]\tLoss: 174.028109\n",
      "Train Epoch: 90 [100000/697932 (14%)]\tLoss: 175.592875\n",
      "Train Epoch: 90 [200000/697932 (29%)]\tLoss: 176.133750\n",
      "Train Epoch: 90 [300000/697932 (43%)]\tLoss: 177.121375\n",
      "Train Epoch: 90 [400000/697932 (57%)]\tLoss: 175.234281\n",
      "Train Epoch: 90 [500000/697932 (72%)]\tLoss: 176.356125\n",
      "Train Epoch: 90 [600000/697932 (86%)]\tLoss: 174.137344\n",
      "====> Epoch: 90 Average loss: 175.0168\n",
      "====> Test set loss: 175.9706\n",
      "Train Epoch: 91 [0/697932 (0%)]\tLoss: 172.344125\n",
      "Train Epoch: 91 [100000/697932 (14%)]\tLoss: 174.222516\n",
      "Train Epoch: 91 [200000/697932 (29%)]\tLoss: 175.817703\n",
      "Train Epoch: 91 [300000/697932 (43%)]\tLoss: 176.663234\n",
      "Train Epoch: 91 [400000/697932 (57%)]\tLoss: 176.118109\n",
      "Train Epoch: 91 [500000/697932 (72%)]\tLoss: 178.103109\n",
      "Train Epoch: 91 [600000/697932 (86%)]\tLoss: 176.592281\n",
      "====> Epoch: 91 Average loss: 175.1251\n",
      "====> Test set loss: 176.3806\n",
      "Train Epoch: 92 [0/697932 (0%)]\tLoss: 176.782422\n",
      "Train Epoch: 92 [100000/697932 (14%)]\tLoss: 174.787906\n",
      "Train Epoch: 92 [200000/697932 (29%)]\tLoss: 173.515125\n",
      "Train Epoch: 92 [300000/697932 (43%)]\tLoss: 174.036828\n",
      "Train Epoch: 92 [400000/697932 (57%)]\tLoss: 176.736891\n",
      "Train Epoch: 92 [500000/697932 (72%)]\tLoss: 178.023000\n",
      "Train Epoch: 92 [600000/697932 (86%)]\tLoss: 174.709563\n",
      "====> Epoch: 92 Average loss: 175.0804\n",
      "====> Test set loss: 176.1524\n",
      "Train Epoch: 93 [0/697932 (0%)]\tLoss: 175.812766\n",
      "Train Epoch: 93 [100000/697932 (14%)]\tLoss: 176.606266\n",
      "Train Epoch: 93 [200000/697932 (29%)]\tLoss: 174.867672\n",
      "Train Epoch: 93 [300000/697932 (43%)]\tLoss: 176.831625\n",
      "Train Epoch: 93 [400000/697932 (57%)]\tLoss: 175.266609\n",
      "Train Epoch: 93 [500000/697932 (72%)]\tLoss: 174.697188\n",
      "Train Epoch: 93 [600000/697932 (86%)]\tLoss: 174.016969\n",
      "====> Epoch: 93 Average loss: 175.1982\n",
      "====> Test set loss: 176.2990\n",
      "Train Epoch: 94 [0/697932 (0%)]\tLoss: 174.179000\n",
      "Train Epoch: 94 [100000/697932 (14%)]\tLoss: 175.568047\n",
      "Train Epoch: 94 [200000/697932 (29%)]\tLoss: 175.027234\n",
      "Train Epoch: 94 [300000/697932 (43%)]\tLoss: 175.652781\n",
      "Train Epoch: 94 [400000/697932 (57%)]\tLoss: 174.767469\n",
      "Train Epoch: 94 [500000/697932 (72%)]\tLoss: 174.811297\n",
      "Train Epoch: 94 [600000/697932 (86%)]\tLoss: 176.517797\n",
      "====> Epoch: 94 Average loss: 175.1148\n",
      "====> Test set loss: 176.1886\n",
      "Train Epoch: 95 [0/697932 (0%)]\tLoss: 173.409953\n",
      "Train Epoch: 95 [100000/697932 (14%)]\tLoss: 173.876812\n",
      "Train Epoch: 95 [200000/697932 (29%)]\tLoss: 178.013031\n",
      "Train Epoch: 95 [300000/697932 (43%)]\tLoss: 171.822203\n",
      "Train Epoch: 95 [400000/697932 (57%)]\tLoss: 177.217375\n",
      "Train Epoch: 95 [500000/697932 (72%)]\tLoss: 175.527625\n",
      "Train Epoch: 95 [600000/697932 (86%)]\tLoss: 176.345094\n",
      "====> Epoch: 95 Average loss: 175.2967\n",
      "====> Test set loss: 176.4289\n",
      "Train Epoch: 96 [0/697932 (0%)]\tLoss: 175.278953\n",
      "Train Epoch: 96 [100000/697932 (14%)]\tLoss: 174.816719\n",
      "Train Epoch: 96 [200000/697932 (29%)]\tLoss: 176.296812\n",
      "Train Epoch: 96 [300000/697932 (43%)]\tLoss: 172.589156\n",
      "Train Epoch: 96 [400000/697932 (57%)]\tLoss: 174.820344\n",
      "Train Epoch: 96 [500000/697932 (72%)]\tLoss: 171.611047\n",
      "Train Epoch: 96 [600000/697932 (86%)]\tLoss: 173.267016\n",
      "====> Epoch: 96 Average loss: 174.9542\n",
      "====> Test set loss: 175.9051\n",
      "Train Epoch: 97 [0/697932 (0%)]\tLoss: 174.022906\n",
      "Train Epoch: 97 [100000/697932 (14%)]\tLoss: 174.852859\n",
      "Train Epoch: 97 [200000/697932 (29%)]\tLoss: 174.060391\n",
      "Train Epoch: 97 [300000/697932 (43%)]\tLoss: 176.605703\n",
      "Train Epoch: 97 [400000/697932 (57%)]\tLoss: 174.091625\n",
      "Train Epoch: 97 [500000/697932 (72%)]\tLoss: 172.640281\n",
      "Train Epoch: 97 [600000/697932 (86%)]\tLoss: 173.493109\n",
      "====> Epoch: 97 Average loss: 174.7376\n",
      "====> Test set loss: 175.8267\n",
      "Train Epoch: 98 [0/697932 (0%)]\tLoss: 176.736938\n",
      "Train Epoch: 98 [100000/697932 (14%)]\tLoss: 174.424734\n",
      "Train Epoch: 98 [200000/697932 (29%)]\tLoss: 177.661578\n",
      "Train Epoch: 98 [300000/697932 (43%)]\tLoss: 171.275734\n",
      "Train Epoch: 98 [400000/697932 (57%)]\tLoss: 174.550906\n",
      "Train Epoch: 98 [500000/697932 (72%)]\tLoss: 173.838516\n",
      "Train Epoch: 98 [600000/697932 (86%)]\tLoss: 173.186563\n",
      "====> Epoch: 98 Average loss: 174.8455\n",
      "====> Test set loss: 175.9333\n",
      "Train Epoch: 99 [0/697932 (0%)]\tLoss: 178.606547\n",
      "Train Epoch: 99 [100000/697932 (14%)]\tLoss: 177.634750\n",
      "Train Epoch: 99 [200000/697932 (29%)]\tLoss: 173.840141\n",
      "Train Epoch: 99 [300000/697932 (43%)]\tLoss: 171.572500\n",
      "Train Epoch: 99 [400000/697932 (57%)]\tLoss: 175.297891\n",
      "Train Epoch: 99 [500000/697932 (72%)]\tLoss: 174.408141\n",
      "Train Epoch: 99 [600000/697932 (86%)]\tLoss: 172.502094\n",
      "====> Epoch: 99 Average loss: 174.7191\n",
      "====> Test set loss: 175.9155\n",
      "Train Epoch: 100 [0/697932 (0%)]\tLoss: 173.241125\n",
      "Train Epoch: 100 [100000/697932 (14%)]\tLoss: 174.365453\n",
      "Train Epoch: 100 [200000/697932 (29%)]\tLoss: 171.124953\n",
      "Train Epoch: 100 [300000/697932 (43%)]\tLoss: 174.728219\n",
      "Train Epoch: 100 [400000/697932 (57%)]\tLoss: 173.672625\n",
      "Train Epoch: 100 [500000/697932 (72%)]\tLoss: 176.817062\n",
      "Train Epoch: 100 [600000/697932 (86%)]\tLoss: 175.794406\n",
      "====> Epoch: 100 Average loss: 175.0528\n",
      "====> Test set loss: 176.1355\n",
      "Train Epoch: 101 [0/697932 (0%)]\tLoss: 173.015109\n",
      "Train Epoch: 101 [100000/697932 (14%)]\tLoss: 175.427906\n",
      "Train Epoch: 101 [200000/697932 (29%)]\tLoss: 174.385578\n",
      "Train Epoch: 101 [300000/697932 (43%)]\tLoss: 176.827578\n",
      "Train Epoch: 101 [400000/697932 (57%)]\tLoss: 175.814063\n",
      "Train Epoch: 101 [500000/697932 (72%)]\tLoss: 176.341797\n",
      "Train Epoch: 101 [600000/697932 (86%)]\tLoss: 173.770266\n",
      "====> Epoch: 101 Average loss: 174.7958\n",
      "====> Test set loss: 175.9592\n",
      "Train Epoch: 102 [0/697932 (0%)]\tLoss: 174.767547\n",
      "Train Epoch: 102 [100000/697932 (14%)]\tLoss: 175.431000\n",
      "Train Epoch: 102 [200000/697932 (29%)]\tLoss: 174.118969\n",
      "Train Epoch: 102 [300000/697932 (43%)]\tLoss: 174.181328\n",
      "Train Epoch: 102 [400000/697932 (57%)]\tLoss: 169.410547\n",
      "Train Epoch: 102 [500000/697932 (72%)]\tLoss: 175.772766\n",
      "Train Epoch: 102 [600000/697932 (86%)]\tLoss: 173.246875\n",
      "====> Epoch: 102 Average loss: 174.7977\n",
      "====> Test set loss: 175.9113\n",
      "Train Epoch: 103 [0/697932 (0%)]\tLoss: 176.717875\n",
      "Train Epoch: 103 [100000/697932 (14%)]\tLoss: 176.731687\n",
      "Train Epoch: 103 [200000/697932 (29%)]\tLoss: 177.974938\n",
      "Train Epoch: 103 [300000/697932 (43%)]\tLoss: 170.119344\n",
      "Train Epoch: 103 [400000/697932 (57%)]\tLoss: 176.575797\n",
      "Train Epoch: 103 [500000/697932 (72%)]\tLoss: 173.814125\n",
      "Train Epoch: 103 [600000/697932 (86%)]\tLoss: 178.886375\n",
      "====> Epoch: 103 Average loss: 174.8064\n",
      "====> Test set loss: 175.9263\n",
      "Train Epoch: 104 [0/697932 (0%)]\tLoss: 177.886422\n",
      "Train Epoch: 104 [100000/697932 (14%)]\tLoss: 173.348922\n",
      "Train Epoch: 104 [200000/697932 (29%)]\tLoss: 179.171500\n",
      "Train Epoch: 104 [300000/697932 (43%)]\tLoss: 170.549016\n",
      "Train Epoch: 104 [400000/697932 (57%)]\tLoss: 178.479984\n",
      "Train Epoch: 104 [500000/697932 (72%)]\tLoss: 175.753266\n",
      "Train Epoch: 104 [600000/697932 (86%)]\tLoss: 173.494656\n",
      "====> Epoch: 104 Average loss: 174.6504\n",
      "====> Test set loss: 175.7111\n",
      "Train Epoch: 105 [0/697932 (0%)]\tLoss: 176.965500\n",
      "Train Epoch: 105 [100000/697932 (14%)]\tLoss: 176.241719\n",
      "Train Epoch: 105 [200000/697932 (29%)]\tLoss: 173.598953\n",
      "Train Epoch: 105 [300000/697932 (43%)]\tLoss: 173.323734\n",
      "Train Epoch: 105 [400000/697932 (57%)]\tLoss: 175.581063\n",
      "Train Epoch: 105 [500000/697932 (72%)]\tLoss: 172.382891\n",
      "Train Epoch: 105 [600000/697932 (86%)]\tLoss: 176.656937\n",
      "====> Epoch: 105 Average loss: 174.6014\n",
      "====> Test set loss: 175.6517\n",
      "Train Epoch: 106 [0/697932 (0%)]\tLoss: 177.537094\n",
      "Train Epoch: 106 [100000/697932 (14%)]\tLoss: 172.573734\n",
      "Train Epoch: 106 [200000/697932 (29%)]\tLoss: 175.214766\n",
      "Train Epoch: 106 [300000/697932 (43%)]\tLoss: 175.092172\n",
      "Train Epoch: 106 [400000/697932 (57%)]\tLoss: 176.728813\n",
      "Train Epoch: 106 [500000/697932 (72%)]\tLoss: 174.749703\n",
      "Train Epoch: 106 [600000/697932 (86%)]\tLoss: 175.834000\n",
      "====> Epoch: 106 Average loss: 174.6181\n",
      "====> Test set loss: 175.7246\n",
      "Train Epoch: 107 [0/697932 (0%)]\tLoss: 174.298188\n",
      "Train Epoch: 107 [100000/697932 (14%)]\tLoss: 174.739844\n",
      "Train Epoch: 107 [200000/697932 (29%)]\tLoss: 177.183437\n",
      "Train Epoch: 107 [300000/697932 (43%)]\tLoss: 174.952719\n",
      "Train Epoch: 107 [400000/697932 (57%)]\tLoss: 177.230922\n",
      "Train Epoch: 107 [500000/697932 (72%)]\tLoss: 176.617047\n",
      "Train Epoch: 107 [600000/697932 (86%)]\tLoss: 175.334297\n",
      "====> Epoch: 107 Average loss: 174.6967\n",
      "====> Test set loss: 176.1460\n",
      "Train Epoch: 108 [0/697932 (0%)]\tLoss: 177.072016\n",
      "Train Epoch: 108 [100000/697932 (14%)]\tLoss: 178.200187\n",
      "Train Epoch: 108 [200000/697932 (29%)]\tLoss: 174.613578\n",
      "Train Epoch: 108 [300000/697932 (43%)]\tLoss: 178.351391\n",
      "Train Epoch: 108 [400000/697932 (57%)]\tLoss: 174.901141\n",
      "Train Epoch: 108 [500000/697932 (72%)]\tLoss: 174.521828\n",
      "Train Epoch: 108 [600000/697932 (86%)]\tLoss: 174.190063\n",
      "====> Epoch: 108 Average loss: 174.7302\n",
      "====> Test set loss: 175.7240\n",
      "Train Epoch: 109 [0/697932 (0%)]\tLoss: 175.468250\n",
      "Train Epoch: 109 [100000/697932 (14%)]\tLoss: 173.719422\n",
      "Train Epoch: 109 [200000/697932 (29%)]\tLoss: 175.732687\n",
      "Train Epoch: 109 [300000/697932 (43%)]\tLoss: 173.003578\n",
      "Train Epoch: 109 [400000/697932 (57%)]\tLoss: 171.857266\n",
      "Train Epoch: 109 [500000/697932 (72%)]\tLoss: 175.620578\n",
      "Train Epoch: 109 [600000/697932 (86%)]\tLoss: 175.439281\n",
      "====> Epoch: 109 Average loss: 174.6701\n",
      "====> Test set loss: 175.9834\n",
      "Train Epoch: 110 [0/697932 (0%)]\tLoss: 174.781328\n",
      "Train Epoch: 110 [100000/697932 (14%)]\tLoss: 171.797844\n",
      "Train Epoch: 110 [200000/697932 (29%)]\tLoss: 177.305813\n",
      "Train Epoch: 110 [300000/697932 (43%)]\tLoss: 170.789938\n",
      "Train Epoch: 110 [400000/697932 (57%)]\tLoss: 176.433516\n",
      "Train Epoch: 110 [500000/697932 (72%)]\tLoss: 176.677203\n",
      "Train Epoch: 110 [600000/697932 (86%)]\tLoss: 174.242813\n",
      "====> Epoch: 110 Average loss: 174.9034\n",
      "====> Test set loss: 175.6165\n",
      "Train Epoch: 111 [0/697932 (0%)]\tLoss: 172.208094\n",
      "Train Epoch: 111 [100000/697932 (14%)]\tLoss: 174.004094\n",
      "Train Epoch: 111 [200000/697932 (29%)]\tLoss: 175.952375\n",
      "Train Epoch: 111 [300000/697932 (43%)]\tLoss: 174.715156\n",
      "Train Epoch: 111 [400000/697932 (57%)]\tLoss: 175.335438\n",
      "Train Epoch: 111 [500000/697932 (72%)]\tLoss: 173.026000\n",
      "Train Epoch: 111 [600000/697932 (86%)]\tLoss: 175.017969\n",
      "====> Epoch: 111 Average loss: 174.6753\n",
      "====> Test set loss: 175.7360\n",
      "Train Epoch: 112 [0/697932 (0%)]\tLoss: 171.633109\n",
      "Train Epoch: 112 [100000/697932 (14%)]\tLoss: 174.878156\n",
      "Train Epoch: 112 [200000/697932 (29%)]\tLoss: 175.627938\n",
      "Train Epoch: 112 [300000/697932 (43%)]\tLoss: 173.682500\n",
      "Train Epoch: 112 [400000/697932 (57%)]\tLoss: 176.448438\n",
      "Train Epoch: 112 [500000/697932 (72%)]\tLoss: 173.869047\n",
      "Train Epoch: 112 [600000/697932 (86%)]\tLoss: 173.443891\n",
      "====> Epoch: 112 Average loss: 174.5598\n",
      "====> Test set loss: 175.5431\n",
      "Train Epoch: 113 [0/697932 (0%)]\tLoss: 175.124687\n",
      "Train Epoch: 113 [100000/697932 (14%)]\tLoss: 173.500406\n",
      "Train Epoch: 113 [200000/697932 (29%)]\tLoss: 176.276563\n",
      "Train Epoch: 113 [300000/697932 (43%)]\tLoss: 172.555891\n",
      "Train Epoch: 113 [400000/697932 (57%)]\tLoss: 174.580047\n",
      "Train Epoch: 113 [500000/697932 (72%)]\tLoss: 173.896000\n",
      "Train Epoch: 113 [600000/697932 (86%)]\tLoss: 173.746781\n",
      "====> Epoch: 113 Average loss: 174.4848\n",
      "====> Test set loss: 175.6225\n",
      "Train Epoch: 114 [0/697932 (0%)]\tLoss: 173.992422\n",
      "Train Epoch: 114 [100000/697932 (14%)]\tLoss: 175.232469\n",
      "Train Epoch: 114 [200000/697932 (29%)]\tLoss: 173.452766\n",
      "Train Epoch: 114 [300000/697932 (43%)]\tLoss: 174.020141\n",
      "Train Epoch: 114 [400000/697932 (57%)]\tLoss: 177.053375\n",
      "Train Epoch: 114 [500000/697932 (72%)]\tLoss: 177.603813\n",
      "Train Epoch: 114 [600000/697932 (86%)]\tLoss: 170.662937\n",
      "====> Epoch: 114 Average loss: 174.4806\n",
      "====> Test set loss: 175.6870\n",
      "Train Epoch: 115 [0/697932 (0%)]\tLoss: 175.044813\n",
      "Train Epoch: 115 [100000/697932 (14%)]\tLoss: 173.275234\n",
      "Train Epoch: 115 [200000/697932 (29%)]\tLoss: 175.627484\n",
      "Train Epoch: 115 [300000/697932 (43%)]\tLoss: 171.597875\n",
      "Train Epoch: 115 [400000/697932 (57%)]\tLoss: 176.639234\n",
      "Train Epoch: 115 [500000/697932 (72%)]\tLoss: 176.988125\n",
      "Train Epoch: 115 [600000/697932 (86%)]\tLoss: 173.276844\n",
      "====> Epoch: 115 Average loss: 174.3970\n",
      "====> Test set loss: 175.7288\n",
      "Train Epoch: 116 [0/697932 (0%)]\tLoss: 176.494531\n",
      "Train Epoch: 116 [100000/697932 (14%)]\tLoss: 173.847844\n",
      "Train Epoch: 116 [200000/697932 (29%)]\tLoss: 175.444422\n",
      "Train Epoch: 116 [300000/697932 (43%)]\tLoss: 173.854312\n",
      "Train Epoch: 116 [400000/697932 (57%)]\tLoss: 174.068953\n",
      "Train Epoch: 116 [500000/697932 (72%)]\tLoss: 174.463172\n",
      "Train Epoch: 116 [600000/697932 (86%)]\tLoss: 177.497016\n",
      "====> Epoch: 116 Average loss: 174.4938\n",
      "====> Test set loss: 175.6722\n",
      "Train Epoch: 117 [0/697932 (0%)]\tLoss: 174.832812\n",
      "Train Epoch: 117 [100000/697932 (14%)]\tLoss: 173.985187\n",
      "Train Epoch: 117 [200000/697932 (29%)]\tLoss: 173.435125\n",
      "Train Epoch: 117 [300000/697932 (43%)]\tLoss: 174.056953\n",
      "Train Epoch: 117 [400000/697932 (57%)]\tLoss: 177.754375\n",
      "Train Epoch: 117 [500000/697932 (72%)]\tLoss: 173.897719\n",
      "Train Epoch: 117 [600000/697932 (86%)]\tLoss: 175.796312\n",
      "====> Epoch: 117 Average loss: 174.6041\n",
      "====> Test set loss: 176.0111\n",
      "Train Epoch: 118 [0/697932 (0%)]\tLoss: 174.877016\n",
      "Train Epoch: 118 [100000/697932 (14%)]\tLoss: 175.730516\n",
      "Train Epoch: 118 [200000/697932 (29%)]\tLoss: 173.121000\n",
      "Train Epoch: 118 [300000/697932 (43%)]\tLoss: 170.816062\n",
      "Train Epoch: 118 [400000/697932 (57%)]\tLoss: 172.607906\n",
      "Train Epoch: 118 [500000/697932 (72%)]\tLoss: 171.418219\n",
      "Train Epoch: 118 [600000/697932 (86%)]\tLoss: 175.636156\n",
      "====> Epoch: 118 Average loss: 174.4569\n",
      "====> Test set loss: 175.4385\n",
      "Train Epoch: 119 [0/697932 (0%)]\tLoss: 174.044797\n",
      "Train Epoch: 119 [100000/697932 (14%)]\tLoss: 171.356469\n",
      "Train Epoch: 119 [200000/697932 (29%)]\tLoss: 175.020969\n",
      "Train Epoch: 119 [300000/697932 (43%)]\tLoss: 176.651344\n",
      "Train Epoch: 119 [400000/697932 (57%)]\tLoss: 175.430062\n",
      "Train Epoch: 119 [500000/697932 (72%)]\tLoss: 176.498328\n",
      "Train Epoch: 119 [600000/697932 (86%)]\tLoss: 175.050031\n",
      "====> Epoch: 119 Average loss: 174.1667\n",
      "====> Test set loss: 175.4116\n",
      "Train Epoch: 120 [0/697932 (0%)]\tLoss: 173.606844\n",
      "Train Epoch: 120 [100000/697932 (14%)]\tLoss: 174.128094\n",
      "Train Epoch: 120 [200000/697932 (29%)]\tLoss: 174.943781\n",
      "Train Epoch: 120 [300000/697932 (43%)]\tLoss: 172.337859\n",
      "Train Epoch: 120 [400000/697932 (57%)]\tLoss: 172.771453\n",
      "Train Epoch: 120 [500000/697932 (72%)]\tLoss: 176.348578\n",
      "Train Epoch: 120 [600000/697932 (86%)]\tLoss: 173.478094\n",
      "====> Epoch: 120 Average loss: 174.2438\n",
      "====> Test set loss: 175.4411\n",
      "Train Epoch: 121 [0/697932 (0%)]\tLoss: 170.917250\n",
      "Train Epoch: 121 [100000/697932 (14%)]\tLoss: 174.142547\n",
      "Train Epoch: 121 [200000/697932 (29%)]\tLoss: 178.400734\n",
      "Train Epoch: 121 [300000/697932 (43%)]\tLoss: 173.081734\n",
      "Train Epoch: 121 [400000/697932 (57%)]\tLoss: 178.336266\n",
      "Train Epoch: 121 [500000/697932 (72%)]\tLoss: 174.252344\n",
      "Train Epoch: 121 [600000/697932 (86%)]\tLoss: 174.246484\n",
      "====> Epoch: 121 Average loss: 174.2975\n",
      "====> Test set loss: 175.8400\n",
      "Train Epoch: 122 [0/697932 (0%)]\tLoss: 173.530328\n",
      "Train Epoch: 122 [100000/697932 (14%)]\tLoss: 176.792047\n",
      "Train Epoch: 122 [200000/697932 (29%)]\tLoss: 176.821547\n",
      "Train Epoch: 122 [300000/697932 (43%)]\tLoss: 172.422359\n",
      "Train Epoch: 122 [400000/697932 (57%)]\tLoss: 173.709578\n",
      "Train Epoch: 122 [500000/697932 (72%)]\tLoss: 174.186594\n",
      "Train Epoch: 122 [600000/697932 (86%)]\tLoss: 177.810422\n",
      "====> Epoch: 122 Average loss: 174.3362\n",
      "====> Test set loss: 175.7477\n",
      "Train Epoch: 123 [0/697932 (0%)]\tLoss: 176.450828\n",
      "Train Epoch: 123 [100000/697932 (14%)]\tLoss: 173.771016\n",
      "Train Epoch: 123 [200000/697932 (29%)]\tLoss: 174.931844\n",
      "Train Epoch: 123 [300000/697932 (43%)]\tLoss: 175.092250\n",
      "Train Epoch: 123 [400000/697932 (57%)]\tLoss: 173.800203\n",
      "Train Epoch: 123 [500000/697932 (72%)]\tLoss: 175.995187\n",
      "Train Epoch: 123 [600000/697932 (86%)]\tLoss: 171.025062\n",
      "====> Epoch: 123 Average loss: 174.4413\n",
      "====> Test set loss: 175.7033\n",
      "Train Epoch: 124 [0/697932 (0%)]\tLoss: 173.814031\n",
      "Train Epoch: 124 [100000/697932 (14%)]\tLoss: 173.481937\n",
      "Train Epoch: 124 [200000/697932 (29%)]\tLoss: 172.353469\n",
      "Train Epoch: 124 [300000/697932 (43%)]\tLoss: 174.047687\n",
      "Train Epoch: 124 [400000/697932 (57%)]\tLoss: 175.589594\n",
      "Train Epoch: 124 [500000/697932 (72%)]\tLoss: 175.596859\n",
      "Train Epoch: 124 [600000/697932 (86%)]\tLoss: 171.473734\n",
      "====> Epoch: 124 Average loss: 174.4480\n",
      "====> Test set loss: 175.7263\n",
      "Train Epoch: 125 [0/697932 (0%)]\tLoss: 177.856656\n",
      "Train Epoch: 125 [100000/697932 (14%)]\tLoss: 173.071531\n",
      "Train Epoch: 125 [200000/697932 (29%)]\tLoss: 172.017188\n",
      "Train Epoch: 125 [300000/697932 (43%)]\tLoss: 174.889609\n",
      "Train Epoch: 125 [400000/697932 (57%)]\tLoss: 174.854688\n",
      "Train Epoch: 125 [500000/697932 (72%)]\tLoss: 173.111312\n",
      "Train Epoch: 125 [600000/697932 (86%)]\tLoss: 175.646969\n",
      "====> Epoch: 125 Average loss: 174.2460\n",
      "====> Test set loss: 175.4974\n",
      "Train Epoch: 126 [0/697932 (0%)]\tLoss: 173.136469\n",
      "Train Epoch: 126 [100000/697932 (14%)]\tLoss: 172.377828\n",
      "Train Epoch: 126 [200000/697932 (29%)]\tLoss: 172.977516\n",
      "Train Epoch: 126 [300000/697932 (43%)]\tLoss: 174.081016\n",
      "Train Epoch: 126 [400000/697932 (57%)]\tLoss: 175.549078\n",
      "Train Epoch: 126 [500000/697932 (72%)]\tLoss: 174.382375\n",
      "Train Epoch: 126 [600000/697932 (86%)]\tLoss: 175.285156\n",
      "====> Epoch: 126 Average loss: 174.3093\n",
      "====> Test set loss: 175.3381\n",
      "Train Epoch: 127 [0/697932 (0%)]\tLoss: 171.983391\n",
      "Train Epoch: 127 [100000/697932 (14%)]\tLoss: 171.767141\n",
      "Train Epoch: 127 [200000/697932 (29%)]\tLoss: 174.025813\n",
      "Train Epoch: 127 [300000/697932 (43%)]\tLoss: 175.602875\n",
      "Train Epoch: 127 [400000/697932 (57%)]\tLoss: 171.207875\n",
      "Train Epoch: 127 [500000/697932 (72%)]\tLoss: 176.009484\n",
      "Train Epoch: 127 [600000/697932 (86%)]\tLoss: 174.057016\n",
      "====> Epoch: 127 Average loss: 174.1356\n",
      "====> Test set loss: 175.2644\n",
      "Train Epoch: 128 [0/697932 (0%)]\tLoss: 174.961625\n",
      "Train Epoch: 128 [100000/697932 (14%)]\tLoss: 172.357188\n",
      "Train Epoch: 128 [200000/697932 (29%)]\tLoss: 171.557938\n",
      "Train Epoch: 128 [300000/697932 (43%)]\tLoss: 172.985187\n",
      "Train Epoch: 128 [400000/697932 (57%)]\tLoss: 172.377922\n",
      "Train Epoch: 128 [500000/697932 (72%)]\tLoss: 175.887391\n",
      "Train Epoch: 128 [600000/697932 (86%)]\tLoss: 176.951203\n",
      "====> Epoch: 128 Average loss: 174.1917\n",
      "====> Test set loss: 175.4000\n",
      "Train Epoch: 129 [0/697932 (0%)]\tLoss: 172.491844\n",
      "Train Epoch: 129 [100000/697932 (14%)]\tLoss: 175.213625\n",
      "Train Epoch: 129 [200000/697932 (29%)]\tLoss: 172.467531\n",
      "Train Epoch: 129 [300000/697932 (43%)]\tLoss: 173.591953\n",
      "Train Epoch: 129 [400000/697932 (57%)]\tLoss: 176.121125\n",
      "Train Epoch: 129 [500000/697932 (72%)]\tLoss: 172.199109\n",
      "Train Epoch: 129 [600000/697932 (86%)]\tLoss: 176.528250\n",
      "====> Epoch: 129 Average loss: 174.1540\n",
      "====> Test set loss: 175.9330\n",
      "Train Epoch: 130 [0/697932 (0%)]\tLoss: 176.768891\n",
      "Train Epoch: 130 [100000/697932 (14%)]\tLoss: 174.172469\n",
      "Train Epoch: 130 [200000/697932 (29%)]\tLoss: 174.815125\n",
      "Train Epoch: 130 [300000/697932 (43%)]\tLoss: 174.561234\n",
      "Train Epoch: 130 [400000/697932 (57%)]\tLoss: 172.892625\n",
      "Train Epoch: 130 [500000/697932 (72%)]\tLoss: 172.223734\n",
      "Train Epoch: 130 [600000/697932 (86%)]\tLoss: 174.292500\n",
      "====> Epoch: 130 Average loss: 174.0609\n",
      "====> Test set loss: 175.3808\n",
      "Train Epoch: 131 [0/697932 (0%)]\tLoss: 172.735781\n",
      "Train Epoch: 131 [100000/697932 (14%)]\tLoss: 176.376438\n",
      "Train Epoch: 131 [200000/697932 (29%)]\tLoss: 170.257703\n",
      "Train Epoch: 131 [300000/697932 (43%)]\tLoss: 178.437547\n",
      "Train Epoch: 131 [400000/697932 (57%)]\tLoss: 172.948516\n",
      "Train Epoch: 131 [500000/697932 (72%)]\tLoss: 171.748531\n",
      "Train Epoch: 131 [600000/697932 (86%)]\tLoss: 176.630094\n",
      "====> Epoch: 131 Average loss: 174.0455\n",
      "====> Test set loss: 175.2538\n",
      "Train Epoch: 132 [0/697932 (0%)]\tLoss: 173.168313\n",
      "Train Epoch: 132 [100000/697932 (14%)]\tLoss: 173.851469\n",
      "Train Epoch: 132 [200000/697932 (29%)]\tLoss: 179.581094\n",
      "Train Epoch: 132 [300000/697932 (43%)]\tLoss: 172.348641\n",
      "Train Epoch: 132 [400000/697932 (57%)]\tLoss: 172.770391\n",
      "Train Epoch: 132 [500000/697932 (72%)]\tLoss: 175.066109\n",
      "Train Epoch: 132 [600000/697932 (86%)]\tLoss: 174.214531\n",
      "====> Epoch: 132 Average loss: 173.9407\n",
      "====> Test set loss: 175.6810\n",
      "Train Epoch: 133 [0/697932 (0%)]\tLoss: 175.061094\n",
      "Train Epoch: 133 [100000/697932 (14%)]\tLoss: 171.996906\n",
      "Train Epoch: 133 [200000/697932 (29%)]\tLoss: 174.590141\n",
      "Train Epoch: 133 [300000/697932 (43%)]\tLoss: 174.488672\n",
      "Train Epoch: 133 [400000/697932 (57%)]\tLoss: 174.876656\n",
      "Train Epoch: 133 [500000/697932 (72%)]\tLoss: 173.975844\n",
      "Train Epoch: 133 [600000/697932 (86%)]\tLoss: 173.491938\n",
      "====> Epoch: 133 Average loss: 173.9778\n",
      "====> Test set loss: 175.2763\n",
      "Train Epoch: 134 [0/697932 (0%)]\tLoss: 172.112891\n",
      "Train Epoch: 134 [100000/697932 (14%)]\tLoss: 174.421406\n",
      "Train Epoch: 134 [200000/697932 (29%)]\tLoss: 175.364469\n",
      "Train Epoch: 134 [300000/697932 (43%)]\tLoss: 173.084625\n",
      "Train Epoch: 134 [400000/697932 (57%)]\tLoss: 173.683531\n",
      "Train Epoch: 134 [500000/697932 (72%)]\tLoss: 174.254703\n",
      "Train Epoch: 134 [600000/697932 (86%)]\tLoss: 173.429594\n",
      "====> Epoch: 134 Average loss: 174.0788\n",
      "====> Test set loss: 175.5656\n",
      "Train Epoch: 135 [0/697932 (0%)]\tLoss: 174.576875\n",
      "Train Epoch: 135 [100000/697932 (14%)]\tLoss: 174.134516\n",
      "Train Epoch: 135 [200000/697932 (29%)]\tLoss: 175.629391\n",
      "Train Epoch: 135 [300000/697932 (43%)]\tLoss: 172.812641\n",
      "Train Epoch: 135 [400000/697932 (57%)]\tLoss: 173.427703\n",
      "Train Epoch: 135 [500000/697932 (72%)]\tLoss: 173.659906\n",
      "Train Epoch: 135 [600000/697932 (86%)]\tLoss: 172.859734\n",
      "====> Epoch: 135 Average loss: 173.9665\n",
      "====> Test set loss: 175.3081\n",
      "Train Epoch: 136 [0/697932 (0%)]\tLoss: 171.807063\n",
      "Train Epoch: 136 [100000/697932 (14%)]\tLoss: 178.041828\n",
      "Train Epoch: 136 [200000/697932 (29%)]\tLoss: 171.524203\n",
      "Train Epoch: 136 [300000/697932 (43%)]\tLoss: 172.781594\n",
      "Train Epoch: 136 [400000/697932 (57%)]\tLoss: 175.850313\n",
      "Train Epoch: 136 [500000/697932 (72%)]\tLoss: 172.565250\n",
      "Train Epoch: 136 [600000/697932 (86%)]\tLoss: 170.701766\n",
      "====> Epoch: 136 Average loss: 173.9499\n",
      "====> Test set loss: 175.3012\n",
      "Train Epoch: 137 [0/697932 (0%)]\tLoss: 174.026094\n",
      "Train Epoch: 137 [100000/697932 (14%)]\tLoss: 173.612719\n",
      "Train Epoch: 137 [200000/697932 (29%)]\tLoss: 174.294234\n",
      "Train Epoch: 137 [300000/697932 (43%)]\tLoss: 170.991469\n",
      "Train Epoch: 137 [400000/697932 (57%)]\tLoss: 175.242437\n",
      "Train Epoch: 137 [500000/697932 (72%)]\tLoss: 172.374625\n",
      "Train Epoch: 137 [600000/697932 (86%)]\tLoss: 172.125281\n",
      "====> Epoch: 137 Average loss: 174.0842\n",
      "====> Test set loss: 175.5101\n",
      "Train Epoch: 138 [0/697932 (0%)]\tLoss: 173.933437\n",
      "Train Epoch: 138 [100000/697932 (14%)]\tLoss: 174.569797\n",
      "Train Epoch: 138 [200000/697932 (29%)]\tLoss: 173.060375\n",
      "Train Epoch: 138 [300000/697932 (43%)]\tLoss: 175.657844\n",
      "Train Epoch: 138 [400000/697932 (57%)]\tLoss: 174.898641\n",
      "Train Epoch: 138 [500000/697932 (72%)]\tLoss: 172.685875\n",
      "Train Epoch: 138 [600000/697932 (86%)]\tLoss: 172.910219\n",
      "====> Epoch: 138 Average loss: 174.1582\n",
      "====> Test set loss: 175.2468\n",
      "Train Epoch: 139 [0/697932 (0%)]\tLoss: 173.966469\n",
      "Train Epoch: 139 [100000/697932 (14%)]\tLoss: 173.651484\n",
      "Train Epoch: 139 [200000/697932 (29%)]\tLoss: 176.619391\n",
      "Train Epoch: 139 [300000/697932 (43%)]\tLoss: 172.654281\n",
      "Train Epoch: 139 [400000/697932 (57%)]\tLoss: 177.024687\n",
      "Train Epoch: 139 [500000/697932 (72%)]\tLoss: 177.304781\n",
      "Train Epoch: 139 [600000/697932 (86%)]\tLoss: 172.786750\n",
      "====> Epoch: 139 Average loss: 174.1027\n",
      "====> Test set loss: 175.3279\n",
      "Train Epoch: 140 [0/697932 (0%)]\tLoss: 174.686859\n",
      "Train Epoch: 140 [100000/697932 (14%)]\tLoss: 176.141531\n",
      "Train Epoch: 140 [200000/697932 (29%)]\tLoss: 175.027844\n",
      "Train Epoch: 140 [300000/697932 (43%)]\tLoss: 176.242656\n",
      "Train Epoch: 140 [400000/697932 (57%)]\tLoss: 174.235203\n",
      "Train Epoch: 140 [500000/697932 (72%)]\tLoss: 173.940531\n",
      "Train Epoch: 140 [600000/697932 (86%)]\tLoss: 175.396625\n",
      "====> Epoch: 140 Average loss: 174.0080\n",
      "====> Test set loss: 175.1496\n",
      "Train Epoch: 141 [0/697932 (0%)]\tLoss: 175.151187\n",
      "Train Epoch: 141 [100000/697932 (14%)]\tLoss: 175.771422\n",
      "Train Epoch: 141 [200000/697932 (29%)]\tLoss: 176.425703\n",
      "Train Epoch: 141 [300000/697932 (43%)]\tLoss: 173.418969\n",
      "Train Epoch: 141 [400000/697932 (57%)]\tLoss: 172.357563\n",
      "Train Epoch: 141 [500000/697932 (72%)]\tLoss: 173.600937\n",
      "Train Epoch: 141 [600000/697932 (86%)]\tLoss: 174.279141\n",
      "====> Epoch: 141 Average loss: 174.0472\n",
      "====> Test set loss: 175.3315\n",
      "Train Epoch: 142 [0/697932 (0%)]\tLoss: 172.940719\n",
      "Train Epoch: 142 [100000/697932 (14%)]\tLoss: 175.732125\n",
      "Train Epoch: 142 [200000/697932 (29%)]\tLoss: 174.277016\n",
      "Train Epoch: 142 [300000/697932 (43%)]\tLoss: 173.111484\n",
      "Train Epoch: 142 [400000/697932 (57%)]\tLoss: 171.799688\n",
      "Train Epoch: 142 [500000/697932 (72%)]\tLoss: 176.116016\n",
      "Train Epoch: 142 [600000/697932 (86%)]\tLoss: 173.970625\n",
      "====> Epoch: 142 Average loss: 173.9391\n",
      "====> Test set loss: 175.2111\n",
      "Train Epoch: 143 [0/697932 (0%)]\tLoss: 173.391797\n",
      "Train Epoch: 143 [100000/697932 (14%)]\tLoss: 173.290391\n",
      "Train Epoch: 143 [200000/697932 (29%)]\tLoss: 173.745125\n",
      "Train Epoch: 143 [300000/697932 (43%)]\tLoss: 174.218672\n",
      "Train Epoch: 143 [400000/697932 (57%)]\tLoss: 172.719406\n",
      "Train Epoch: 143 [500000/697932 (72%)]\tLoss: 175.409250\n",
      "Train Epoch: 143 [600000/697932 (86%)]\tLoss: 172.936906\n",
      "====> Epoch: 143 Average loss: 173.9485\n",
      "====> Test set loss: 175.4045\n",
      "Train Epoch: 144 [0/697932 (0%)]\tLoss: 172.447391\n",
      "Train Epoch: 144 [100000/697932 (14%)]\tLoss: 173.826016\n",
      "Train Epoch: 144 [200000/697932 (29%)]\tLoss: 176.748797\n",
      "Train Epoch: 144 [300000/697932 (43%)]\tLoss: 176.409562\n",
      "Train Epoch: 144 [400000/697932 (57%)]\tLoss: 174.613406\n",
      "Train Epoch: 144 [500000/697932 (72%)]\tLoss: 172.894375\n",
      "Train Epoch: 144 [600000/697932 (86%)]\tLoss: 172.312266\n",
      "====> Epoch: 144 Average loss: 173.9169\n",
      "====> Test set loss: 175.0959\n",
      "Train Epoch: 145 [0/697932 (0%)]\tLoss: 171.667656\n",
      "Train Epoch: 145 [100000/697932 (14%)]\tLoss: 174.825078\n",
      "Train Epoch: 145 [200000/697932 (29%)]\tLoss: 174.739234\n",
      "Train Epoch: 145 [300000/697932 (43%)]\tLoss: 171.683609\n",
      "Train Epoch: 145 [400000/697932 (57%)]\tLoss: 174.924234\n",
      "Train Epoch: 145 [500000/697932 (72%)]\tLoss: 175.650094\n",
      "Train Epoch: 145 [600000/697932 (86%)]\tLoss: 174.486281\n",
      "====> Epoch: 145 Average loss: 173.9356\n",
      "====> Test set loss: 175.3631\n",
      "Train Epoch: 146 [0/697932 (0%)]\tLoss: 171.780406\n",
      "Train Epoch: 146 [100000/697932 (14%)]\tLoss: 174.601531\n",
      "Train Epoch: 146 [200000/697932 (29%)]\tLoss: 172.485688\n",
      "Train Epoch: 146 [300000/697932 (43%)]\tLoss: 177.680922\n",
      "Train Epoch: 146 [400000/697932 (57%)]\tLoss: 171.112313\n",
      "Train Epoch: 146 [500000/697932 (72%)]\tLoss: 174.016469\n",
      "Train Epoch: 146 [600000/697932 (86%)]\tLoss: 174.084438\n",
      "====> Epoch: 146 Average loss: 173.7713\n",
      "====> Test set loss: 175.2681\n",
      "Train Epoch: 147 [0/697932 (0%)]\tLoss: 173.309406\n",
      "Train Epoch: 147 [100000/697932 (14%)]\tLoss: 178.209812\n",
      "Train Epoch: 147 [200000/697932 (29%)]\tLoss: 174.084281\n",
      "Train Epoch: 147 [300000/697932 (43%)]\tLoss: 176.012844\n",
      "Train Epoch: 147 [400000/697932 (57%)]\tLoss: 171.082875\n",
      "Train Epoch: 147 [500000/697932 (72%)]\tLoss: 171.712125\n",
      "Train Epoch: 147 [600000/697932 (86%)]\tLoss: 175.261203\n",
      "====> Epoch: 147 Average loss: 173.7704\n",
      "====> Test set loss: 174.9483\n",
      "Train Epoch: 148 [0/697932 (0%)]\tLoss: 171.489656\n",
      "Train Epoch: 148 [100000/697932 (14%)]\tLoss: 173.813328\n",
      "Train Epoch: 148 [200000/697932 (29%)]\tLoss: 172.364984\n",
      "Train Epoch: 148 [300000/697932 (43%)]\tLoss: 173.833609\n",
      "Train Epoch: 148 [400000/697932 (57%)]\tLoss: 176.741391\n",
      "Train Epoch: 148 [500000/697932 (72%)]\tLoss: 175.194641\n",
      "Train Epoch: 148 [600000/697932 (86%)]\tLoss: 171.919875\n",
      "====> Epoch: 148 Average loss: 173.8560\n",
      "====> Test set loss: 175.2105\n",
      "Train Epoch: 149 [0/697932 (0%)]\tLoss: 173.699750\n",
      "Train Epoch: 149 [100000/697932 (14%)]\tLoss: 169.884859\n",
      "Train Epoch: 149 [200000/697932 (29%)]\tLoss: 175.718609\n",
      "Train Epoch: 149 [300000/697932 (43%)]\tLoss: 174.748453\n",
      "Train Epoch: 149 [400000/697932 (57%)]\tLoss: 174.890062\n",
      "Train Epoch: 149 [500000/697932 (72%)]\tLoss: 175.404937\n",
      "Train Epoch: 149 [600000/697932 (86%)]\tLoss: 177.528531\n",
      "====> Epoch: 149 Average loss: 173.8596\n",
      "====> Test set loss: 174.9901\n",
      "Train Epoch: 150 [0/697932 (0%)]\tLoss: 174.519031\n",
      "Train Epoch: 150 [100000/697932 (14%)]\tLoss: 173.298469\n",
      "Train Epoch: 150 [200000/697932 (29%)]\tLoss: 171.748531\n",
      "Train Epoch: 150 [300000/697932 (43%)]\tLoss: 173.303656\n",
      "Train Epoch: 150 [400000/697932 (57%)]\tLoss: 173.342437\n",
      "Train Epoch: 150 [500000/697932 (72%)]\tLoss: 172.313156\n",
      "Train Epoch: 150 [600000/697932 (86%)]\tLoss: 174.962938\n",
      "====> Epoch: 150 Average loss: 173.7109\n",
      "====> Test set loss: 175.0043\n",
      "Train Epoch: 151 [0/697932 (0%)]\tLoss: 173.054234\n",
      "Train Epoch: 151 [100000/697932 (14%)]\tLoss: 172.644563\n",
      "Train Epoch: 151 [200000/697932 (29%)]\tLoss: 172.262687\n",
      "Train Epoch: 151 [300000/697932 (43%)]\tLoss: 172.489297\n",
      "Train Epoch: 151 [400000/697932 (57%)]\tLoss: 172.627750\n",
      "Train Epoch: 151 [500000/697932 (72%)]\tLoss: 170.065047\n",
      "Train Epoch: 151 [600000/697932 (86%)]\tLoss: 174.246563\n",
      "====> Epoch: 151 Average loss: 173.7270\n",
      "====> Test set loss: 175.0006\n",
      "Train Epoch: 152 [0/697932 (0%)]\tLoss: 176.095250\n",
      "Train Epoch: 152 [100000/697932 (14%)]\tLoss: 172.740797\n",
      "Train Epoch: 152 [200000/697932 (29%)]\tLoss: 174.629797\n",
      "Train Epoch: 152 [300000/697932 (43%)]\tLoss: 176.575969\n",
      "Train Epoch: 152 [400000/697932 (57%)]\tLoss: 172.423656\n",
      "Train Epoch: 152 [500000/697932 (72%)]\tLoss: 174.314297\n",
      "Train Epoch: 152 [600000/697932 (86%)]\tLoss: 174.621797\n",
      "====> Epoch: 152 Average loss: 173.8299\n",
      "====> Test set loss: 175.0768\n",
      "Train Epoch: 153 [0/697932 (0%)]\tLoss: 174.167266\n",
      "Train Epoch: 153 [100000/697932 (14%)]\tLoss: 173.039062\n",
      "Train Epoch: 153 [200000/697932 (29%)]\tLoss: 173.886453\n",
      "Train Epoch: 153 [300000/697932 (43%)]\tLoss: 174.523094\n",
      "Train Epoch: 153 [400000/697932 (57%)]\tLoss: 174.817469\n",
      "Train Epoch: 153 [500000/697932 (72%)]\tLoss: 172.072953\n",
      "Train Epoch: 153 [600000/697932 (86%)]\tLoss: 171.216969\n",
      "====> Epoch: 153 Average loss: 173.8808\n",
      "====> Test set loss: 175.0872\n",
      "Train Epoch: 154 [0/697932 (0%)]\tLoss: 171.913672\n",
      "Train Epoch: 154 [100000/697932 (14%)]\tLoss: 173.315687\n",
      "Train Epoch: 154 [200000/697932 (29%)]\tLoss: 170.688641\n",
      "Train Epoch: 154 [300000/697932 (43%)]\tLoss: 170.051797\n",
      "Train Epoch: 154 [400000/697932 (57%)]\tLoss: 172.035828\n",
      "Train Epoch: 154 [500000/697932 (72%)]\tLoss: 173.278687\n",
      "Train Epoch: 154 [600000/697932 (86%)]\tLoss: 172.852922\n",
      "====> Epoch: 154 Average loss: 173.8588\n",
      "====> Test set loss: 175.2056\n",
      "Train Epoch: 155 [0/697932 (0%)]\tLoss: 175.988531\n",
      "Train Epoch: 155 [100000/697932 (14%)]\tLoss: 173.565641\n",
      "Train Epoch: 155 [200000/697932 (29%)]\tLoss: 174.561563\n",
      "Train Epoch: 155 [300000/697932 (43%)]\tLoss: 173.886469\n",
      "Train Epoch: 155 [400000/697932 (57%)]\tLoss: 172.618484\n",
      "Train Epoch: 155 [500000/697932 (72%)]\tLoss: 173.409031\n",
      "Train Epoch: 155 [600000/697932 (86%)]\tLoss: 175.745891\n",
      "====> Epoch: 155 Average loss: 173.7808\n",
      "====> Test set loss: 174.9465\n",
      "Train Epoch: 156 [0/697932 (0%)]\tLoss: 174.397578\n",
      "Train Epoch: 156 [100000/697932 (14%)]\tLoss: 174.371984\n",
      "Train Epoch: 156 [200000/697932 (29%)]\tLoss: 176.306688\n",
      "Train Epoch: 156 [300000/697932 (43%)]\tLoss: 172.331063\n",
      "Train Epoch: 156 [400000/697932 (57%)]\tLoss: 176.980984\n",
      "Train Epoch: 156 [500000/697932 (72%)]\tLoss: 173.660922\n",
      "Train Epoch: 156 [600000/697932 (86%)]\tLoss: 176.347016\n",
      "====> Epoch: 156 Average loss: 173.7800\n",
      "====> Test set loss: 175.0937\n",
      "Train Epoch: 157 [0/697932 (0%)]\tLoss: 175.202063\n",
      "Train Epoch: 157 [100000/697932 (14%)]\tLoss: 173.431609\n",
      "Train Epoch: 157 [200000/697932 (29%)]\tLoss: 172.665906\n",
      "Train Epoch: 157 [300000/697932 (43%)]\tLoss: 172.188109\n",
      "Train Epoch: 157 [400000/697932 (57%)]\tLoss: 174.986766\n",
      "Train Epoch: 157 [500000/697932 (72%)]\tLoss: 171.093437\n",
      "Train Epoch: 157 [600000/697932 (86%)]\tLoss: 171.625359\n",
      "====> Epoch: 157 Average loss: 173.6581\n",
      "====> Test set loss: 175.2067\n",
      "Train Epoch: 158 [0/697932 (0%)]\tLoss: 171.161172\n",
      "Train Epoch: 158 [100000/697932 (14%)]\tLoss: 173.401047\n",
      "Train Epoch: 158 [200000/697932 (29%)]\tLoss: 172.020047\n",
      "Train Epoch: 158 [300000/697932 (43%)]\tLoss: 171.591141\n",
      "Train Epoch: 158 [400000/697932 (57%)]\tLoss: 173.995453\n",
      "Train Epoch: 158 [500000/697932 (72%)]\tLoss: 173.493359\n",
      "Train Epoch: 158 [600000/697932 (86%)]\tLoss: 171.308969\n",
      "====> Epoch: 158 Average loss: 173.7874\n",
      "====> Test set loss: 175.1887\n",
      "Train Epoch: 159 [0/697932 (0%)]\tLoss: 174.468906\n",
      "Train Epoch: 159 [100000/697932 (14%)]\tLoss: 173.861203\n",
      "Train Epoch: 159 [200000/697932 (29%)]\tLoss: 173.529438\n",
      "Train Epoch: 159 [300000/697932 (43%)]\tLoss: 174.587469\n",
      "Train Epoch: 159 [400000/697932 (57%)]\tLoss: 173.232922\n",
      "Train Epoch: 159 [500000/697932 (72%)]\tLoss: 175.198203\n",
      "Train Epoch: 159 [600000/697932 (86%)]\tLoss: 171.787875\n",
      "====> Epoch: 159 Average loss: 173.7113\n",
      "====> Test set loss: 174.8777\n",
      "Train Epoch: 160 [0/697932 (0%)]\tLoss: 175.004219\n",
      "Train Epoch: 160 [100000/697932 (14%)]\tLoss: 172.968891\n",
      "Train Epoch: 160 [200000/697932 (29%)]\tLoss: 175.214188\n",
      "Train Epoch: 160 [300000/697932 (43%)]\tLoss: 174.090984\n",
      "Train Epoch: 160 [400000/697932 (57%)]\tLoss: 176.166422\n",
      "Train Epoch: 160 [500000/697932 (72%)]\tLoss: 172.566188\n",
      "Train Epoch: 160 [600000/697932 (86%)]\tLoss: 174.490781\n",
      "====> Epoch: 160 Average loss: 173.5205\n",
      "====> Test set loss: 174.8981\n",
      "Train Epoch: 161 [0/697932 (0%)]\tLoss: 173.071391\n",
      "Train Epoch: 161 [100000/697932 (14%)]\tLoss: 172.794219\n",
      "Train Epoch: 161 [200000/697932 (29%)]\tLoss: 173.297797\n",
      "Train Epoch: 161 [300000/697932 (43%)]\tLoss: 176.256281\n",
      "Train Epoch: 161 [400000/697932 (57%)]\tLoss: 175.481719\n",
      "Train Epoch: 161 [500000/697932 (72%)]\tLoss: 173.767859\n",
      "Train Epoch: 161 [600000/697932 (86%)]\tLoss: 173.313875\n",
      "====> Epoch: 161 Average loss: 173.5217\n",
      "====> Test set loss: 174.8327\n",
      "Train Epoch: 162 [0/697932 (0%)]\tLoss: 175.068359\n",
      "Train Epoch: 162 [100000/697932 (14%)]\tLoss: 172.222375\n",
      "Train Epoch: 162 [200000/697932 (29%)]\tLoss: 174.795578\n",
      "Train Epoch: 162 [300000/697932 (43%)]\tLoss: 169.558391\n",
      "Train Epoch: 162 [400000/697932 (57%)]\tLoss: 175.912734\n",
      "Train Epoch: 162 [500000/697932 (72%)]\tLoss: 175.519219\n",
      "Train Epoch: 162 [600000/697932 (86%)]\tLoss: 173.450344\n",
      "====> Epoch: 162 Average loss: 173.8072\n",
      "====> Test set loss: 175.5411\n",
      "Train Epoch: 163 [0/697932 (0%)]\tLoss: 175.127766\n",
      "Train Epoch: 163 [100000/697932 (14%)]\tLoss: 172.914016\n",
      "Train Epoch: 163 [200000/697932 (29%)]\tLoss: 174.906094\n",
      "Train Epoch: 163 [300000/697932 (43%)]\tLoss: 172.502547\n",
      "Train Epoch: 163 [400000/697932 (57%)]\tLoss: 175.133281\n",
      "Train Epoch: 163 [500000/697932 (72%)]\tLoss: 176.029266\n",
      "Train Epoch: 163 [600000/697932 (86%)]\tLoss: 174.587094\n",
      "====> Epoch: 163 Average loss: 174.1027\n",
      "====> Test set loss: 175.4415\n",
      "Train Epoch: 164 [0/697932 (0%)]\tLoss: 174.534328\n",
      "Train Epoch: 164 [100000/697932 (14%)]\tLoss: 175.081344\n",
      "Train Epoch: 164 [200000/697932 (29%)]\tLoss: 172.045797\n",
      "Train Epoch: 164 [300000/697932 (43%)]\tLoss: 172.924875\n",
      "Train Epoch: 164 [400000/697932 (57%)]\tLoss: 173.552516\n",
      "Train Epoch: 164 [500000/697932 (72%)]\tLoss: 172.710234\n",
      "Train Epoch: 164 [600000/697932 (86%)]\tLoss: 173.647156\n",
      "====> Epoch: 164 Average loss: 173.5889\n",
      "====> Test set loss: 174.8486\n",
      "Train Epoch: 165 [0/697932 (0%)]\tLoss: 171.326891\n",
      "Train Epoch: 165 [100000/697932 (14%)]\tLoss: 171.547469\n",
      "Train Epoch: 165 [200000/697932 (29%)]\tLoss: 169.771094\n",
      "Train Epoch: 165 [300000/697932 (43%)]\tLoss: 173.128766\n",
      "Train Epoch: 165 [400000/697932 (57%)]\tLoss: 171.316781\n",
      "Train Epoch: 165 [500000/697932 (72%)]\tLoss: 173.361984\n",
      "Train Epoch: 165 [600000/697932 (86%)]\tLoss: 175.320500\n",
      "====> Epoch: 165 Average loss: 173.5447\n",
      "====> Test set loss: 174.8350\n",
      "Train Epoch: 166 [0/697932 (0%)]\tLoss: 172.293391\n",
      "Train Epoch: 166 [100000/697932 (14%)]\tLoss: 174.312547\n",
      "Train Epoch: 166 [200000/697932 (29%)]\tLoss: 174.138922\n",
      "Train Epoch: 166 [300000/697932 (43%)]\tLoss: 172.277406\n",
      "Train Epoch: 166 [400000/697932 (57%)]\tLoss: 173.700250\n",
      "Train Epoch: 166 [500000/697932 (72%)]\tLoss: 169.178438\n",
      "Train Epoch: 166 [600000/697932 (86%)]\tLoss: 172.951484\n",
      "====> Epoch: 166 Average loss: 173.5249\n",
      "====> Test set loss: 174.7491\n",
      "Train Epoch: 167 [0/697932 (0%)]\tLoss: 169.720937\n",
      "Train Epoch: 167 [100000/697932 (14%)]\tLoss: 172.525719\n",
      "Train Epoch: 167 [200000/697932 (29%)]\tLoss: 176.091359\n",
      "Train Epoch: 167 [300000/697932 (43%)]\tLoss: 177.826578\n",
      "Train Epoch: 167 [400000/697932 (57%)]\tLoss: 174.658703\n",
      "Train Epoch: 167 [500000/697932 (72%)]\tLoss: 170.995344\n",
      "Train Epoch: 167 [600000/697932 (86%)]\tLoss: 170.850781\n",
      "====> Epoch: 167 Average loss: 173.7924\n",
      "====> Test set loss: 175.3590\n",
      "Train Epoch: 168 [0/697932 (0%)]\tLoss: 173.730063\n",
      "Train Epoch: 168 [100000/697932 (14%)]\tLoss: 174.416953\n",
      "Train Epoch: 168 [200000/697932 (29%)]\tLoss: 172.511906\n",
      "Train Epoch: 168 [300000/697932 (43%)]\tLoss: 175.728656\n",
      "Train Epoch: 168 [400000/697932 (57%)]\tLoss: 172.942906\n",
      "Train Epoch: 168 [500000/697932 (72%)]\tLoss: 172.482297\n",
      "Train Epoch: 168 [600000/697932 (86%)]\tLoss: 174.741766\n",
      "====> Epoch: 168 Average loss: 173.6837\n",
      "====> Test set loss: 175.0092\n",
      "Train Epoch: 169 [0/697932 (0%)]\tLoss: 175.107500\n",
      "Train Epoch: 169 [100000/697932 (14%)]\tLoss: 170.632359\n",
      "Train Epoch: 169 [200000/697932 (29%)]\tLoss: 173.808703\n",
      "Train Epoch: 169 [300000/697932 (43%)]\tLoss: 174.727812\n",
      "Train Epoch: 169 [400000/697932 (57%)]\tLoss: 170.471656\n",
      "Train Epoch: 169 [500000/697932 (72%)]\tLoss: 173.505141\n",
      "Train Epoch: 169 [600000/697932 (86%)]\tLoss: 173.855859\n",
      "====> Epoch: 169 Average loss: 173.6191\n",
      "====> Test set loss: 174.8201\n",
      "Train Epoch: 170 [0/697932 (0%)]\tLoss: 173.119219\n",
      "Train Epoch: 170 [100000/697932 (14%)]\tLoss: 174.102781\n",
      "Train Epoch: 170 [200000/697932 (29%)]\tLoss: 171.643766\n",
      "Train Epoch: 170 [300000/697932 (43%)]\tLoss: 174.032281\n",
      "Train Epoch: 170 [400000/697932 (57%)]\tLoss: 172.385406\n",
      "Train Epoch: 170 [500000/697932 (72%)]\tLoss: 173.921359\n",
      "Train Epoch: 170 [600000/697932 (86%)]\tLoss: 173.850578\n",
      "====> Epoch: 170 Average loss: 173.5140\n",
      "====> Test set loss: 174.7368\n",
      "Train Epoch: 171 [0/697932 (0%)]\tLoss: 173.829031\n",
      "Train Epoch: 171 [100000/697932 (14%)]\tLoss: 172.216641\n",
      "Train Epoch: 171 [200000/697932 (29%)]\tLoss: 176.088281\n",
      "Train Epoch: 171 [300000/697932 (43%)]\tLoss: 172.807453\n",
      "Train Epoch: 171 [400000/697932 (57%)]\tLoss: 173.563578\n",
      "Train Epoch: 171 [500000/697932 (72%)]\tLoss: 171.969063\n",
      "Train Epoch: 171 [600000/697932 (86%)]\tLoss: 174.622109\n",
      "====> Epoch: 171 Average loss: 173.3531\n",
      "====> Test set loss: 174.9168\n",
      "Train Epoch: 172 [0/697932 (0%)]\tLoss: 173.637328\n",
      "Train Epoch: 172 [100000/697932 (14%)]\tLoss: 176.017703\n",
      "Train Epoch: 172 [200000/697932 (29%)]\tLoss: 174.520625\n",
      "Train Epoch: 172 [300000/697932 (43%)]\tLoss: 170.012328\n",
      "Train Epoch: 172 [400000/697932 (57%)]\tLoss: 174.231547\n",
      "Train Epoch: 172 [500000/697932 (72%)]\tLoss: 170.410344\n",
      "Train Epoch: 172 [600000/697932 (86%)]\tLoss: 172.334062\n",
      "====> Epoch: 172 Average loss: 173.6177\n",
      "====> Test set loss: 175.1821\n",
      "Train Epoch: 173 [0/697932 (0%)]\tLoss: 174.819922\n",
      "Train Epoch: 173 [100000/697932 (14%)]\tLoss: 174.535828\n",
      "Train Epoch: 173 [200000/697932 (29%)]\tLoss: 174.434359\n",
      "Train Epoch: 173 [300000/697932 (43%)]\tLoss: 176.732922\n",
      "Train Epoch: 173 [400000/697932 (57%)]\tLoss: 170.507047\n",
      "Train Epoch: 173 [500000/697932 (72%)]\tLoss: 171.633641\n",
      "Train Epoch: 173 [600000/697932 (86%)]\tLoss: 168.012375\n",
      "====> Epoch: 173 Average loss: 173.5955\n",
      "====> Test set loss: 175.0115\n",
      "Train Epoch: 174 [0/697932 (0%)]\tLoss: 172.500672\n",
      "Train Epoch: 174 [100000/697932 (14%)]\tLoss: 171.967156\n",
      "Train Epoch: 174 [200000/697932 (29%)]\tLoss: 174.984344\n",
      "Train Epoch: 174 [300000/697932 (43%)]\tLoss: 170.280672\n",
      "Train Epoch: 174 [400000/697932 (57%)]\tLoss: 174.755437\n",
      "Train Epoch: 174 [500000/697932 (72%)]\tLoss: 173.369328\n",
      "Train Epoch: 174 [600000/697932 (86%)]\tLoss: 174.731266\n",
      "====> Epoch: 174 Average loss: 173.4438\n",
      "====> Test set loss: 174.7755\n",
      "Train Epoch: 175 [0/697932 (0%)]\tLoss: 174.488484\n",
      "Train Epoch: 175 [100000/697932 (14%)]\tLoss: 177.385016\n",
      "Train Epoch: 175 [200000/697932 (29%)]\tLoss: 172.045031\n",
      "Train Epoch: 175 [300000/697932 (43%)]\tLoss: 171.885391\n",
      "Train Epoch: 175 [400000/697932 (57%)]\tLoss: 172.070719\n",
      "Train Epoch: 175 [500000/697932 (72%)]\tLoss: 172.385672\n",
      "Train Epoch: 175 [600000/697932 (86%)]\tLoss: 174.416812\n",
      "====> Epoch: 175 Average loss: 173.4849\n",
      "====> Test set loss: 174.8779\n",
      "Train Epoch: 176 [0/697932 (0%)]\tLoss: 173.315797\n",
      "Train Epoch: 176 [100000/697932 (14%)]\tLoss: 169.042031\n",
      "Train Epoch: 176 [200000/697932 (29%)]\tLoss: 172.980203\n",
      "Train Epoch: 176 [300000/697932 (43%)]\tLoss: 173.348187\n",
      "Train Epoch: 176 [400000/697932 (57%)]\tLoss: 170.521375\n",
      "Train Epoch: 176 [500000/697932 (72%)]\tLoss: 172.015766\n",
      "Train Epoch: 176 [600000/697932 (86%)]\tLoss: 175.411687\n",
      "====> Epoch: 176 Average loss: 173.4667\n",
      "====> Test set loss: 175.0879\n",
      "Train Epoch: 177 [0/697932 (0%)]\tLoss: 173.110938\n",
      "Train Epoch: 177 [100000/697932 (14%)]\tLoss: 172.173563\n",
      "Train Epoch: 177 [200000/697932 (29%)]\tLoss: 173.399453\n",
      "Train Epoch: 177 [300000/697932 (43%)]\tLoss: 174.184297\n",
      "Train Epoch: 177 [400000/697932 (57%)]\tLoss: 169.686438\n",
      "Train Epoch: 177 [500000/697932 (72%)]\tLoss: 173.722031\n",
      "Train Epoch: 177 [600000/697932 (86%)]\tLoss: 173.289656\n",
      "====> Epoch: 177 Average loss: 173.5850\n",
      "====> Test set loss: 174.6909\n",
      "Train Epoch: 178 [0/697932 (0%)]\tLoss: 173.525641\n",
      "Train Epoch: 178 [100000/697932 (14%)]\tLoss: 172.734375\n",
      "Train Epoch: 178 [200000/697932 (29%)]\tLoss: 174.512344\n",
      "Train Epoch: 178 [300000/697932 (43%)]\tLoss: 174.736422\n",
      "Train Epoch: 178 [400000/697932 (57%)]\tLoss: 173.826437\n",
      "Train Epoch: 178 [500000/697932 (72%)]\tLoss: 172.546828\n",
      "Train Epoch: 178 [600000/697932 (86%)]\tLoss: 174.389000\n",
      "====> Epoch: 178 Average loss: 173.6137\n",
      "====> Test set loss: 175.1477\n",
      "Train Epoch: 179 [0/697932 (0%)]\tLoss: 173.238406\n",
      "Train Epoch: 179 [100000/697932 (14%)]\tLoss: 173.932781\n",
      "Train Epoch: 179 [200000/697932 (29%)]\tLoss: 173.691703\n",
      "Train Epoch: 179 [300000/697932 (43%)]\tLoss: 173.109547\n",
      "Train Epoch: 179 [400000/697932 (57%)]\tLoss: 174.637531\n",
      "Train Epoch: 179 [500000/697932 (72%)]\tLoss: 175.767719\n",
      "Train Epoch: 179 [600000/697932 (86%)]\tLoss: 173.164719\n",
      "====> Epoch: 179 Average loss: 173.7414\n",
      "====> Test set loss: 175.1059\n",
      "Train Epoch: 180 [0/697932 (0%)]\tLoss: 174.372281\n",
      "Train Epoch: 180 [100000/697932 (14%)]\tLoss: 172.167844\n",
      "Train Epoch: 180 [200000/697932 (29%)]\tLoss: 174.582375\n",
      "Train Epoch: 180 [300000/697932 (43%)]\tLoss: 174.873312\n",
      "Train Epoch: 180 [400000/697932 (57%)]\tLoss: 171.230187\n",
      "Train Epoch: 180 [500000/697932 (72%)]\tLoss: 172.843953\n",
      "Train Epoch: 180 [600000/697932 (86%)]\tLoss: 170.733359\n",
      "====> Epoch: 180 Average loss: 173.5776\n",
      "====> Test set loss: 174.9127\n",
      "Train Epoch: 181 [0/697932 (0%)]\tLoss: 173.273016\n",
      "Train Epoch: 181 [100000/697932 (14%)]\tLoss: 173.228766\n",
      "Train Epoch: 181 [200000/697932 (29%)]\tLoss: 172.940172\n",
      "Train Epoch: 181 [300000/697932 (43%)]\tLoss: 174.269297\n",
      "Train Epoch: 181 [400000/697932 (57%)]\tLoss: 175.581125\n",
      "Train Epoch: 181 [500000/697932 (72%)]\tLoss: 174.657109\n",
      "Train Epoch: 181 [600000/697932 (86%)]\tLoss: 176.926047\n",
      "====> Epoch: 181 Average loss: 173.5140\n",
      "====> Test set loss: 174.7998\n",
      "Train Epoch: 182 [0/697932 (0%)]\tLoss: 172.364141\n",
      "Train Epoch: 182 [100000/697932 (14%)]\tLoss: 173.885000\n",
      "Train Epoch: 182 [200000/697932 (29%)]\tLoss: 172.438109\n",
      "Train Epoch: 182 [300000/697932 (43%)]\tLoss: 171.149406\n",
      "Train Epoch: 182 [400000/697932 (57%)]\tLoss: 173.727500\n",
      "Train Epoch: 182 [500000/697932 (72%)]\tLoss: 175.659359\n",
      "Train Epoch: 182 [600000/697932 (86%)]\tLoss: 174.621234\n",
      "====> Epoch: 182 Average loss: 173.4227\n",
      "====> Test set loss: 174.6993\n",
      "Train Epoch: 183 [0/697932 (0%)]\tLoss: 175.387953\n",
      "Train Epoch: 183 [100000/697932 (14%)]\tLoss: 172.989547\n",
      "Train Epoch: 183 [200000/697932 (29%)]\tLoss: 172.608875\n",
      "Train Epoch: 183 [300000/697932 (43%)]\tLoss: 170.184297\n",
      "Train Epoch: 183 [400000/697932 (57%)]\tLoss: 176.051125\n",
      "Train Epoch: 183 [500000/697932 (72%)]\tLoss: 172.274188\n",
      "Train Epoch: 183 [600000/697932 (86%)]\tLoss: 171.612859\n",
      "====> Epoch: 183 Average loss: 173.4176\n",
      "====> Test set loss: 174.6120\n",
      "Train Epoch: 184 [0/697932 (0%)]\tLoss: 175.216656\n",
      "Train Epoch: 184 [100000/697932 (14%)]\tLoss: 173.760234\n",
      "Train Epoch: 184 [200000/697932 (29%)]\tLoss: 173.609719\n",
      "Train Epoch: 184 [300000/697932 (43%)]\tLoss: 170.290297\n",
      "Train Epoch: 184 [400000/697932 (57%)]\tLoss: 172.005094\n",
      "Train Epoch: 184 [500000/697932 (72%)]\tLoss: 173.655031\n",
      "Train Epoch: 184 [600000/697932 (86%)]\tLoss: 174.594469\n",
      "====> Epoch: 184 Average loss: 173.4061\n",
      "====> Test set loss: 174.9479\n",
      "Train Epoch: 185 [0/697932 (0%)]\tLoss: 172.496266\n",
      "Train Epoch: 185 [100000/697932 (14%)]\tLoss: 177.687969\n",
      "Train Epoch: 185 [200000/697932 (29%)]\tLoss: 172.187656\n",
      "Train Epoch: 185 [300000/697932 (43%)]\tLoss: 171.356141\n",
      "Train Epoch: 185 [400000/697932 (57%)]\tLoss: 174.156703\n",
      "Train Epoch: 185 [500000/697932 (72%)]\tLoss: 174.275016\n",
      "Train Epoch: 185 [600000/697932 (86%)]\tLoss: 175.196562\n",
      "====> Epoch: 185 Average loss: 173.7171\n",
      "====> Test set loss: 175.6390\n",
      "Train Epoch: 186 [0/697932 (0%)]\tLoss: 176.984500\n",
      "Train Epoch: 186 [100000/697932 (14%)]\tLoss: 176.778703\n",
      "Train Epoch: 186 [200000/697932 (29%)]\tLoss: 174.194187\n",
      "Train Epoch: 186 [300000/697932 (43%)]\tLoss: 171.243453\n",
      "Train Epoch: 186 [400000/697932 (57%)]\tLoss: 173.641734\n",
      "Train Epoch: 186 [500000/697932 (72%)]\tLoss: 172.812219\n",
      "Train Epoch: 186 [600000/697932 (86%)]\tLoss: 171.933391\n",
      "====> Epoch: 186 Average loss: 173.7152\n",
      "====> Test set loss: 174.7540\n",
      "Train Epoch: 187 [0/697932 (0%)]\tLoss: 173.037594\n",
      "Train Epoch: 187 [100000/697932 (14%)]\tLoss: 177.060281\n",
      "Train Epoch: 187 [200000/697932 (29%)]\tLoss: 174.834281\n",
      "Train Epoch: 187 [300000/697932 (43%)]\tLoss: 172.424984\n",
      "Train Epoch: 187 [400000/697932 (57%)]\tLoss: 175.507516\n",
      "Train Epoch: 187 [500000/697932 (72%)]\tLoss: 175.066438\n",
      "Train Epoch: 187 [600000/697932 (86%)]\tLoss: 173.414094\n",
      "====> Epoch: 187 Average loss: 173.5994\n",
      "====> Test set loss: 175.0636\n",
      "Train Epoch: 188 [0/697932 (0%)]\tLoss: 177.254531\n",
      "Train Epoch: 188 [100000/697932 (14%)]\tLoss: 171.412062\n",
      "Train Epoch: 188 [200000/697932 (29%)]\tLoss: 175.056062\n",
      "Train Epoch: 188 [300000/697932 (43%)]\tLoss: 172.459734\n",
      "Train Epoch: 188 [400000/697932 (57%)]\tLoss: 172.929000\n",
      "Train Epoch: 188 [500000/697932 (72%)]\tLoss: 173.180484\n",
      "Train Epoch: 188 [600000/697932 (86%)]\tLoss: 173.943953\n",
      "====> Epoch: 188 Average loss: 173.4367\n",
      "====> Test set loss: 174.6716\n",
      "Train Epoch: 189 [0/697932 (0%)]\tLoss: 173.799609\n",
      "Train Epoch: 189 [100000/697932 (14%)]\tLoss: 171.668313\n",
      "Train Epoch: 189 [200000/697932 (29%)]\tLoss: 171.163813\n",
      "Train Epoch: 189 [300000/697932 (43%)]\tLoss: 175.634984\n",
      "Train Epoch: 189 [400000/697932 (57%)]\tLoss: 172.728594\n",
      "Train Epoch: 189 [500000/697932 (72%)]\tLoss: 173.761375\n",
      "Train Epoch: 189 [600000/697932 (86%)]\tLoss: 174.710969\n",
      "====> Epoch: 189 Average loss: 173.4957\n",
      "====> Test set loss: 174.7786\n",
      "Train Epoch: 190 [0/697932 (0%)]\tLoss: 172.023313\n",
      "Train Epoch: 190 [100000/697932 (14%)]\tLoss: 170.121172\n",
      "Train Epoch: 190 [200000/697932 (29%)]\tLoss: 174.415984\n",
      "Train Epoch: 190 [300000/697932 (43%)]\tLoss: 173.852203\n",
      "Train Epoch: 190 [400000/697932 (57%)]\tLoss: 175.829297\n",
      "Train Epoch: 190 [500000/697932 (72%)]\tLoss: 173.705750\n",
      "Train Epoch: 190 [600000/697932 (86%)]\tLoss: 174.320172\n",
      "====> Epoch: 190 Average loss: 173.5016\n",
      "====> Test set loss: 175.2544\n",
      "Train Epoch: 191 [0/697932 (0%)]\tLoss: 176.533969\n",
      "Train Epoch: 191 [100000/697932 (14%)]\tLoss: 177.505750\n",
      "Train Epoch: 191 [200000/697932 (29%)]\tLoss: 170.551672\n",
      "Train Epoch: 191 [300000/697932 (43%)]\tLoss: 174.914641\n",
      "Train Epoch: 191 [400000/697932 (57%)]\tLoss: 174.201375\n",
      "Train Epoch: 191 [500000/697932 (72%)]\tLoss: 173.095438\n",
      "Train Epoch: 191 [600000/697932 (86%)]\tLoss: 176.830312\n",
      "====> Epoch: 191 Average loss: 173.4576\n",
      "====> Test set loss: 174.5016\n",
      "Train Epoch: 192 [0/697932 (0%)]\tLoss: 174.588313\n",
      "Train Epoch: 192 [100000/697932 (14%)]\tLoss: 174.103922\n",
      "Train Epoch: 192 [200000/697932 (29%)]\tLoss: 175.326344\n",
      "Train Epoch: 192 [300000/697932 (43%)]\tLoss: 172.717328\n",
      "Train Epoch: 192 [400000/697932 (57%)]\tLoss: 172.801703\n",
      "Train Epoch: 192 [500000/697932 (72%)]\tLoss: 173.801078\n",
      "Train Epoch: 192 [600000/697932 (86%)]\tLoss: 171.530891\n",
      "====> Epoch: 192 Average loss: 173.2734\n",
      "====> Test set loss: 174.7022\n",
      "Train Epoch: 193 [0/697932 (0%)]\tLoss: 175.005656\n",
      "Train Epoch: 193 [100000/697932 (14%)]\tLoss: 172.943406\n",
      "Train Epoch: 193 [200000/697932 (29%)]\tLoss: 168.182609\n",
      "Train Epoch: 193 [300000/697932 (43%)]\tLoss: 173.579266\n",
      "Train Epoch: 193 [400000/697932 (57%)]\tLoss: 171.357266\n",
      "Train Epoch: 193 [500000/697932 (72%)]\tLoss: 172.668328\n",
      "Train Epoch: 193 [600000/697932 (86%)]\tLoss: 175.075125\n",
      "====> Epoch: 193 Average loss: 173.2279\n",
      "====> Test set loss: 174.4333\n",
      "Train Epoch: 194 [0/697932 (0%)]\tLoss: 171.798984\n",
      "Train Epoch: 194 [100000/697932 (14%)]\tLoss: 174.997406\n",
      "Train Epoch: 194 [200000/697932 (29%)]\tLoss: 172.644438\n",
      "Train Epoch: 194 [300000/697932 (43%)]\tLoss: 170.808078\n",
      "Train Epoch: 194 [400000/697932 (57%)]\tLoss: 176.102156\n",
      "Train Epoch: 194 [500000/697932 (72%)]\tLoss: 176.471016\n",
      "Train Epoch: 194 [600000/697932 (86%)]\tLoss: 172.976953\n",
      "====> Epoch: 194 Average loss: 173.3461\n",
      "====> Test set loss: 174.8173\n",
      "Train Epoch: 195 [0/697932 (0%)]\tLoss: 169.897797\n",
      "Train Epoch: 195 [100000/697932 (14%)]\tLoss: 169.925281\n",
      "Train Epoch: 195 [200000/697932 (29%)]\tLoss: 169.864578\n",
      "Train Epoch: 195 [300000/697932 (43%)]\tLoss: 176.834516\n",
      "Train Epoch: 195 [400000/697932 (57%)]\tLoss: 175.289984\n",
      "Train Epoch: 195 [500000/697932 (72%)]\tLoss: 173.492625\n",
      "Train Epoch: 195 [600000/697932 (86%)]\tLoss: 171.903359\n",
      "====> Epoch: 195 Average loss: 173.6556\n",
      "====> Test set loss: 174.8454\n",
      "Train Epoch: 196 [0/697932 (0%)]\tLoss: 173.737891\n",
      "Train Epoch: 196 [100000/697932 (14%)]\tLoss: 174.172562\n",
      "Train Epoch: 196 [200000/697932 (29%)]\tLoss: 174.119812\n",
      "Train Epoch: 196 [300000/697932 (43%)]\tLoss: 172.944906\n",
      "Train Epoch: 196 [400000/697932 (57%)]\tLoss: 175.391719\n",
      "Train Epoch: 196 [500000/697932 (72%)]\tLoss: 172.962969\n",
      "Train Epoch: 196 [600000/697932 (86%)]\tLoss: 173.169437\n",
      "====> Epoch: 196 Average loss: 173.2729\n",
      "====> Test set loss: 174.8639\n",
      "Train Epoch: 197 [0/697932 (0%)]\tLoss: 172.243406\n",
      "Train Epoch: 197 [100000/697932 (14%)]\tLoss: 177.170969\n",
      "Train Epoch: 197 [200000/697932 (29%)]\tLoss: 173.734406\n",
      "Train Epoch: 197 [300000/697932 (43%)]\tLoss: 171.769141\n",
      "Train Epoch: 197 [400000/697932 (57%)]\tLoss: 172.577719\n",
      "Train Epoch: 197 [500000/697932 (72%)]\tLoss: 171.706906\n",
      "Train Epoch: 197 [600000/697932 (86%)]\tLoss: 174.994641\n",
      "====> Epoch: 197 Average loss: 173.3548\n",
      "====> Test set loss: 174.6371\n",
      "Train Epoch: 198 [0/697932 (0%)]\tLoss: 174.199813\n",
      "Train Epoch: 198 [100000/697932 (14%)]\tLoss: 173.697359\n",
      "Train Epoch: 198 [200000/697932 (29%)]\tLoss: 172.921938\n",
      "Train Epoch: 198 [300000/697932 (43%)]\tLoss: 171.309672\n",
      "Train Epoch: 198 [400000/697932 (57%)]\tLoss: 169.848062\n",
      "Train Epoch: 198 [500000/697932 (72%)]\tLoss: 173.798344\n",
      "Train Epoch: 198 [600000/697932 (86%)]\tLoss: 173.171828\n",
      "====> Epoch: 198 Average loss: 173.2288\n",
      "====> Test set loss: 174.5240\n",
      "Train Epoch: 199 [0/697932 (0%)]\tLoss: 175.617625\n",
      "Train Epoch: 199 [100000/697932 (14%)]\tLoss: 173.533891\n",
      "Train Epoch: 199 [200000/697932 (29%)]\tLoss: 172.367641\n",
      "Train Epoch: 199 [300000/697932 (43%)]\tLoss: 174.656781\n",
      "Train Epoch: 199 [400000/697932 (57%)]\tLoss: 176.134422\n",
      "Train Epoch: 199 [500000/697932 (72%)]\tLoss: 168.806484\n",
      "Train Epoch: 199 [600000/697932 (86%)]\tLoss: 174.039750\n",
      "====> Epoch: 199 Average loss: 173.4488\n",
      "====> Test set loss: 174.6411\n",
      "Train Epoch: 200 [0/697932 (0%)]\tLoss: 174.672984\n",
      "Train Epoch: 200 [100000/697932 (14%)]\tLoss: 173.867781\n",
      "Train Epoch: 200 [200000/697932 (29%)]\tLoss: 171.151406\n",
      "Train Epoch: 200 [300000/697932 (43%)]\tLoss: 170.902156\n",
      "Train Epoch: 200 [400000/697932 (57%)]\tLoss: 174.384141\n",
      "Train Epoch: 200 [500000/697932 (72%)]\tLoss: 175.059969\n",
      "Train Epoch: 200 [600000/697932 (86%)]\tLoss: 173.963594\n",
      "====> Epoch: 200 Average loss: 173.2169\n",
      "====> Test set loss: 174.6988\n",
      "Train Epoch: 201 [0/697932 (0%)]\tLoss: 174.901844\n",
      "Train Epoch: 201 [100000/697932 (14%)]\tLoss: 172.400125\n",
      "Train Epoch: 201 [200000/697932 (29%)]\tLoss: 174.154281\n",
      "Train Epoch: 201 [300000/697932 (43%)]\tLoss: 171.661703\n",
      "Train Epoch: 201 [400000/697932 (57%)]\tLoss: 174.035156\n",
      "Train Epoch: 201 [500000/697932 (72%)]\tLoss: 173.652469\n",
      "Train Epoch: 201 [600000/697932 (86%)]\tLoss: 172.384484\n",
      "====> Epoch: 201 Average loss: 173.2603\n",
      "====> Test set loss: 174.5810\n",
      "Train Epoch: 202 [0/697932 (0%)]\tLoss: 174.829937\n",
      "Train Epoch: 202 [100000/697932 (14%)]\tLoss: 173.433719\n",
      "Train Epoch: 202 [200000/697932 (29%)]\tLoss: 170.677594\n",
      "Train Epoch: 202 [300000/697932 (43%)]\tLoss: 171.564781\n",
      "Train Epoch: 202 [400000/697932 (57%)]\tLoss: 173.316719\n",
      "Train Epoch: 202 [500000/697932 (72%)]\tLoss: 170.543750\n",
      "Train Epoch: 202 [600000/697932 (86%)]\tLoss: 169.507656\n",
      "====> Epoch: 202 Average loss: 173.1935\n",
      "====> Test set loss: 174.7768\n",
      "Train Epoch: 203 [0/697932 (0%)]\tLoss: 173.524219\n",
      "Train Epoch: 203 [100000/697932 (14%)]\tLoss: 174.771547\n",
      "Train Epoch: 203 [200000/697932 (29%)]\tLoss: 176.189016\n",
      "Train Epoch: 203 [300000/697932 (43%)]\tLoss: 171.751578\n",
      "Train Epoch: 203 [400000/697932 (57%)]\tLoss: 171.097156\n",
      "Train Epoch: 203 [500000/697932 (72%)]\tLoss: 173.976812\n",
      "Train Epoch: 203 [600000/697932 (86%)]\tLoss: 173.589766\n",
      "====> Epoch: 203 Average loss: 173.3291\n",
      "====> Test set loss: 175.2848\n",
      "Train Epoch: 204 [0/697932 (0%)]\tLoss: 171.902922\n",
      "Train Epoch: 204 [100000/697932 (14%)]\tLoss: 171.951219\n",
      "Train Epoch: 204 [200000/697932 (29%)]\tLoss: 174.328250\n",
      "Train Epoch: 204 [300000/697932 (43%)]\tLoss: 174.109172\n",
      "Train Epoch: 204 [400000/697932 (57%)]\tLoss: 173.966844\n",
      "Train Epoch: 204 [500000/697932 (72%)]\tLoss: 173.235609\n",
      "Train Epoch: 204 [600000/697932 (86%)]\tLoss: 176.597969\n",
      "====> Epoch: 204 Average loss: 173.5691\n",
      "====> Test set loss: 174.7275\n",
      "Train Epoch: 205 [0/697932 (0%)]\tLoss: 172.200609\n",
      "Train Epoch: 205 [100000/697932 (14%)]\tLoss: 173.149375\n",
      "Train Epoch: 205 [200000/697932 (29%)]\tLoss: 173.669344\n",
      "Train Epoch: 205 [300000/697932 (43%)]\tLoss: 172.893781\n",
      "Train Epoch: 205 [400000/697932 (57%)]\tLoss: 175.810062\n",
      "Train Epoch: 205 [500000/697932 (72%)]\tLoss: 175.732453\n",
      "Train Epoch: 205 [600000/697932 (86%)]\tLoss: 175.251422\n",
      "====> Epoch: 205 Average loss: 173.3965\n",
      "====> Test set loss: 175.3301\n",
      "Train Epoch: 206 [0/697932 (0%)]\tLoss: 177.062234\n",
      "Train Epoch: 206 [100000/697932 (14%)]\tLoss: 171.153578\n",
      "Train Epoch: 206 [200000/697932 (29%)]\tLoss: 174.668141\n",
      "Train Epoch: 206 [300000/697932 (43%)]\tLoss: 175.150438\n",
      "Train Epoch: 206 [400000/697932 (57%)]\tLoss: 173.952078\n",
      "Train Epoch: 206 [500000/697932 (72%)]\tLoss: 171.893375\n",
      "Train Epoch: 206 [600000/697932 (86%)]\tLoss: 175.177016\n",
      "====> Epoch: 206 Average loss: 173.4874\n",
      "====> Test set loss: 174.7348\n",
      "Train Epoch: 207 [0/697932 (0%)]\tLoss: 172.273453\n",
      "Train Epoch: 207 [100000/697932 (14%)]\tLoss: 174.358563\n",
      "Train Epoch: 207 [200000/697932 (29%)]\tLoss: 174.060922\n",
      "Train Epoch: 207 [300000/697932 (43%)]\tLoss: 171.032359\n",
      "Train Epoch: 207 [400000/697932 (57%)]\tLoss: 172.730125\n",
      "Train Epoch: 207 [500000/697932 (72%)]\tLoss: 169.581437\n",
      "Train Epoch: 207 [600000/697932 (86%)]\tLoss: 173.014250\n",
      "====> Epoch: 207 Average loss: 173.3361\n",
      "====> Test set loss: 174.8504\n",
      "Train Epoch: 208 [0/697932 (0%)]\tLoss: 174.228359\n",
      "Train Epoch: 208 [100000/697932 (14%)]\tLoss: 175.565344\n",
      "Train Epoch: 208 [200000/697932 (29%)]\tLoss: 176.142922\n",
      "Train Epoch: 208 [300000/697932 (43%)]\tLoss: 172.961125\n",
      "Train Epoch: 208 [400000/697932 (57%)]\tLoss: 173.359391\n",
      "Train Epoch: 208 [500000/697932 (72%)]\tLoss: 172.708344\n",
      "Train Epoch: 208 [600000/697932 (86%)]\tLoss: 172.920453\n",
      "====> Epoch: 208 Average loss: 173.2889\n",
      "====> Test set loss: 174.3240\n",
      "Train Epoch: 209 [0/697932 (0%)]\tLoss: 174.415437\n",
      "Train Epoch: 209 [100000/697932 (14%)]\tLoss: 171.029984\n",
      "Train Epoch: 209 [200000/697932 (29%)]\tLoss: 173.693875\n",
      "Train Epoch: 209 [300000/697932 (43%)]\tLoss: 171.838375\n",
      "Train Epoch: 209 [400000/697932 (57%)]\tLoss: 173.023625\n",
      "Train Epoch: 209 [500000/697932 (72%)]\tLoss: 170.304266\n",
      "Train Epoch: 209 [600000/697932 (86%)]\tLoss: 171.139438\n",
      "====> Epoch: 209 Average loss: 172.9193\n",
      "====> Test set loss: 174.5151\n",
      "Train Epoch: 210 [0/697932 (0%)]\tLoss: 173.473484\n",
      "Train Epoch: 210 [100000/697932 (14%)]\tLoss: 171.093422\n",
      "Train Epoch: 210 [200000/697932 (29%)]\tLoss: 172.903781\n",
      "Train Epoch: 210 [300000/697932 (43%)]\tLoss: 174.328359\n",
      "Train Epoch: 210 [400000/697932 (57%)]\tLoss: 172.851719\n",
      "Train Epoch: 210 [500000/697932 (72%)]\tLoss: 171.823484\n",
      "Train Epoch: 210 [600000/697932 (86%)]\tLoss: 173.374484\n",
      "====> Epoch: 210 Average loss: 172.9669\n",
      "====> Test set loss: 174.2958\n",
      "Train Epoch: 211 [0/697932 (0%)]\tLoss: 172.210844\n",
      "Train Epoch: 211 [100000/697932 (14%)]\tLoss: 173.415609\n",
      "Train Epoch: 211 [200000/697932 (29%)]\tLoss: 175.967859\n",
      "Train Epoch: 211 [300000/697932 (43%)]\tLoss: 171.286500\n",
      "Train Epoch: 211 [400000/697932 (57%)]\tLoss: 172.483578\n",
      "Train Epoch: 211 [500000/697932 (72%)]\tLoss: 168.499250\n",
      "Train Epoch: 211 [600000/697932 (86%)]\tLoss: 176.060719\n",
      "====> Epoch: 211 Average loss: 172.9970\n",
      "====> Test set loss: 174.4083\n",
      "Train Epoch: 212 [0/697932 (0%)]\tLoss: 170.852234\n",
      "Train Epoch: 212 [100000/697932 (14%)]\tLoss: 171.659344\n",
      "Train Epoch: 212 [200000/697932 (29%)]\tLoss: 172.110047\n",
      "Train Epoch: 212 [300000/697932 (43%)]\tLoss: 173.027750\n",
      "Train Epoch: 212 [400000/697932 (57%)]\tLoss: 172.191375\n",
      "Train Epoch: 212 [500000/697932 (72%)]\tLoss: 174.976156\n",
      "Train Epoch: 212 [600000/697932 (86%)]\tLoss: 175.228969\n",
      "====> Epoch: 212 Average loss: 173.1627\n",
      "====> Test set loss: 174.3005\n",
      "Train Epoch: 213 [0/697932 (0%)]\tLoss: 175.060859\n",
      "Train Epoch: 213 [100000/697932 (14%)]\tLoss: 169.275703\n",
      "Train Epoch: 213 [200000/697932 (29%)]\tLoss: 172.038609\n",
      "Train Epoch: 213 [300000/697932 (43%)]\tLoss: 168.572422\n",
      "Train Epoch: 213 [400000/697932 (57%)]\tLoss: 173.174844\n",
      "Train Epoch: 213 [500000/697932 (72%)]\tLoss: 170.505219\n",
      "Train Epoch: 213 [600000/697932 (86%)]\tLoss: 173.057656\n",
      "====> Epoch: 213 Average loss: 173.0166\n",
      "====> Test set loss: 174.6311\n",
      "Train Epoch: 214 [0/697932 (0%)]\tLoss: 170.791016\n",
      "Train Epoch: 214 [100000/697932 (14%)]\tLoss: 171.244828\n",
      "Train Epoch: 214 [200000/697932 (29%)]\tLoss: 173.138859\n",
      "Train Epoch: 214 [300000/697932 (43%)]\tLoss: 173.595500\n",
      "Train Epoch: 214 [400000/697932 (57%)]\tLoss: 174.594437\n",
      "Train Epoch: 214 [500000/697932 (72%)]\tLoss: 173.612000\n",
      "Train Epoch: 214 [600000/697932 (86%)]\tLoss: 169.306312\n",
      "====> Epoch: 214 Average loss: 173.1156\n",
      "====> Test set loss: 174.3547\n",
      "Train Epoch: 215 [0/697932 (0%)]\tLoss: 173.562359\n",
      "Train Epoch: 215 [100000/697932 (14%)]\tLoss: 170.605125\n",
      "Train Epoch: 215 [200000/697932 (29%)]\tLoss: 171.242500\n",
      "Train Epoch: 215 [300000/697932 (43%)]\tLoss: 174.446078\n",
      "Train Epoch: 215 [400000/697932 (57%)]\tLoss: 170.575594\n",
      "Train Epoch: 215 [500000/697932 (72%)]\tLoss: 174.113188\n",
      "Train Epoch: 215 [600000/697932 (86%)]\tLoss: 171.575906\n",
      "====> Epoch: 215 Average loss: 172.9377\n",
      "====> Test set loss: 174.5316\n",
      "Train Epoch: 216 [0/697932 (0%)]\tLoss: 175.711109\n",
      "Train Epoch: 216 [100000/697932 (14%)]\tLoss: 174.910125\n",
      "Train Epoch: 216 [200000/697932 (29%)]\tLoss: 172.945031\n",
      "Train Epoch: 216 [300000/697932 (43%)]\tLoss: 171.919578\n",
      "Train Epoch: 216 [400000/697932 (57%)]\tLoss: 175.493672\n",
      "Train Epoch: 216 [500000/697932 (72%)]\tLoss: 171.963891\n",
      "Train Epoch: 216 [600000/697932 (86%)]\tLoss: 173.475281\n",
      "====> Epoch: 216 Average loss: 172.8538\n",
      "====> Test set loss: 174.0933\n",
      "Train Epoch: 217 [0/697932 (0%)]\tLoss: 173.245109\n",
      "Train Epoch: 217 [100000/697932 (14%)]\tLoss: 173.062813\n",
      "Train Epoch: 217 [200000/697932 (29%)]\tLoss: 172.672250\n",
      "Train Epoch: 217 [300000/697932 (43%)]\tLoss: 172.228703\n",
      "Train Epoch: 217 [400000/697932 (57%)]\tLoss: 172.219547\n",
      "Train Epoch: 217 [500000/697932 (72%)]\tLoss: 176.237594\n",
      "Train Epoch: 217 [600000/697932 (86%)]\tLoss: 171.703016\n",
      "====> Epoch: 217 Average loss: 172.7581\n",
      "====> Test set loss: 174.4128\n",
      "Train Epoch: 218 [0/697932 (0%)]\tLoss: 173.076297\n",
      "Train Epoch: 218 [100000/697932 (14%)]\tLoss: 175.765844\n",
      "Train Epoch: 218 [200000/697932 (29%)]\tLoss: 173.168078\n",
      "Train Epoch: 218 [300000/697932 (43%)]\tLoss: 172.147969\n",
      "Train Epoch: 218 [400000/697932 (57%)]\tLoss: 175.262859\n",
      "Train Epoch: 218 [500000/697932 (72%)]\tLoss: 171.586781\n",
      "Train Epoch: 218 [600000/697932 (86%)]\tLoss: 171.971094\n",
      "====> Epoch: 218 Average loss: 172.7723\n",
      "====> Test set loss: 174.4019\n",
      "Train Epoch: 219 [0/697932 (0%)]\tLoss: 173.848156\n",
      "Train Epoch: 219 [100000/697932 (14%)]\tLoss: 173.777094\n",
      "Train Epoch: 219 [200000/697932 (29%)]\tLoss: 174.099141\n",
      "Train Epoch: 219 [300000/697932 (43%)]\tLoss: 169.362562\n",
      "Train Epoch: 219 [400000/697932 (57%)]\tLoss: 175.282406\n",
      "Train Epoch: 219 [500000/697932 (72%)]\tLoss: 175.149375\n",
      "Train Epoch: 219 [600000/697932 (86%)]\tLoss: 172.742750\n",
      "====> Epoch: 219 Average loss: 172.8557\n",
      "====> Test set loss: 174.1970\n",
      "Train Epoch: 220 [0/697932 (0%)]\tLoss: 173.188500\n",
      "Train Epoch: 220 [100000/697932 (14%)]\tLoss: 175.292469\n",
      "Train Epoch: 220 [200000/697932 (29%)]\tLoss: 173.706938\n",
      "Train Epoch: 220 [300000/697932 (43%)]\tLoss: 171.711906\n",
      "Train Epoch: 220 [400000/697932 (57%)]\tLoss: 172.802141\n",
      "Train Epoch: 220 [500000/697932 (72%)]\tLoss: 173.519922\n",
      "Train Epoch: 220 [600000/697932 (86%)]\tLoss: 170.510797\n",
      "====> Epoch: 220 Average loss: 172.7244\n",
      "====> Test set loss: 174.1654\n",
      "Train Epoch: 221 [0/697932 (0%)]\tLoss: 172.702375\n",
      "Train Epoch: 221 [100000/697932 (14%)]\tLoss: 172.734094\n",
      "Train Epoch: 221 [200000/697932 (29%)]\tLoss: 173.410109\n",
      "Train Epoch: 221 [300000/697932 (43%)]\tLoss: 173.703141\n",
      "Train Epoch: 221 [400000/697932 (57%)]\tLoss: 171.133234\n",
      "Train Epoch: 221 [500000/697932 (72%)]\tLoss: 173.640234\n",
      "Train Epoch: 221 [600000/697932 (86%)]\tLoss: 169.308203\n",
      "====> Epoch: 221 Average loss: 172.8012\n",
      "====> Test set loss: 174.3255\n",
      "Train Epoch: 222 [0/697932 (0%)]\tLoss: 171.936438\n",
      "Train Epoch: 222 [100000/697932 (14%)]\tLoss: 173.425469\n",
      "Train Epoch: 222 [200000/697932 (29%)]\tLoss: 171.367625\n",
      "Train Epoch: 222 [300000/697932 (43%)]\tLoss: 171.871250\n",
      "Train Epoch: 222 [400000/697932 (57%)]\tLoss: 169.233672\n",
      "Train Epoch: 222 [500000/697932 (72%)]\tLoss: 173.708687\n",
      "Train Epoch: 222 [600000/697932 (86%)]\tLoss: 170.105719\n",
      "====> Epoch: 222 Average loss: 173.0011\n",
      "====> Test set loss: 174.5860\n",
      "Train Epoch: 223 [0/697932 (0%)]\tLoss: 171.513234\n",
      "Train Epoch: 223 [100000/697932 (14%)]\tLoss: 175.096656\n",
      "Train Epoch: 223 [200000/697932 (29%)]\tLoss: 172.685625\n",
      "Train Epoch: 223 [300000/697932 (43%)]\tLoss: 169.361766\n",
      "Train Epoch: 223 [400000/697932 (57%)]\tLoss: 174.708922\n",
      "Train Epoch: 223 [500000/697932 (72%)]\tLoss: 173.073828\n",
      "Train Epoch: 223 [600000/697932 (86%)]\tLoss: 171.162516\n",
      "====> Epoch: 223 Average loss: 173.0223\n",
      "====> Test set loss: 174.3299\n",
      "Train Epoch: 224 [0/697932 (0%)]\tLoss: 173.516031\n",
      "Train Epoch: 224 [100000/697932 (14%)]\tLoss: 173.932328\n",
      "Train Epoch: 224 [200000/697932 (29%)]\tLoss: 172.735953\n",
      "Train Epoch: 224 [300000/697932 (43%)]\tLoss: 173.012172\n",
      "Train Epoch: 224 [400000/697932 (57%)]\tLoss: 170.181187\n",
      "Train Epoch: 224 [500000/697932 (72%)]\tLoss: 173.774859\n",
      "Train Epoch: 224 [600000/697932 (86%)]\tLoss: 172.773297\n",
      "====> Epoch: 224 Average loss: 173.0787\n",
      "====> Test set loss: 174.6717\n",
      "Train Epoch: 225 [0/697932 (0%)]\tLoss: 177.064141\n",
      "Train Epoch: 225 [100000/697932 (14%)]\tLoss: 171.218578\n",
      "Train Epoch: 225 [200000/697932 (29%)]\tLoss: 171.865609\n",
      "Train Epoch: 225 [300000/697932 (43%)]\tLoss: 173.016047\n",
      "Train Epoch: 225 [400000/697932 (57%)]\tLoss: 171.534562\n",
      "Train Epoch: 225 [500000/697932 (72%)]\tLoss: 173.498047\n",
      "Train Epoch: 225 [600000/697932 (86%)]\tLoss: 176.173891\n",
      "====> Epoch: 225 Average loss: 172.9081\n",
      "====> Test set loss: 174.1252\n",
      "Train Epoch: 226 [0/697932 (0%)]\tLoss: 173.539984\n",
      "Train Epoch: 226 [100000/697932 (14%)]\tLoss: 171.644547\n",
      "Train Epoch: 226 [200000/697932 (29%)]\tLoss: 171.916672\n",
      "Train Epoch: 226 [300000/697932 (43%)]\tLoss: 171.723781\n",
      "Train Epoch: 226 [400000/697932 (57%)]\tLoss: 170.303000\n",
      "Train Epoch: 226 [500000/697932 (72%)]\tLoss: 171.515969\n",
      "Train Epoch: 226 [600000/697932 (86%)]\tLoss: 174.463109\n",
      "====> Epoch: 226 Average loss: 172.8692\n",
      "====> Test set loss: 174.3541\n",
      "Train Epoch: 227 [0/697932 (0%)]\tLoss: 171.853875\n",
      "Train Epoch: 227 [100000/697932 (14%)]\tLoss: 168.784812\n",
      "Train Epoch: 227 [200000/697932 (29%)]\tLoss: 170.522375\n",
      "Train Epoch: 227 [300000/697932 (43%)]\tLoss: 172.683719\n",
      "Train Epoch: 227 [400000/697932 (57%)]\tLoss: 175.183344\n",
      "Train Epoch: 227 [500000/697932 (72%)]\tLoss: 172.850812\n",
      "Train Epoch: 227 [600000/697932 (86%)]\tLoss: 173.277203\n",
      "====> Epoch: 227 Average loss: 172.6517\n",
      "====> Test set loss: 174.4390\n",
      "Train Epoch: 228 [0/697932 (0%)]\tLoss: 171.364234\n",
      "Train Epoch: 228 [100000/697932 (14%)]\tLoss: 177.189906\n",
      "Train Epoch: 228 [200000/697932 (29%)]\tLoss: 173.855125\n",
      "Train Epoch: 228 [300000/697932 (43%)]\tLoss: 175.145516\n",
      "Train Epoch: 228 [400000/697932 (57%)]\tLoss: 173.813531\n",
      "Train Epoch: 228 [500000/697932 (72%)]\tLoss: 171.134859\n",
      "Train Epoch: 228 [600000/697932 (86%)]\tLoss: 174.960016\n",
      "====> Epoch: 228 Average loss: 172.9031\n",
      "====> Test set loss: 174.7815\n",
      "Train Epoch: 229 [0/697932 (0%)]\tLoss: 173.128391\n",
      "Train Epoch: 229 [100000/697932 (14%)]\tLoss: 174.350578\n",
      "Train Epoch: 229 [200000/697932 (29%)]\tLoss: 171.916281\n",
      "Train Epoch: 229 [300000/697932 (43%)]\tLoss: 173.199672\n",
      "Train Epoch: 229 [400000/697932 (57%)]\tLoss: 171.235781\n",
      "Train Epoch: 229 [500000/697932 (72%)]\tLoss: 174.564766\n",
      "Train Epoch: 229 [600000/697932 (86%)]\tLoss: 174.842547\n",
      "====> Epoch: 229 Average loss: 172.9935\n",
      "====> Test set loss: 174.3337\n",
      "Train Epoch: 230 [0/697932 (0%)]\tLoss: 171.113188\n",
      "Train Epoch: 230 [100000/697932 (14%)]\tLoss: 171.571000\n",
      "Train Epoch: 230 [200000/697932 (29%)]\tLoss: 173.821047\n",
      "Train Epoch: 230 [300000/697932 (43%)]\tLoss: 173.212437\n",
      "Train Epoch: 230 [400000/697932 (57%)]\tLoss: 171.051187\n",
      "Train Epoch: 230 [500000/697932 (72%)]\tLoss: 172.238516\n",
      "Train Epoch: 230 [600000/697932 (86%)]\tLoss: 173.905016\n",
      "====> Epoch: 230 Average loss: 173.0864\n",
      "====> Test set loss: 174.9107\n",
      "Train Epoch: 231 [0/697932 (0%)]\tLoss: 173.744844\n",
      "Train Epoch: 231 [100000/697932 (14%)]\tLoss: 170.616703\n",
      "Train Epoch: 231 [200000/697932 (29%)]\tLoss: 173.144016\n",
      "Train Epoch: 231 [300000/697932 (43%)]\tLoss: 172.620375\n",
      "Train Epoch: 231 [400000/697932 (57%)]\tLoss: 173.796766\n",
      "Train Epoch: 231 [500000/697932 (72%)]\tLoss: 174.745922\n",
      "Train Epoch: 231 [600000/697932 (86%)]\tLoss: 176.106250\n",
      "====> Epoch: 231 Average loss: 173.0525\n",
      "====> Test set loss: 174.5730\n",
      "Train Epoch: 232 [0/697932 (0%)]\tLoss: 174.191922\n",
      "Train Epoch: 232 [100000/697932 (14%)]\tLoss: 171.217844\n",
      "Train Epoch: 232 [200000/697932 (29%)]\tLoss: 173.955031\n",
      "Train Epoch: 232 [300000/697932 (43%)]\tLoss: 172.818813\n",
      "Train Epoch: 232 [400000/697932 (57%)]\tLoss: 173.901828\n",
      "Train Epoch: 232 [500000/697932 (72%)]\tLoss: 172.957000\n",
      "Train Epoch: 232 [600000/697932 (86%)]\tLoss: 172.971656\n",
      "====> Epoch: 232 Average loss: 172.7322\n",
      "====> Test set loss: 174.1675\n",
      "Train Epoch: 233 [0/697932 (0%)]\tLoss: 172.269250\n",
      "Train Epoch: 233 [100000/697932 (14%)]\tLoss: 172.941984\n",
      "Train Epoch: 233 [200000/697932 (29%)]\tLoss: 170.220469\n",
      "Train Epoch: 233 [300000/697932 (43%)]\tLoss: 175.633156\n",
      "Train Epoch: 233 [400000/697932 (57%)]\tLoss: 170.241406\n",
      "Train Epoch: 233 [500000/697932 (72%)]\tLoss: 172.177672\n",
      "Train Epoch: 233 [600000/697932 (86%)]\tLoss: 171.606469\n",
      "====> Epoch: 233 Average loss: 172.6775\n",
      "====> Test set loss: 174.1668\n",
      "Train Epoch: 234 [0/697932 (0%)]\tLoss: 174.635828\n",
      "Train Epoch: 234 [100000/697932 (14%)]\tLoss: 172.122531\n",
      "Train Epoch: 234 [200000/697932 (29%)]\tLoss: 172.964328\n",
      "Train Epoch: 234 [300000/697932 (43%)]\tLoss: 174.769984\n",
      "Train Epoch: 234 [400000/697932 (57%)]\tLoss: 172.922766\n",
      "Train Epoch: 234 [500000/697932 (72%)]\tLoss: 173.062078\n",
      "Train Epoch: 234 [600000/697932 (86%)]\tLoss: 173.734109\n",
      "====> Epoch: 234 Average loss: 172.5904\n",
      "====> Test set loss: 174.4181\n",
      "Train Epoch: 235 [0/697932 (0%)]\tLoss: 171.624094\n",
      "Train Epoch: 235 [100000/697932 (14%)]\tLoss: 173.791422\n",
      "Train Epoch: 235 [200000/697932 (29%)]\tLoss: 171.794281\n",
      "Train Epoch: 235 [300000/697932 (43%)]\tLoss: 173.108922\n",
      "Train Epoch: 235 [400000/697932 (57%)]\tLoss: 172.510000\n",
      "Train Epoch: 235 [500000/697932 (72%)]\tLoss: 171.087297\n",
      "Train Epoch: 235 [600000/697932 (86%)]\tLoss: 170.997656\n",
      "====> Epoch: 235 Average loss: 172.6250\n",
      "====> Test set loss: 173.9884\n",
      "Train Epoch: 236 [0/697932 (0%)]\tLoss: 173.091312\n",
      "Train Epoch: 236 [100000/697932 (14%)]\tLoss: 175.628219\n",
      "Train Epoch: 236 [200000/697932 (29%)]\tLoss: 174.528484\n",
      "Train Epoch: 236 [300000/697932 (43%)]\tLoss: 172.834078\n",
      "Train Epoch: 236 [400000/697932 (57%)]\tLoss: 171.625531\n",
      "Train Epoch: 236 [500000/697932 (72%)]\tLoss: 174.228187\n",
      "Train Epoch: 236 [600000/697932 (86%)]\tLoss: 172.139500\n",
      "====> Epoch: 236 Average loss: 172.6578\n",
      "====> Test set loss: 174.3038\n",
      "Train Epoch: 237 [0/697932 (0%)]\tLoss: 172.762563\n",
      "Train Epoch: 237 [100000/697932 (14%)]\tLoss: 173.529359\n",
      "Train Epoch: 237 [200000/697932 (29%)]\tLoss: 171.928531\n",
      "Train Epoch: 237 [300000/697932 (43%)]\tLoss: 172.453469\n",
      "Train Epoch: 237 [400000/697932 (57%)]\tLoss: 172.603453\n",
      "Train Epoch: 237 [500000/697932 (72%)]\tLoss: 172.992891\n",
      "Train Epoch: 237 [600000/697932 (86%)]\tLoss: 172.402391\n",
      "====> Epoch: 237 Average loss: 172.7499\n",
      "====> Test set loss: 174.3502\n",
      "Train Epoch: 238 [0/697932 (0%)]\tLoss: 170.902312\n",
      "Train Epoch: 238 [100000/697932 (14%)]\tLoss: 171.411594\n",
      "Train Epoch: 238 [200000/697932 (29%)]\tLoss: 173.461531\n",
      "Train Epoch: 238 [300000/697932 (43%)]\tLoss: 172.919031\n",
      "Train Epoch: 238 [400000/697932 (57%)]\tLoss: 174.789859\n",
      "Train Epoch: 238 [500000/697932 (72%)]\tLoss: 175.550344\n",
      "Train Epoch: 238 [600000/697932 (86%)]\tLoss: 170.927156\n",
      "====> Epoch: 238 Average loss: 172.8480\n",
      "====> Test set loss: 174.2488\n",
      "Train Epoch: 239 [0/697932 (0%)]\tLoss: 170.246813\n",
      "Train Epoch: 239 [100000/697932 (14%)]\tLoss: 172.448172\n",
      "Train Epoch: 239 [200000/697932 (29%)]\tLoss: 173.788719\n",
      "Train Epoch: 239 [300000/697932 (43%)]\tLoss: 172.529234\n",
      "Train Epoch: 239 [400000/697932 (57%)]\tLoss: 170.785734\n",
      "Train Epoch: 239 [500000/697932 (72%)]\tLoss: 173.304781\n",
      "Train Epoch: 239 [600000/697932 (86%)]\tLoss: 171.670375\n",
      "====> Epoch: 239 Average loss: 172.9059\n",
      "====> Test set loss: 174.1864\n",
      "Train Epoch: 240 [0/697932 (0%)]\tLoss: 170.820703\n",
      "Train Epoch: 240 [100000/697932 (14%)]\tLoss: 173.203125\n",
      "Train Epoch: 240 [200000/697932 (29%)]\tLoss: 173.879094\n",
      "Train Epoch: 240 [300000/697932 (43%)]\tLoss: 177.006750\n",
      "Train Epoch: 240 [400000/697932 (57%)]\tLoss: 174.225844\n",
      "Train Epoch: 240 [500000/697932 (72%)]\tLoss: 169.933047\n",
      "Train Epoch: 240 [600000/697932 (86%)]\tLoss: 170.895094\n",
      "====> Epoch: 240 Average loss: 172.8359\n",
      "====> Test set loss: 174.6658\n",
      "Train Epoch: 241 [0/697932 (0%)]\tLoss: 173.760844\n",
      "Train Epoch: 241 [100000/697932 (14%)]\tLoss: 176.217734\n",
      "Train Epoch: 241 [200000/697932 (29%)]\tLoss: 176.075172\n",
      "Train Epoch: 241 [300000/697932 (43%)]\tLoss: 171.310516\n",
      "Train Epoch: 241 [400000/697932 (57%)]\tLoss: 172.602516\n",
      "Train Epoch: 241 [500000/697932 (72%)]\tLoss: 169.391672\n",
      "Train Epoch: 241 [600000/697932 (86%)]\tLoss: 171.574109\n",
      "====> Epoch: 241 Average loss: 172.7166\n",
      "====> Test set loss: 174.3085\n",
      "Train Epoch: 242 [0/697932 (0%)]\tLoss: 170.855797\n",
      "Train Epoch: 242 [100000/697932 (14%)]\tLoss: 173.370328\n",
      "Train Epoch: 242 [200000/697932 (29%)]\tLoss: 171.392609\n",
      "Train Epoch: 242 [300000/697932 (43%)]\tLoss: 171.132047\n",
      "Train Epoch: 242 [400000/697932 (57%)]\tLoss: 172.120812\n",
      "Train Epoch: 242 [500000/697932 (72%)]\tLoss: 172.689609\n",
      "Train Epoch: 242 [600000/697932 (86%)]\tLoss: 173.555719\n",
      "====> Epoch: 242 Average loss: 172.6852\n",
      "====> Test set loss: 174.2474\n",
      "Train Epoch: 243 [0/697932 (0%)]\tLoss: 172.411516\n",
      "Train Epoch: 243 [100000/697932 (14%)]\tLoss: 171.472312\n",
      "Train Epoch: 243 [200000/697932 (29%)]\tLoss: 174.884453\n",
      "Train Epoch: 243 [300000/697932 (43%)]\tLoss: 170.592672\n",
      "Train Epoch: 243 [400000/697932 (57%)]\tLoss: 170.151297\n",
      "Train Epoch: 243 [500000/697932 (72%)]\tLoss: 171.820609\n",
      "Train Epoch: 243 [600000/697932 (86%)]\tLoss: 173.921234\n",
      "====> Epoch: 243 Average loss: 172.6669\n",
      "====> Test set loss: 174.2236\n",
      "Train Epoch: 244 [0/697932 (0%)]\tLoss: 173.321797\n",
      "Train Epoch: 244 [100000/697932 (14%)]\tLoss: 172.603219\n",
      "Train Epoch: 244 [200000/697932 (29%)]\tLoss: 172.220125\n",
      "Train Epoch: 244 [300000/697932 (43%)]\tLoss: 178.339453\n",
      "Train Epoch: 244 [400000/697932 (57%)]\tLoss: 173.568984\n",
      "Train Epoch: 244 [500000/697932 (72%)]\tLoss: 173.292641\n",
      "Train Epoch: 244 [600000/697932 (86%)]\tLoss: 172.035625\n",
      "====> Epoch: 244 Average loss: 172.8079\n",
      "====> Test set loss: 174.2426\n",
      "Train Epoch: 245 [0/697932 (0%)]\tLoss: 173.203781\n",
      "Train Epoch: 245 [100000/697932 (14%)]\tLoss: 171.745094\n",
      "Train Epoch: 245 [200000/697932 (29%)]\tLoss: 174.219875\n",
      "Train Epoch: 245 [300000/697932 (43%)]\tLoss: 172.871953\n",
      "Train Epoch: 245 [400000/697932 (57%)]\tLoss: 171.017500\n",
      "Train Epoch: 245 [500000/697932 (72%)]\tLoss: 173.860016\n",
      "Train Epoch: 245 [600000/697932 (86%)]\tLoss: 170.314812\n",
      "====> Epoch: 245 Average loss: 172.6958\n",
      "====> Test set loss: 174.0333\n",
      "Train Epoch: 246 [0/697932 (0%)]\tLoss: 173.304000\n",
      "Train Epoch: 246 [100000/697932 (14%)]\tLoss: 174.068000\n",
      "Train Epoch: 246 [200000/697932 (29%)]\tLoss: 172.043578\n",
      "Train Epoch: 246 [300000/697932 (43%)]\tLoss: 174.249937\n",
      "Train Epoch: 246 [400000/697932 (57%)]\tLoss: 172.431484\n",
      "Train Epoch: 246 [500000/697932 (72%)]\tLoss: 175.737031\n",
      "Train Epoch: 246 [600000/697932 (86%)]\tLoss: 173.886562\n",
      "====> Epoch: 246 Average loss: 172.5310\n",
      "====> Test set loss: 174.0796\n",
      "Train Epoch: 247 [0/697932 (0%)]\tLoss: 169.866750\n",
      "Train Epoch: 247 [100000/697932 (14%)]\tLoss: 174.814312\n",
      "Train Epoch: 247 [200000/697932 (29%)]\tLoss: 172.655625\n",
      "Train Epoch: 247 [300000/697932 (43%)]\tLoss: 171.018500\n",
      "Train Epoch: 247 [400000/697932 (57%)]\tLoss: 170.227922\n",
      "Train Epoch: 247 [500000/697932 (72%)]\tLoss: 175.100141\n",
      "Train Epoch: 247 [600000/697932 (86%)]\tLoss: 174.377688\n",
      "====> Epoch: 247 Average loss: 172.8572\n",
      "====> Test set loss: 174.2039\n",
      "Train Epoch: 248 [0/697932 (0%)]\tLoss: 172.558531\n",
      "Train Epoch: 248 [100000/697932 (14%)]\tLoss: 173.337891\n",
      "Train Epoch: 248 [200000/697932 (29%)]\tLoss: 171.266547\n",
      "Train Epoch: 248 [300000/697932 (43%)]\tLoss: 169.702500\n",
      "Train Epoch: 248 [400000/697932 (57%)]\tLoss: 173.666906\n",
      "Train Epoch: 248 [500000/697932 (72%)]\tLoss: 171.189437\n",
      "Train Epoch: 248 [600000/697932 (86%)]\tLoss: 170.262641\n",
      "====> Epoch: 248 Average loss: 172.7999\n",
      "====> Test set loss: 174.2925\n",
      "Train Epoch: 249 [0/697932 (0%)]\tLoss: 174.151609\n",
      "Train Epoch: 249 [100000/697932 (14%)]\tLoss: 174.231625\n",
      "Train Epoch: 249 [200000/697932 (29%)]\tLoss: 173.586828\n",
      "Train Epoch: 249 [300000/697932 (43%)]\tLoss: 175.048266\n",
      "Train Epoch: 249 [400000/697932 (57%)]\tLoss: 173.617281\n",
      "Train Epoch: 249 [500000/697932 (72%)]\tLoss: 170.699328\n",
      "Train Epoch: 249 [600000/697932 (86%)]\tLoss: 173.375391\n",
      "====> Epoch: 249 Average loss: 172.6394\n",
      "====> Test set loss: 174.1192\n",
      "Train Epoch: 250 [0/697932 (0%)]\tLoss: 172.652938\n",
      "Train Epoch: 250 [100000/697932 (14%)]\tLoss: 172.489344\n",
      "Train Epoch: 250 [200000/697932 (29%)]\tLoss: 172.949094\n",
      "Train Epoch: 250 [300000/697932 (43%)]\tLoss: 172.972516\n",
      "Train Epoch: 250 [400000/697932 (57%)]\tLoss: 173.424781\n",
      "Train Epoch: 250 [500000/697932 (72%)]\tLoss: 174.306297\n",
      "Train Epoch: 250 [600000/697932 (86%)]\tLoss: 172.792844\n",
      "====> Epoch: 250 Average loss: 172.6709\n",
      "====> Test set loss: 174.2090\n",
      "Train Epoch: 251 [0/697932 (0%)]\tLoss: 171.532328\n",
      "Train Epoch: 251 [100000/697932 (14%)]\tLoss: 172.311062\n",
      "Train Epoch: 251 [200000/697932 (29%)]\tLoss: 174.277578\n",
      "Train Epoch: 251 [300000/697932 (43%)]\tLoss: 171.498109\n",
      "Train Epoch: 251 [400000/697932 (57%)]\tLoss: 171.765984\n",
      "Train Epoch: 251 [500000/697932 (72%)]\tLoss: 174.287313\n",
      "Train Epoch: 251 [600000/697932 (86%)]\tLoss: 174.395031\n",
      "====> Epoch: 251 Average loss: 172.6605\n",
      "====> Test set loss: 174.1058\n",
      "Train Epoch: 252 [0/697932 (0%)]\tLoss: 175.460391\n",
      "Train Epoch: 252 [100000/697932 (14%)]\tLoss: 171.325250\n",
      "Train Epoch: 252 [200000/697932 (29%)]\tLoss: 171.793469\n",
      "Train Epoch: 252 [300000/697932 (43%)]\tLoss: 173.469297\n",
      "Train Epoch: 252 [400000/697932 (57%)]\tLoss: 174.320328\n",
      "Train Epoch: 252 [500000/697932 (72%)]\tLoss: 171.365391\n",
      "Train Epoch: 252 [600000/697932 (86%)]\tLoss: 173.467875\n",
      "====> Epoch: 252 Average loss: 172.6942\n",
      "====> Test set loss: 174.2342\n",
      "Train Epoch: 253 [0/697932 (0%)]\tLoss: 172.575609\n",
      "Train Epoch: 253 [100000/697932 (14%)]\tLoss: 176.186656\n",
      "Train Epoch: 253 [200000/697932 (29%)]\tLoss: 171.823719\n",
      "Train Epoch: 253 [300000/697932 (43%)]\tLoss: 175.811062\n",
      "Train Epoch: 253 [400000/697932 (57%)]\tLoss: 172.474828\n",
      "Train Epoch: 253 [500000/697932 (72%)]\tLoss: 174.468500\n",
      "Train Epoch: 253 [600000/697932 (86%)]\tLoss: 172.510719\n",
      "====> Epoch: 253 Average loss: 172.5726\n",
      "====> Test set loss: 174.1903\n",
      "Train Epoch: 254 [0/697932 (0%)]\tLoss: 170.452000\n",
      "Train Epoch: 254 [100000/697932 (14%)]\tLoss: 170.403828\n",
      "Train Epoch: 254 [200000/697932 (29%)]\tLoss: 173.861609\n",
      "Train Epoch: 254 [300000/697932 (43%)]\tLoss: 169.914125\n",
      "Train Epoch: 254 [400000/697932 (57%)]\tLoss: 174.668281\n",
      "Train Epoch: 254 [500000/697932 (72%)]\tLoss: 172.552500\n",
      "Train Epoch: 254 [600000/697932 (86%)]\tLoss: 170.973469\n",
      "====> Epoch: 254 Average loss: 172.6347\n",
      "====> Test set loss: 174.4673\n",
      "Train Epoch: 255 [0/697932 (0%)]\tLoss: 172.906656\n",
      "Train Epoch: 255 [100000/697932 (14%)]\tLoss: 171.948688\n",
      "Train Epoch: 255 [200000/697932 (29%)]\tLoss: 172.264469\n",
      "Train Epoch: 255 [300000/697932 (43%)]\tLoss: 171.221062\n",
      "Train Epoch: 255 [400000/697932 (57%)]\tLoss: 171.139656\n",
      "Train Epoch: 255 [500000/697932 (72%)]\tLoss: 174.630172\n",
      "Train Epoch: 255 [600000/697932 (86%)]\tLoss: 169.667000\n",
      "====> Epoch: 255 Average loss: 172.4370\n",
      "====> Test set loss: 173.7827\n",
      "Train Epoch: 256 [0/697932 (0%)]\tLoss: 170.176203\n",
      "Train Epoch: 256 [100000/697932 (14%)]\tLoss: 173.198719\n",
      "Train Epoch: 256 [200000/697932 (29%)]\tLoss: 173.202578\n",
      "Train Epoch: 256 [300000/697932 (43%)]\tLoss: 173.434750\n",
      "Train Epoch: 256 [400000/697932 (57%)]\tLoss: 173.140781\n",
      "Train Epoch: 256 [500000/697932 (72%)]\tLoss: 171.382563\n",
      "Train Epoch: 256 [600000/697932 (86%)]\tLoss: 176.014563\n",
      "====> Epoch: 256 Average loss: 172.4585\n",
      "====> Test set loss: 173.9846\n",
      "Train Epoch: 257 [0/697932 (0%)]\tLoss: 169.292969\n",
      "Train Epoch: 257 [100000/697932 (14%)]\tLoss: 174.190969\n",
      "Train Epoch: 257 [200000/697932 (29%)]\tLoss: 173.778063\n",
      "Train Epoch: 257 [300000/697932 (43%)]\tLoss: 173.186875\n",
      "Train Epoch: 257 [400000/697932 (57%)]\tLoss: 172.447734\n",
      "Train Epoch: 257 [500000/697932 (72%)]\tLoss: 173.368141\n",
      "Train Epoch: 257 [600000/697932 (86%)]\tLoss: 170.927063\n",
      "====> Epoch: 257 Average loss: 172.5751\n",
      "====> Test set loss: 174.3957\n",
      "Train Epoch: 258 [0/697932 (0%)]\tLoss: 172.835922\n",
      "Train Epoch: 258 [100000/697932 (14%)]\tLoss: 175.371813\n",
      "Train Epoch: 258 [200000/697932 (29%)]\tLoss: 169.407187\n",
      "Train Epoch: 258 [300000/697932 (43%)]\tLoss: 174.115969\n",
      "Train Epoch: 258 [400000/697932 (57%)]\tLoss: 171.535516\n",
      "Train Epoch: 258 [500000/697932 (72%)]\tLoss: 171.837469\n",
      "Train Epoch: 258 [600000/697932 (86%)]\tLoss: 174.319828\n",
      "====> Epoch: 258 Average loss: 172.8360\n",
      "====> Test set loss: 174.7074\n",
      "Train Epoch: 259 [0/697932 (0%)]\tLoss: 173.041656\n",
      "Train Epoch: 259 [100000/697932 (14%)]\tLoss: 172.501891\n",
      "Train Epoch: 259 [200000/697932 (29%)]\tLoss: 174.004312\n",
      "Train Epoch: 259 [300000/697932 (43%)]\tLoss: 171.554922\n",
      "Train Epoch: 259 [400000/697932 (57%)]\tLoss: 172.575391\n",
      "Train Epoch: 259 [500000/697932 (72%)]\tLoss: 171.505641\n",
      "Train Epoch: 259 [600000/697932 (86%)]\tLoss: 169.547313\n",
      "====> Epoch: 259 Average loss: 172.8813\n",
      "====> Test set loss: 174.2359\n",
      "Train Epoch: 260 [0/697932 (0%)]\tLoss: 174.610688\n",
      "Train Epoch: 260 [100000/697932 (14%)]\tLoss: 170.985437\n",
      "Train Epoch: 260 [200000/697932 (29%)]\tLoss: 173.238250\n",
      "Train Epoch: 260 [300000/697932 (43%)]\tLoss: 176.785453\n",
      "Train Epoch: 260 [400000/697932 (57%)]\tLoss: 170.989344\n",
      "Train Epoch: 260 [500000/697932 (72%)]\tLoss: 173.513453\n",
      "Train Epoch: 260 [600000/697932 (86%)]\tLoss: 174.646437\n",
      "====> Epoch: 260 Average loss: 172.7095\n",
      "====> Test set loss: 174.4391\n",
      "Train Epoch: 261 [0/697932 (0%)]\tLoss: 172.969078\n",
      "Train Epoch: 261 [100000/697932 (14%)]\tLoss: 173.549797\n",
      "Train Epoch: 261 [200000/697932 (29%)]\tLoss: 172.050547\n",
      "Train Epoch: 261 [300000/697932 (43%)]\tLoss: 174.362875\n",
      "Train Epoch: 261 [400000/697932 (57%)]\tLoss: 172.479375\n",
      "Train Epoch: 261 [500000/697932 (72%)]\tLoss: 174.547641\n",
      "Train Epoch: 261 [600000/697932 (86%)]\tLoss: 174.110141\n",
      "====> Epoch: 261 Average loss: 172.8512\n",
      "====> Test set loss: 174.1664\n",
      "Train Epoch: 262 [0/697932 (0%)]\tLoss: 175.871531\n",
      "Train Epoch: 262 [100000/697932 (14%)]\tLoss: 172.328406\n",
      "Train Epoch: 262 [200000/697932 (29%)]\tLoss: 173.370609\n",
      "Train Epoch: 262 [300000/697932 (43%)]\tLoss: 174.029891\n",
      "Train Epoch: 262 [400000/697932 (57%)]\tLoss: 174.230484\n",
      "Train Epoch: 262 [500000/697932 (72%)]\tLoss: 170.903516\n",
      "Train Epoch: 262 [600000/697932 (86%)]\tLoss: 173.370797\n",
      "====> Epoch: 262 Average loss: 172.7612\n",
      "====> Test set loss: 174.1345\n",
      "Train Epoch: 263 [0/697932 (0%)]\tLoss: 169.942781\n",
      "Train Epoch: 263 [100000/697932 (14%)]\tLoss: 173.646484\n",
      "Train Epoch: 263 [200000/697932 (29%)]\tLoss: 170.629359\n",
      "Train Epoch: 263 [300000/697932 (43%)]\tLoss: 174.750969\n",
      "Train Epoch: 263 [400000/697932 (57%)]\tLoss: 174.313516\n",
      "Train Epoch: 263 [500000/697932 (72%)]\tLoss: 173.294906\n",
      "Train Epoch: 263 [600000/697932 (86%)]\tLoss: 172.495734\n",
      "====> Epoch: 263 Average loss: 172.6706\n",
      "====> Test set loss: 174.2498\n",
      "Train Epoch: 264 [0/697932 (0%)]\tLoss: 173.779344\n",
      "Train Epoch: 264 [100000/697932 (14%)]\tLoss: 170.619656\n",
      "Train Epoch: 264 [200000/697932 (29%)]\tLoss: 173.231969\n",
      "Train Epoch: 264 [300000/697932 (43%)]\tLoss: 173.126281\n",
      "Train Epoch: 264 [400000/697932 (57%)]\tLoss: 172.382062\n",
      "Train Epoch: 264 [500000/697932 (72%)]\tLoss: 174.015297\n",
      "Train Epoch: 264 [600000/697932 (86%)]\tLoss: 172.750844\n",
      "====> Epoch: 264 Average loss: 172.5719\n",
      "====> Test set loss: 173.9378\n",
      "Train Epoch: 265 [0/697932 (0%)]\tLoss: 169.482266\n",
      "Train Epoch: 265 [100000/697932 (14%)]\tLoss: 173.025484\n",
      "Train Epoch: 265 [200000/697932 (29%)]\tLoss: 169.875609\n",
      "Train Epoch: 265 [300000/697932 (43%)]\tLoss: 170.161266\n",
      "Train Epoch: 265 [400000/697932 (57%)]\tLoss: 173.127781\n",
      "Train Epoch: 265 [500000/697932 (72%)]\tLoss: 174.968828\n",
      "Train Epoch: 265 [600000/697932 (86%)]\tLoss: 172.783078\n",
      "====> Epoch: 265 Average loss: 172.4448\n",
      "====> Test set loss: 174.0966\n",
      "Train Epoch: 266 [0/697932 (0%)]\tLoss: 173.121875\n",
      "Train Epoch: 266 [100000/697932 (14%)]\tLoss: 174.057859\n",
      "Train Epoch: 266 [200000/697932 (29%)]\tLoss: 172.042797\n",
      "Train Epoch: 266 [300000/697932 (43%)]\tLoss: 173.491641\n",
      "Train Epoch: 266 [400000/697932 (57%)]\tLoss: 175.047047\n",
      "Train Epoch: 266 [500000/697932 (72%)]\tLoss: 171.871141\n",
      "Train Epoch: 266 [600000/697932 (86%)]\tLoss: 174.743172\n",
      "====> Epoch: 266 Average loss: 172.4810\n",
      "====> Test set loss: 173.8838\n",
      "Train Epoch: 267 [0/697932 (0%)]\tLoss: 173.168203\n",
      "Train Epoch: 267 [100000/697932 (14%)]\tLoss: 170.471641\n",
      "Train Epoch: 267 [200000/697932 (29%)]\tLoss: 171.267844\n",
      "Train Epoch: 267 [300000/697932 (43%)]\tLoss: 172.727547\n",
      "Train Epoch: 267 [400000/697932 (57%)]\tLoss: 173.516906\n",
      "Train Epoch: 267 [500000/697932 (72%)]\tLoss: 169.666656\n",
      "Train Epoch: 267 [600000/697932 (86%)]\tLoss: 170.994906\n",
      "====> Epoch: 267 Average loss: 172.3001\n",
      "====> Test set loss: 173.8590\n",
      "Train Epoch: 268 [0/697932 (0%)]\tLoss: 170.359828\n",
      "Train Epoch: 268 [100000/697932 (14%)]\tLoss: 174.691891\n",
      "Train Epoch: 268 [200000/697932 (29%)]\tLoss: 172.054000\n",
      "Train Epoch: 268 [300000/697932 (43%)]\tLoss: 171.857812\n",
      "Train Epoch: 268 [400000/697932 (57%)]\tLoss: 169.711516\n",
      "Train Epoch: 268 [500000/697932 (72%)]\tLoss: 171.138563\n",
      "Train Epoch: 268 [600000/697932 (86%)]\tLoss: 174.443016\n",
      "====> Epoch: 268 Average loss: 172.4110\n",
      "====> Test set loss: 174.0915\n",
      "Train Epoch: 269 [0/697932 (0%)]\tLoss: 171.072375\n",
      "Train Epoch: 269 [100000/697932 (14%)]\tLoss: 171.085062\n",
      "Train Epoch: 269 [200000/697932 (29%)]\tLoss: 170.230484\n",
      "Train Epoch: 269 [300000/697932 (43%)]\tLoss: 174.875063\n",
      "Train Epoch: 269 [400000/697932 (57%)]\tLoss: 171.601703\n",
      "Train Epoch: 269 [500000/697932 (72%)]\tLoss: 171.044547\n",
      "Train Epoch: 269 [600000/697932 (86%)]\tLoss: 169.705094\n",
      "====> Epoch: 269 Average loss: 172.4342\n",
      "====> Test set loss: 174.2233\n",
      "Train Epoch: 270 [0/697932 (0%)]\tLoss: 174.870172\n",
      "Train Epoch: 270 [100000/697932 (14%)]\tLoss: 172.370000\n",
      "Train Epoch: 270 [200000/697932 (29%)]\tLoss: 174.098062\n",
      "Train Epoch: 270 [300000/697932 (43%)]\tLoss: 170.054391\n",
      "Train Epoch: 270 [400000/697932 (57%)]\tLoss: 169.869109\n",
      "Train Epoch: 270 [500000/697932 (72%)]\tLoss: 172.620484\n",
      "Train Epoch: 270 [600000/697932 (86%)]\tLoss: 170.319078\n",
      "====> Epoch: 270 Average loss: 172.5271\n",
      "====> Test set loss: 173.9776\n",
      "Train Epoch: 271 [0/697932 (0%)]\tLoss: 172.919453\n",
      "Train Epoch: 271 [100000/697932 (14%)]\tLoss: 171.479469\n",
      "Train Epoch: 271 [200000/697932 (29%)]\tLoss: 172.345375\n",
      "Train Epoch: 271 [300000/697932 (43%)]\tLoss: 172.515922\n",
      "Train Epoch: 271 [400000/697932 (57%)]\tLoss: 170.935609\n",
      "Train Epoch: 271 [500000/697932 (72%)]\tLoss: 171.572891\n",
      "Train Epoch: 271 [600000/697932 (86%)]\tLoss: 172.122203\n",
      "====> Epoch: 271 Average loss: 172.5254\n",
      "====> Test set loss: 174.1271\n",
      "Train Epoch: 272 [0/697932 (0%)]\tLoss: 173.494875\n",
      "Train Epoch: 272 [100000/697932 (14%)]\tLoss: 172.366547\n",
      "Train Epoch: 272 [200000/697932 (29%)]\tLoss: 173.428812\n",
      "Train Epoch: 272 [300000/697932 (43%)]\tLoss: 172.413750\n",
      "Train Epoch: 272 [400000/697932 (57%)]\tLoss: 171.999250\n",
      "Train Epoch: 272 [500000/697932 (72%)]\tLoss: 176.688813\n",
      "Train Epoch: 272 [600000/697932 (86%)]\tLoss: 171.855422\n",
      "====> Epoch: 272 Average loss: 172.6100\n",
      "====> Test set loss: 174.3966\n",
      "Train Epoch: 273 [0/697932 (0%)]\tLoss: 172.261516\n",
      "Train Epoch: 273 [100000/697932 (14%)]\tLoss: 173.882469\n",
      "Train Epoch: 273 [200000/697932 (29%)]\tLoss: 171.259234\n",
      "Train Epoch: 273 [300000/697932 (43%)]\tLoss: 173.281281\n",
      "Train Epoch: 273 [400000/697932 (57%)]\tLoss: 171.144828\n",
      "Train Epoch: 273 [500000/697932 (72%)]\tLoss: 171.510781\n",
      "Train Epoch: 273 [600000/697932 (86%)]\tLoss: 168.440500\n",
      "====> Epoch: 273 Average loss: 172.5488\n",
      "====> Test set loss: 174.0156\n",
      "Train Epoch: 274 [0/697932 (0%)]\tLoss: 173.368828\n",
      "Train Epoch: 274 [100000/697932 (14%)]\tLoss: 172.559500\n",
      "Train Epoch: 274 [200000/697932 (29%)]\tLoss: 174.743250\n",
      "Train Epoch: 274 [300000/697932 (43%)]\tLoss: 171.734906\n",
      "Train Epoch: 274 [400000/697932 (57%)]\tLoss: 173.508312\n",
      "Train Epoch: 274 [500000/697932 (72%)]\tLoss: 171.036578\n",
      "Train Epoch: 274 [600000/697932 (86%)]\tLoss: 172.302922\n",
      "====> Epoch: 274 Average loss: 172.4834\n",
      "====> Test set loss: 174.0992\n",
      "Train Epoch: 275 [0/697932 (0%)]\tLoss: 173.222891\n",
      "Train Epoch: 275 [100000/697932 (14%)]\tLoss: 174.638281\n",
      "Train Epoch: 275 [200000/697932 (29%)]\tLoss: 172.130234\n",
      "Train Epoch: 275 [300000/697932 (43%)]\tLoss: 176.601547\n",
      "Train Epoch: 275 [400000/697932 (57%)]\tLoss: 174.064672\n",
      "Train Epoch: 275 [500000/697932 (72%)]\tLoss: 175.785063\n",
      "Train Epoch: 275 [600000/697932 (86%)]\tLoss: 173.321906\n",
      "====> Epoch: 275 Average loss: 172.5584\n",
      "====> Test set loss: 174.1992\n",
      "Train Epoch: 276 [0/697932 (0%)]\tLoss: 172.784750\n",
      "Train Epoch: 276 [100000/697932 (14%)]\tLoss: 170.416969\n",
      "Train Epoch: 276 [200000/697932 (29%)]\tLoss: 171.032828\n",
      "Train Epoch: 276 [300000/697932 (43%)]\tLoss: 173.265000\n",
      "Train Epoch: 276 [400000/697932 (57%)]\tLoss: 170.983281\n",
      "Train Epoch: 276 [500000/697932 (72%)]\tLoss: 172.505578\n",
      "Train Epoch: 276 [600000/697932 (86%)]\tLoss: 171.174937\n",
      "====> Epoch: 276 Average loss: 172.5074\n",
      "====> Test set loss: 174.1124\n",
      "Train Epoch: 277 [0/697932 (0%)]\tLoss: 172.940031\n",
      "Train Epoch: 277 [100000/697932 (14%)]\tLoss: 172.916844\n",
      "Train Epoch: 277 [200000/697932 (29%)]\tLoss: 173.053406\n",
      "Train Epoch: 277 [300000/697932 (43%)]\tLoss: 172.787453\n",
      "Train Epoch: 277 [400000/697932 (57%)]\tLoss: 171.521594\n",
      "Train Epoch: 277 [500000/697932 (72%)]\tLoss: 171.265484\n",
      "Train Epoch: 277 [600000/697932 (86%)]\tLoss: 175.537969\n",
      "====> Epoch: 277 Average loss: 172.5402\n",
      "====> Test set loss: 174.3026\n",
      "Train Epoch: 278 [0/697932 (0%)]\tLoss: 172.120125\n",
      "Train Epoch: 278 [100000/697932 (14%)]\tLoss: 175.755500\n",
      "Train Epoch: 278 [200000/697932 (29%)]\tLoss: 176.017359\n",
      "Train Epoch: 278 [300000/697932 (43%)]\tLoss: 170.941766\n",
      "Train Epoch: 278 [400000/697932 (57%)]\tLoss: 171.954312\n",
      "Train Epoch: 278 [500000/697932 (72%)]\tLoss: 175.877484\n",
      "Train Epoch: 278 [600000/697932 (86%)]\tLoss: 171.117328\n",
      "====> Epoch: 278 Average loss: 172.5844\n",
      "====> Test set loss: 174.1190\n",
      "Train Epoch: 279 [0/697932 (0%)]\tLoss: 172.668734\n",
      "Train Epoch: 279 [100000/697932 (14%)]\tLoss: 173.251656\n",
      "Train Epoch: 279 [200000/697932 (29%)]\tLoss: 171.202156\n",
      "Train Epoch: 279 [300000/697932 (43%)]\tLoss: 173.430813\n",
      "Train Epoch: 279 [400000/697932 (57%)]\tLoss: 168.921875\n",
      "Train Epoch: 279 [500000/697932 (72%)]\tLoss: 172.831484\n",
      "Train Epoch: 279 [600000/697932 (86%)]\tLoss: 174.431781\n",
      "====> Epoch: 279 Average loss: 172.6443\n",
      "====> Test set loss: 174.2815\n",
      "Train Epoch: 280 [0/697932 (0%)]\tLoss: 172.496281\n",
      "Train Epoch: 280 [100000/697932 (14%)]\tLoss: 170.216906\n",
      "Train Epoch: 280 [200000/697932 (29%)]\tLoss: 170.857313\n",
      "Train Epoch: 280 [300000/697932 (43%)]\tLoss: 176.972984\n",
      "Train Epoch: 280 [400000/697932 (57%)]\tLoss: 174.619156\n",
      "Train Epoch: 280 [500000/697932 (72%)]\tLoss: 169.864922\n",
      "Train Epoch: 280 [600000/697932 (86%)]\tLoss: 173.049391\n",
      "====> Epoch: 280 Average loss: 172.4789\n",
      "====> Test set loss: 174.4586\n",
      "Train Epoch: 281 [0/697932 (0%)]\tLoss: 171.781531\n",
      "Train Epoch: 281 [100000/697932 (14%)]\tLoss: 172.069344\n",
      "Train Epoch: 281 [200000/697932 (29%)]\tLoss: 172.527516\n",
      "Train Epoch: 281 [300000/697932 (43%)]\tLoss: 171.598969\n",
      "Train Epoch: 281 [400000/697932 (57%)]\tLoss: 170.677719\n",
      "Train Epoch: 281 [500000/697932 (72%)]\tLoss: 171.570812\n",
      "Train Epoch: 281 [600000/697932 (86%)]\tLoss: 172.831656\n",
      "====> Epoch: 281 Average loss: 172.5218\n",
      "====> Test set loss: 174.4650\n",
      "Train Epoch: 282 [0/697932 (0%)]\tLoss: 172.728891\n",
      "Train Epoch: 282 [100000/697932 (14%)]\tLoss: 171.293375\n",
      "Train Epoch: 282 [200000/697932 (29%)]\tLoss: 172.021328\n",
      "Train Epoch: 282 [300000/697932 (43%)]\tLoss: 170.215750\n",
      "Train Epoch: 282 [400000/697932 (57%)]\tLoss: 168.700938\n",
      "Train Epoch: 282 [500000/697932 (72%)]\tLoss: 172.460156\n",
      "Train Epoch: 282 [600000/697932 (86%)]\tLoss: 172.158391\n",
      "====> Epoch: 282 Average loss: 172.8113\n",
      "====> Test set loss: 174.2345\n",
      "Train Epoch: 283 [0/697932 (0%)]\tLoss: 169.853187\n",
      "Train Epoch: 283 [100000/697932 (14%)]\tLoss: 171.461578\n",
      "Train Epoch: 283 [200000/697932 (29%)]\tLoss: 170.492031\n",
      "Train Epoch: 283 [300000/697932 (43%)]\tLoss: 172.316328\n",
      "Train Epoch: 283 [400000/697932 (57%)]\tLoss: 171.720266\n",
      "Train Epoch: 283 [500000/697932 (72%)]\tLoss: 174.100141\n",
      "Train Epoch: 283 [600000/697932 (86%)]\tLoss: 173.409156\n",
      "====> Epoch: 283 Average loss: 172.5931\n",
      "====> Test set loss: 174.2891\n",
      "Train Epoch: 284 [0/697932 (0%)]\tLoss: 172.449797\n",
      "Train Epoch: 284 [100000/697932 (14%)]\tLoss: 170.544437\n",
      "Train Epoch: 284 [200000/697932 (29%)]\tLoss: 167.298578\n",
      "Train Epoch: 284 [300000/697932 (43%)]\tLoss: 173.867531\n",
      "Train Epoch: 284 [400000/697932 (57%)]\tLoss: 170.685016\n",
      "Train Epoch: 284 [500000/697932 (72%)]\tLoss: 172.008297\n",
      "Train Epoch: 284 [600000/697932 (86%)]\tLoss: 171.138500\n",
      "====> Epoch: 284 Average loss: 172.4274\n",
      "====> Test set loss: 173.8385\n",
      "Train Epoch: 285 [0/697932 (0%)]\tLoss: 176.245125\n",
      "Train Epoch: 285 [100000/697932 (14%)]\tLoss: 173.090750\n",
      "Train Epoch: 285 [200000/697932 (29%)]\tLoss: 174.075703\n",
      "Train Epoch: 285 [300000/697932 (43%)]\tLoss: 177.696531\n",
      "Train Epoch: 285 [400000/697932 (57%)]\tLoss: 173.617172\n",
      "Train Epoch: 285 [500000/697932 (72%)]\tLoss: 169.901297\n",
      "Train Epoch: 285 [600000/697932 (86%)]\tLoss: 174.875531\n",
      "====> Epoch: 285 Average loss: 172.5481\n",
      "====> Test set loss: 174.5755\n",
      "Train Epoch: 286 [0/697932 (0%)]\tLoss: 172.681984\n",
      "Train Epoch: 286 [100000/697932 (14%)]\tLoss: 174.042078\n",
      "Train Epoch: 286 [200000/697932 (29%)]\tLoss: 174.723750\n",
      "Train Epoch: 286 [300000/697932 (43%)]\tLoss: 173.985828\n",
      "Train Epoch: 286 [400000/697932 (57%)]\tLoss: 173.276203\n",
      "Train Epoch: 286 [500000/697932 (72%)]\tLoss: 174.714359\n",
      "Train Epoch: 286 [600000/697932 (86%)]\tLoss: 174.331375\n",
      "====> Epoch: 286 Average loss: 173.4123\n",
      "====> Test set loss: 174.3947\n",
      "Train Epoch: 287 [0/697932 (0%)]\tLoss: 172.397312\n",
      "Train Epoch: 287 [100000/697932 (14%)]\tLoss: 171.225219\n",
      "Train Epoch: 287 [200000/697932 (29%)]\tLoss: 170.035922\n",
      "Train Epoch: 287 [300000/697932 (43%)]\tLoss: 174.245844\n",
      "Train Epoch: 287 [400000/697932 (57%)]\tLoss: 175.857484\n",
      "Train Epoch: 287 [500000/697932 (72%)]\tLoss: 171.053063\n",
      "Train Epoch: 287 [600000/697932 (86%)]\tLoss: 174.494812\n",
      "====> Epoch: 287 Average loss: 172.9770\n",
      "====> Test set loss: 174.6512\n",
      "Train Epoch: 288 [0/697932 (0%)]\tLoss: 173.284094\n",
      "Train Epoch: 288 [100000/697932 (14%)]\tLoss: 174.348109\n",
      "Train Epoch: 288 [200000/697932 (29%)]\tLoss: 174.203375\n",
      "Train Epoch: 288 [300000/697932 (43%)]\tLoss: 172.341703\n",
      "Train Epoch: 288 [400000/697932 (57%)]\tLoss: 172.740578\n",
      "Train Epoch: 288 [500000/697932 (72%)]\tLoss: 168.303953\n",
      "Train Epoch: 288 [600000/697932 (86%)]\tLoss: 173.333000\n",
      "====> Epoch: 288 Average loss: 173.0412\n",
      "====> Test set loss: 174.2933\n",
      "Train Epoch: 289 [0/697932 (0%)]\tLoss: 172.405687\n",
      "Train Epoch: 289 [100000/697932 (14%)]\tLoss: 176.403984\n",
      "Train Epoch: 289 [200000/697932 (29%)]\tLoss: 173.369875\n",
      "Train Epoch: 289 [300000/697932 (43%)]\tLoss: 172.876313\n",
      "Train Epoch: 289 [400000/697932 (57%)]\tLoss: 175.063406\n",
      "Train Epoch: 289 [500000/697932 (72%)]\tLoss: 171.503719\n",
      "Train Epoch: 289 [600000/697932 (86%)]\tLoss: 170.406969\n",
      "====> Epoch: 289 Average loss: 172.7625\n",
      "====> Test set loss: 174.2099\n",
      "Train Epoch: 290 [0/697932 (0%)]\tLoss: 173.447672\n",
      "Train Epoch: 290 [100000/697932 (14%)]\tLoss: 172.618734\n",
      "Train Epoch: 290 [200000/697932 (29%)]\tLoss: 174.765656\n",
      "Train Epoch: 290 [300000/697932 (43%)]\tLoss: 169.910984\n",
      "Train Epoch: 290 [400000/697932 (57%)]\tLoss: 172.600609\n",
      "Train Epoch: 290 [500000/697932 (72%)]\tLoss: 170.945812\n",
      "Train Epoch: 290 [600000/697932 (86%)]\tLoss: 173.686062\n",
      "====> Epoch: 290 Average loss: 172.6151\n",
      "====> Test set loss: 174.1034\n",
      "Train Epoch: 291 [0/697932 (0%)]\tLoss: 174.263109\n",
      "Train Epoch: 291 [100000/697932 (14%)]\tLoss: 174.366375\n",
      "Train Epoch: 291 [200000/697932 (29%)]\tLoss: 173.634844\n",
      "Train Epoch: 291 [300000/697932 (43%)]\tLoss: 171.435797\n",
      "Train Epoch: 291 [400000/697932 (57%)]\tLoss: 172.055641\n",
      "Train Epoch: 291 [500000/697932 (72%)]\tLoss: 170.721219\n",
      "Train Epoch: 291 [600000/697932 (86%)]\tLoss: 171.858781\n",
      "====> Epoch: 291 Average loss: 172.5850\n",
      "====> Test set loss: 174.6318\n",
      "Train Epoch: 292 [0/697932 (0%)]\tLoss: 176.912281\n",
      "Train Epoch: 292 [100000/697932 (14%)]\tLoss: 170.159406\n",
      "Train Epoch: 292 [200000/697932 (29%)]\tLoss: 174.239172\n",
      "Train Epoch: 292 [300000/697932 (43%)]\tLoss: 170.836625\n",
      "Train Epoch: 292 [400000/697932 (57%)]\tLoss: 173.019797\n",
      "Train Epoch: 292 [500000/697932 (72%)]\tLoss: 171.951828\n",
      "Train Epoch: 292 [600000/697932 (86%)]\tLoss: 172.941469\n",
      "====> Epoch: 292 Average loss: 172.4858\n",
      "====> Test set loss: 173.9276\n",
      "Train Epoch: 293 [0/697932 (0%)]\tLoss: 174.479469\n",
      "Train Epoch: 293 [100000/697932 (14%)]\tLoss: 170.616328\n",
      "Train Epoch: 293 [200000/697932 (29%)]\tLoss: 172.119156\n",
      "Train Epoch: 293 [300000/697932 (43%)]\tLoss: 173.055516\n",
      "Train Epoch: 293 [400000/697932 (57%)]\tLoss: 170.072125\n",
      "Train Epoch: 293 [500000/697932 (72%)]\tLoss: 174.569312\n",
      "Train Epoch: 293 [600000/697932 (86%)]\tLoss: 172.440984\n",
      "====> Epoch: 293 Average loss: 172.4890\n",
      "====> Test set loss: 174.2474\n",
      "Train Epoch: 294 [0/697932 (0%)]\tLoss: 172.980281\n",
      "Train Epoch: 294 [100000/697932 (14%)]\tLoss: 174.035687\n",
      "Train Epoch: 294 [200000/697932 (29%)]\tLoss: 176.475437\n",
      "Train Epoch: 294 [300000/697932 (43%)]\tLoss: 176.008516\n",
      "Train Epoch: 294 [400000/697932 (57%)]\tLoss: 174.617078\n",
      "Train Epoch: 294 [500000/697932 (72%)]\tLoss: 173.827188\n",
      "Train Epoch: 294 [600000/697932 (86%)]\tLoss: 173.921250\n",
      "====> Epoch: 294 Average loss: 172.5377\n",
      "====> Test set loss: 174.1539\n",
      "Train Epoch: 295 [0/697932 (0%)]\tLoss: 171.641297\n",
      "Train Epoch: 295 [100000/697932 (14%)]\tLoss: 171.371031\n",
      "Train Epoch: 295 [200000/697932 (29%)]\tLoss: 172.651375\n",
      "Train Epoch: 295 [300000/697932 (43%)]\tLoss: 173.780141\n",
      "Train Epoch: 295 [400000/697932 (57%)]\tLoss: 170.729922\n",
      "Train Epoch: 295 [500000/697932 (72%)]\tLoss: 172.626625\n",
      "Train Epoch: 295 [600000/697932 (86%)]\tLoss: 173.900594\n",
      "====> Epoch: 295 Average loss: 172.4461\n",
      "====> Test set loss: 173.7657\n",
      "Train Epoch: 296 [0/697932 (0%)]\tLoss: 172.516703\n",
      "Train Epoch: 296 [100000/697932 (14%)]\tLoss: 169.690422\n",
      "Train Epoch: 296 [200000/697932 (29%)]\tLoss: 172.186937\n",
      "Train Epoch: 296 [300000/697932 (43%)]\tLoss: 171.847469\n",
      "Train Epoch: 296 [400000/697932 (57%)]\tLoss: 171.051828\n",
      "Train Epoch: 296 [500000/697932 (72%)]\tLoss: 173.592781\n",
      "Train Epoch: 296 [600000/697932 (86%)]\tLoss: 170.513688\n",
      "====> Epoch: 296 Average loss: 172.2549\n",
      "====> Test set loss: 173.8438\n",
      "Train Epoch: 297 [0/697932 (0%)]\tLoss: 171.324891\n",
      "Train Epoch: 297 [100000/697932 (14%)]\tLoss: 169.440672\n",
      "Train Epoch: 297 [200000/697932 (29%)]\tLoss: 174.057094\n",
      "Train Epoch: 297 [300000/697932 (43%)]\tLoss: 170.185484\n",
      "Train Epoch: 297 [400000/697932 (57%)]\tLoss: 169.456516\n",
      "Train Epoch: 297 [500000/697932 (72%)]\tLoss: 174.005375\n",
      "Train Epoch: 297 [600000/697932 (86%)]\tLoss: 172.289453\n",
      "====> Epoch: 297 Average loss: 172.2709\n",
      "====> Test set loss: 173.6508\n",
      "Train Epoch: 298 [0/697932 (0%)]\tLoss: 168.578500\n",
      "Train Epoch: 298 [100000/697932 (14%)]\tLoss: 175.443062\n",
      "Train Epoch: 298 [200000/697932 (29%)]\tLoss: 175.512109\n",
      "Train Epoch: 298 [300000/697932 (43%)]\tLoss: 171.678797\n",
      "Train Epoch: 298 [400000/697932 (57%)]\tLoss: 174.158344\n",
      "Train Epoch: 298 [500000/697932 (72%)]\tLoss: 171.949859\n",
      "Train Epoch: 298 [600000/697932 (86%)]\tLoss: 172.199469\n",
      "====> Epoch: 298 Average loss: 172.2218\n",
      "====> Test set loss: 174.0252\n",
      "Train Epoch: 299 [0/697932 (0%)]\tLoss: 171.473625\n",
      "Train Epoch: 299 [100000/697932 (14%)]\tLoss: 172.545469\n",
      "Train Epoch: 299 [200000/697932 (29%)]\tLoss: 171.315984\n",
      "Train Epoch: 299 [300000/697932 (43%)]\tLoss: 171.930969\n",
      "Train Epoch: 299 [400000/697932 (57%)]\tLoss: 169.983406\n",
      "Train Epoch: 299 [500000/697932 (72%)]\tLoss: 170.499188\n",
      "Train Epoch: 299 [600000/697932 (86%)]\tLoss: 173.052812\n",
      "====> Epoch: 299 Average loss: 172.3306\n",
      "====> Test set loss: 173.8975\n",
      "Train Epoch: 300 [0/697932 (0%)]\tLoss: 172.294609\n",
      "Train Epoch: 300 [100000/697932 (14%)]\tLoss: 173.797594\n",
      "Train Epoch: 300 [200000/697932 (29%)]\tLoss: 171.598500\n",
      "Train Epoch: 300 [300000/697932 (43%)]\tLoss: 176.949984\n",
      "Train Epoch: 300 [400000/697932 (57%)]\tLoss: 172.054484\n",
      "Train Epoch: 300 [500000/697932 (72%)]\tLoss: 172.349953\n",
      "Train Epoch: 300 [600000/697932 (86%)]\tLoss: 172.857547\n",
      "====> Epoch: 300 Average loss: 172.4736\n",
      "====> Test set loss: 174.1201\n",
      "Train Epoch: 301 [0/697932 (0%)]\tLoss: 172.335906\n",
      "Train Epoch: 301 [100000/697932 (14%)]\tLoss: 173.908141\n",
      "Train Epoch: 301 [200000/697932 (29%)]\tLoss: 168.719500\n",
      "Train Epoch: 301 [300000/697932 (43%)]\tLoss: 175.717641\n",
      "Train Epoch: 301 [400000/697932 (57%)]\tLoss: 172.051094\n",
      "Train Epoch: 301 [500000/697932 (72%)]\tLoss: 174.344937\n",
      "Train Epoch: 301 [600000/697932 (86%)]\tLoss: 168.520016\n",
      "====> Epoch: 301 Average loss: 172.5063\n",
      "====> Test set loss: 174.1217\n",
      "Train Epoch: 302 [0/697932 (0%)]\tLoss: 175.043875\n",
      "Train Epoch: 302 [100000/697932 (14%)]\tLoss: 172.827797\n",
      "Train Epoch: 302 [200000/697932 (29%)]\tLoss: 174.384891\n",
      "Train Epoch: 302 [300000/697932 (43%)]\tLoss: 170.470625\n",
      "Train Epoch: 302 [400000/697932 (57%)]\tLoss: 170.826422\n",
      "Train Epoch: 302 [500000/697932 (72%)]\tLoss: 171.879531\n",
      "Train Epoch: 302 [600000/697932 (86%)]\tLoss: 171.618031\n",
      "====> Epoch: 302 Average loss: 172.4639\n",
      "====> Test set loss: 174.2687\n",
      "Train Epoch: 303 [0/697932 (0%)]\tLoss: 172.689328\n",
      "Train Epoch: 303 [100000/697932 (14%)]\tLoss: 172.310391\n",
      "Train Epoch: 303 [200000/697932 (29%)]\tLoss: 171.952391\n",
      "Train Epoch: 303 [300000/697932 (43%)]\tLoss: 174.031438\n",
      "Train Epoch: 303 [400000/697932 (57%)]\tLoss: 173.700234\n",
      "Train Epoch: 303 [500000/697932 (72%)]\tLoss: 174.435766\n",
      "Train Epoch: 303 [600000/697932 (86%)]\tLoss: 172.938312\n",
      "====> Epoch: 303 Average loss: 172.6093\n",
      "====> Test set loss: 174.1705\n",
      "Train Epoch: 304 [0/697932 (0%)]\tLoss: 173.643141\n",
      "Train Epoch: 304 [100000/697932 (14%)]\tLoss: 173.575437\n",
      "Train Epoch: 304 [200000/697932 (29%)]\tLoss: 172.890062\n",
      "Train Epoch: 304 [300000/697932 (43%)]\tLoss: 172.422531\n",
      "Train Epoch: 304 [400000/697932 (57%)]\tLoss: 171.636063\n",
      "Train Epoch: 304 [500000/697932 (72%)]\tLoss: 170.372062\n",
      "Train Epoch: 304 [600000/697932 (86%)]\tLoss: 174.121391\n",
      "====> Epoch: 304 Average loss: 172.7650\n",
      "====> Test set loss: 174.4348\n",
      "Train Epoch: 305 [0/697932 (0%)]\tLoss: 170.946188\n",
      "Train Epoch: 305 [100000/697932 (14%)]\tLoss: 170.950094\n",
      "Train Epoch: 305 [200000/697932 (29%)]\tLoss: 173.322656\n",
      "Train Epoch: 305 [300000/697932 (43%)]\tLoss: 169.411844\n",
      "Train Epoch: 305 [400000/697932 (57%)]\tLoss: 172.978781\n",
      "Train Epoch: 305 [500000/697932 (72%)]\tLoss: 174.428516\n",
      "Train Epoch: 305 [600000/697932 (86%)]\tLoss: 173.806484\n",
      "====> Epoch: 305 Average loss: 172.7107\n",
      "====> Test set loss: 174.5147\n",
      "Train Epoch: 306 [0/697932 (0%)]\tLoss: 169.211156\n",
      "Train Epoch: 306 [100000/697932 (14%)]\tLoss: 171.569203\n",
      "Train Epoch: 306 [200000/697932 (29%)]\tLoss: 170.632828\n",
      "Train Epoch: 306 [300000/697932 (43%)]\tLoss: 175.378906\n",
      "Train Epoch: 306 [400000/697932 (57%)]\tLoss: 172.026844\n",
      "Train Epoch: 306 [500000/697932 (72%)]\tLoss: 175.190656\n",
      "Train Epoch: 306 [600000/697932 (86%)]\tLoss: 171.413922\n",
      "====> Epoch: 306 Average loss: 172.6319\n",
      "====> Test set loss: 174.0966\n",
      "Train Epoch: 307 [0/697932 (0%)]\tLoss: 172.765766\n",
      "Train Epoch: 307 [100000/697932 (14%)]\tLoss: 173.652359\n",
      "Train Epoch: 307 [200000/697932 (29%)]\tLoss: 170.787797\n",
      "Train Epoch: 307 [300000/697932 (43%)]\tLoss: 172.547531\n",
      "Train Epoch: 307 [400000/697932 (57%)]\tLoss: 173.792438\n",
      "Train Epoch: 307 [500000/697932 (72%)]\tLoss: 172.531250\n",
      "Train Epoch: 307 [600000/697932 (86%)]\tLoss: 172.849125\n",
      "====> Epoch: 307 Average loss: 172.5560\n",
      "====> Test set loss: 174.1644\n",
      "Train Epoch: 308 [0/697932 (0%)]\tLoss: 169.947469\n",
      "Train Epoch: 308 [100000/697932 (14%)]\tLoss: 175.080578\n",
      "Train Epoch: 308 [200000/697932 (29%)]\tLoss: 173.227672\n",
      "Train Epoch: 308 [300000/697932 (43%)]\tLoss: 174.515344\n",
      "Train Epoch: 308 [400000/697932 (57%)]\tLoss: 170.701188\n",
      "Train Epoch: 308 [500000/697932 (72%)]\tLoss: 174.426516\n",
      "Train Epoch: 308 [600000/697932 (86%)]\tLoss: 174.869844\n",
      "====> Epoch: 308 Average loss: 172.5706\n",
      "====> Test set loss: 174.1075\n",
      "Train Epoch: 309 [0/697932 (0%)]\tLoss: 170.859531\n",
      "Train Epoch: 309 [100000/697932 (14%)]\tLoss: 173.868453\n",
      "Train Epoch: 309 [200000/697932 (29%)]\tLoss: 172.763563\n",
      "Train Epoch: 309 [300000/697932 (43%)]\tLoss: 175.078812\n",
      "Train Epoch: 309 [400000/697932 (57%)]\tLoss: 172.605875\n",
      "Train Epoch: 309 [500000/697932 (72%)]\tLoss: 174.829047\n",
      "Train Epoch: 309 [600000/697932 (86%)]\tLoss: 173.510109\n",
      "====> Epoch: 309 Average loss: 172.8842\n",
      "====> Test set loss: 174.1688\n",
      "Train Epoch: 310 [0/697932 (0%)]\tLoss: 172.658312\n",
      "Train Epoch: 310 [100000/697932 (14%)]\tLoss: 175.002578\n",
      "Train Epoch: 310 [200000/697932 (29%)]\tLoss: 172.895016\n",
      "Train Epoch: 310 [300000/697932 (43%)]\tLoss: 171.560203\n",
      "Train Epoch: 310 [400000/697932 (57%)]\tLoss: 171.640406\n",
      "Train Epoch: 310 [500000/697932 (72%)]\tLoss: 172.580672\n",
      "Train Epoch: 310 [600000/697932 (86%)]\tLoss: 171.407688\n",
      "====> Epoch: 310 Average loss: 172.6878\n",
      "====> Test set loss: 174.2397\n",
      "Train Epoch: 311 [0/697932 (0%)]\tLoss: 172.654203\n",
      "Train Epoch: 311 [100000/697932 (14%)]\tLoss: 174.250984\n",
      "Train Epoch: 311 [200000/697932 (29%)]\tLoss: 170.810359\n",
      "Train Epoch: 311 [300000/697932 (43%)]\tLoss: 172.757656\n",
      "Train Epoch: 311 [400000/697932 (57%)]\tLoss: 170.735281\n",
      "Train Epoch: 311 [500000/697932 (72%)]\tLoss: 174.788531\n",
      "Train Epoch: 311 [600000/697932 (86%)]\tLoss: 171.615531\n",
      "====> Epoch: 311 Average loss: 172.7047\n",
      "====> Test set loss: 174.1029\n",
      "Train Epoch: 312 [0/697932 (0%)]\tLoss: 170.687187\n",
      "Train Epoch: 312 [100000/697932 (14%)]\tLoss: 172.378125\n",
      "Train Epoch: 312 [200000/697932 (29%)]\tLoss: 168.962859\n",
      "Train Epoch: 312 [300000/697932 (43%)]\tLoss: 174.785750\n",
      "Train Epoch: 312 [400000/697932 (57%)]\tLoss: 171.278594\n",
      "Train Epoch: 312 [500000/697932 (72%)]\tLoss: 171.895031\n",
      "Train Epoch: 312 [600000/697932 (86%)]\tLoss: 172.398828\n",
      "====> Epoch: 312 Average loss: 172.6814\n",
      "====> Test set loss: 173.9337\n",
      "Train Epoch: 313 [0/697932 (0%)]\tLoss: 168.561578\n",
      "Train Epoch: 313 [100000/697932 (14%)]\tLoss: 170.302281\n",
      "Train Epoch: 313 [200000/697932 (29%)]\tLoss: 174.087531\n",
      "Train Epoch: 313 [300000/697932 (43%)]\tLoss: 172.605516\n",
      "Train Epoch: 313 [400000/697932 (57%)]\tLoss: 172.451578\n",
      "Train Epoch: 313 [500000/697932 (72%)]\tLoss: 172.334547\n",
      "Train Epoch: 313 [600000/697932 (86%)]\tLoss: 170.198750\n",
      "====> Epoch: 313 Average loss: 172.6570\n",
      "====> Test set loss: 174.2433\n",
      "Train Epoch: 314 [0/697932 (0%)]\tLoss: 170.274422\n",
      "Train Epoch: 314 [100000/697932 (14%)]\tLoss: 172.436828\n",
      "Train Epoch: 314 [200000/697932 (29%)]\tLoss: 172.448969\n",
      "Train Epoch: 314 [300000/697932 (43%)]\tLoss: 173.417656\n",
      "Train Epoch: 314 [400000/697932 (57%)]\tLoss: 174.580938\n",
      "Train Epoch: 314 [500000/697932 (72%)]\tLoss: 173.730516\n",
      "Train Epoch: 314 [600000/697932 (86%)]\tLoss: 174.183188\n",
      "====> Epoch: 314 Average loss: 172.5046\n",
      "====> Test set loss: 174.0279\n",
      "Train Epoch: 315 [0/697932 (0%)]\tLoss: 171.967234\n",
      "Train Epoch: 315 [100000/697932 (14%)]\tLoss: 172.760063\n",
      "Train Epoch: 315 [200000/697932 (29%)]\tLoss: 171.911422\n",
      "Train Epoch: 315 [300000/697932 (43%)]\tLoss: 172.317828\n",
      "Train Epoch: 315 [400000/697932 (57%)]\tLoss: 171.449563\n",
      "Train Epoch: 315 [500000/697932 (72%)]\tLoss: 169.883344\n",
      "Train Epoch: 315 [600000/697932 (86%)]\tLoss: 172.538281\n",
      "====> Epoch: 315 Average loss: 172.5356\n",
      "====> Test set loss: 174.3306\n",
      "Train Epoch: 316 [0/697932 (0%)]\tLoss: 171.307469\n",
      "Train Epoch: 316 [100000/697932 (14%)]\tLoss: 171.832500\n",
      "Train Epoch: 316 [200000/697932 (29%)]\tLoss: 171.886484\n",
      "Train Epoch: 316 [300000/697932 (43%)]\tLoss: 172.618891\n",
      "Train Epoch: 316 [400000/697932 (57%)]\tLoss: 171.639484\n",
      "Train Epoch: 316 [500000/697932 (72%)]\tLoss: 170.216656\n",
      "Train Epoch: 316 [600000/697932 (86%)]\tLoss: 176.556953\n",
      "====> Epoch: 316 Average loss: 172.5607\n",
      "====> Test set loss: 174.2263\n",
      "Train Epoch: 317 [0/697932 (0%)]\tLoss: 174.161469\n",
      "Train Epoch: 317 [100000/697932 (14%)]\tLoss: 173.638187\n",
      "Train Epoch: 317 [200000/697932 (29%)]\tLoss: 172.513406\n",
      "Train Epoch: 317 [300000/697932 (43%)]\tLoss: 171.714469\n",
      "Train Epoch: 317 [400000/697932 (57%)]\tLoss: 172.506953\n",
      "Train Epoch: 317 [500000/697932 (72%)]\tLoss: 169.087609\n",
      "Train Epoch: 317 [600000/697932 (86%)]\tLoss: 172.981063\n",
      "====> Epoch: 317 Average loss: 172.3395\n",
      "====> Test set loss: 174.0070\n",
      "Train Epoch: 318 [0/697932 (0%)]\tLoss: 172.181922\n",
      "Train Epoch: 318 [100000/697932 (14%)]\tLoss: 172.689125\n",
      "Train Epoch: 318 [200000/697932 (29%)]\tLoss: 174.437563\n",
      "Train Epoch: 318 [300000/697932 (43%)]\tLoss: 170.473984\n",
      "Train Epoch: 318 [400000/697932 (57%)]\tLoss: 169.488812\n",
      "Train Epoch: 318 [500000/697932 (72%)]\tLoss: 172.704062\n",
      "Train Epoch: 318 [600000/697932 (86%)]\tLoss: 174.636547\n",
      "====> Epoch: 318 Average loss: 172.3392\n",
      "====> Test set loss: 174.2576\n",
      "Train Epoch: 319 [0/697932 (0%)]\tLoss: 174.838922\n",
      "Train Epoch: 319 [100000/697932 (14%)]\tLoss: 173.610281\n",
      "Train Epoch: 319 [200000/697932 (29%)]\tLoss: 172.769266\n",
      "Train Epoch: 319 [300000/697932 (43%)]\tLoss: 169.517656\n",
      "Train Epoch: 319 [400000/697932 (57%)]\tLoss: 170.514703\n",
      "Train Epoch: 319 [500000/697932 (72%)]\tLoss: 169.918359\n",
      "Train Epoch: 319 [600000/697932 (86%)]\tLoss: 172.868234\n",
      "====> Epoch: 319 Average loss: 172.6663\n",
      "====> Test set loss: 174.3872\n",
      "Train Epoch: 320 [0/697932 (0%)]\tLoss: 172.641750\n",
      "Train Epoch: 320 [100000/697932 (14%)]\tLoss: 173.280750\n",
      "Train Epoch: 320 [200000/697932 (29%)]\tLoss: 177.713875\n",
      "Train Epoch: 320 [300000/697932 (43%)]\tLoss: 170.008781\n",
      "Train Epoch: 320 [400000/697932 (57%)]\tLoss: 172.583438\n",
      "Train Epoch: 320 [500000/697932 (72%)]\tLoss: 174.575984\n",
      "Train Epoch: 320 [600000/697932 (86%)]\tLoss: 173.071422\n",
      "====> Epoch: 320 Average loss: 172.9201\n",
      "====> Test set loss: 174.5365\n",
      "Train Epoch: 321 [0/697932 (0%)]\tLoss: 170.002125\n",
      "Train Epoch: 321 [100000/697932 (14%)]\tLoss: 174.230016\n",
      "Train Epoch: 321 [200000/697932 (29%)]\tLoss: 171.733406\n",
      "Train Epoch: 321 [300000/697932 (43%)]\tLoss: 171.883906\n",
      "Train Epoch: 321 [400000/697932 (57%)]\tLoss: 169.957250\n",
      "Train Epoch: 321 [500000/697932 (72%)]\tLoss: 171.038859\n",
      "Train Epoch: 321 [600000/697932 (86%)]\tLoss: 173.825125\n",
      "====> Epoch: 321 Average loss: 172.4689\n",
      "====> Test set loss: 174.1104\n",
      "Train Epoch: 322 [0/697932 (0%)]\tLoss: 174.361781\n",
      "Train Epoch: 322 [100000/697932 (14%)]\tLoss: 172.864656\n",
      "Train Epoch: 322 [200000/697932 (29%)]\tLoss: 175.373891\n",
      "Train Epoch: 322 [300000/697932 (43%)]\tLoss: 173.594500\n",
      "Train Epoch: 322 [400000/697932 (57%)]\tLoss: 174.325141\n",
      "Train Epoch: 322 [500000/697932 (72%)]\tLoss: 171.299734\n",
      "Train Epoch: 322 [600000/697932 (86%)]\tLoss: 172.986187\n",
      "====> Epoch: 322 Average loss: 172.3718\n",
      "====> Test set loss: 173.7874\n",
      "Train Epoch: 323 [0/697932 (0%)]\tLoss: 171.149891\n",
      "Train Epoch: 323 [100000/697932 (14%)]\tLoss: 173.532781\n",
      "Train Epoch: 323 [200000/697932 (29%)]\tLoss: 174.141453\n",
      "Train Epoch: 323 [300000/697932 (43%)]\tLoss: 171.992781\n",
      "Train Epoch: 323 [400000/697932 (57%)]\tLoss: 173.348906\n",
      "Train Epoch: 323 [500000/697932 (72%)]\tLoss: 173.980328\n",
      "Train Epoch: 323 [600000/697932 (86%)]\tLoss: 170.272562\n",
      "====> Epoch: 323 Average loss: 172.2825\n",
      "====> Test set loss: 174.0809\n",
      "Train Epoch: 324 [0/697932 (0%)]\tLoss: 175.441984\n",
      "Train Epoch: 324 [100000/697932 (14%)]\tLoss: 172.158125\n",
      "Train Epoch: 324 [200000/697932 (29%)]\tLoss: 170.842453\n",
      "Train Epoch: 324 [300000/697932 (43%)]\tLoss: 173.222000\n",
      "Train Epoch: 324 [400000/697932 (57%)]\tLoss: 172.431875\n",
      "Train Epoch: 324 [500000/697932 (72%)]\tLoss: 172.002484\n",
      "Train Epoch: 324 [600000/697932 (86%)]\tLoss: 175.241859\n",
      "====> Epoch: 324 Average loss: 172.5818\n",
      "====> Test set loss: 174.0602\n",
      "Train Epoch: 325 [0/697932 (0%)]\tLoss: 173.483875\n",
      "Train Epoch: 325 [100000/697932 (14%)]\tLoss: 171.734938\n",
      "Train Epoch: 325 [200000/697932 (29%)]\tLoss: 170.030281\n",
      "Train Epoch: 325 [300000/697932 (43%)]\tLoss: 169.362375\n",
      "Train Epoch: 325 [400000/697932 (57%)]\tLoss: 170.121047\n",
      "Train Epoch: 325 [500000/697932 (72%)]\tLoss: 173.094828\n",
      "Train Epoch: 325 [600000/697932 (86%)]\tLoss: 175.848359\n",
      "====> Epoch: 325 Average loss: 172.5150\n",
      "====> Test set loss: 174.0381\n",
      "Train Epoch: 326 [0/697932 (0%)]\tLoss: 173.180203\n",
      "Train Epoch: 326 [100000/697932 (14%)]\tLoss: 172.276609\n",
      "Train Epoch: 326 [200000/697932 (29%)]\tLoss: 171.427344\n",
      "Train Epoch: 326 [300000/697932 (43%)]\tLoss: 171.000422\n",
      "Train Epoch: 326 [400000/697932 (57%)]\tLoss: 171.298812\n",
      "Train Epoch: 326 [500000/697932 (72%)]\tLoss: 173.107812\n",
      "Train Epoch: 326 [600000/697932 (86%)]\tLoss: 170.900766\n",
      "====> Epoch: 326 Average loss: 172.2876\n",
      "====> Test set loss: 173.8704\n",
      "Train Epoch: 327 [0/697932 (0%)]\tLoss: 172.357484\n",
      "Train Epoch: 327 [100000/697932 (14%)]\tLoss: 173.739234\n",
      "Train Epoch: 327 [200000/697932 (29%)]\tLoss: 170.895062\n",
      "Train Epoch: 327 [300000/697932 (43%)]\tLoss: 170.856594\n",
      "Train Epoch: 327 [400000/697932 (57%)]\tLoss: 171.185328\n",
      "Train Epoch: 327 [500000/697932 (72%)]\tLoss: 174.957469\n",
      "Train Epoch: 327 [600000/697932 (86%)]\tLoss: 172.787234\n",
      "====> Epoch: 327 Average loss: 172.5296\n",
      "====> Test set loss: 174.6104\n",
      "Train Epoch: 328 [0/697932 (0%)]\tLoss: 170.996297\n",
      "Train Epoch: 328 [100000/697932 (14%)]\tLoss: 172.578094\n",
      "Train Epoch: 328 [200000/697932 (29%)]\tLoss: 174.063437\n",
      "Train Epoch: 328 [300000/697932 (43%)]\tLoss: 170.537875\n",
      "Train Epoch: 328 [400000/697932 (57%)]\tLoss: 174.076219\n",
      "Train Epoch: 328 [500000/697932 (72%)]\tLoss: 173.559422\n",
      "Train Epoch: 328 [600000/697932 (86%)]\tLoss: 174.660625\n",
      "====> Epoch: 328 Average loss: 172.6073\n",
      "====> Test set loss: 174.1395\n",
      "Train Epoch: 329 [0/697932 (0%)]\tLoss: 173.979609\n",
      "Train Epoch: 329 [100000/697932 (14%)]\tLoss: 171.191969\n",
      "Train Epoch: 329 [200000/697932 (29%)]\tLoss: 166.293281\n",
      "Train Epoch: 329 [300000/697932 (43%)]\tLoss: 174.711813\n",
      "Train Epoch: 329 [400000/697932 (57%)]\tLoss: 172.548531\n",
      "Train Epoch: 329 [500000/697932 (72%)]\tLoss: 174.262375\n",
      "Train Epoch: 329 [600000/697932 (86%)]\tLoss: 169.851266\n",
      "====> Epoch: 329 Average loss: 172.4212\n",
      "====> Test set loss: 174.0927\n",
      "Train Epoch: 330 [0/697932 (0%)]\tLoss: 173.825500\n",
      "Train Epoch: 330 [100000/697932 (14%)]\tLoss: 173.450562\n",
      "Train Epoch: 330 [200000/697932 (29%)]\tLoss: 169.684937\n",
      "Train Epoch: 330 [300000/697932 (43%)]\tLoss: 170.576547\n",
      "Train Epoch: 330 [400000/697932 (57%)]\tLoss: 175.604500\n",
      "Train Epoch: 330 [500000/697932 (72%)]\tLoss: 170.274656\n",
      "Train Epoch: 330 [600000/697932 (86%)]\tLoss: 173.979250\n",
      "====> Epoch: 330 Average loss: 172.5132\n",
      "====> Test set loss: 174.3351\n",
      "Train Epoch: 331 [0/697932 (0%)]\tLoss: 172.637719\n",
      "Train Epoch: 331 [100000/697932 (14%)]\tLoss: 171.996406\n",
      "Train Epoch: 331 [200000/697932 (29%)]\tLoss: 174.215906\n",
      "Train Epoch: 331 [300000/697932 (43%)]\tLoss: 173.930656\n",
      "Train Epoch: 331 [400000/697932 (57%)]\tLoss: 172.652594\n",
      "Train Epoch: 331 [500000/697932 (72%)]\tLoss: 172.284734\n",
      "Train Epoch: 331 [600000/697932 (86%)]\tLoss: 171.387172\n",
      "====> Epoch: 331 Average loss: 172.4249\n",
      "====> Test set loss: 174.2416\n",
      "Train Epoch: 332 [0/697932 (0%)]\tLoss: 172.675703\n",
      "Train Epoch: 332 [100000/697932 (14%)]\tLoss: 172.932469\n",
      "Train Epoch: 332 [200000/697932 (29%)]\tLoss: 170.665906\n",
      "Train Epoch: 332 [300000/697932 (43%)]\tLoss: 172.631328\n",
      "Train Epoch: 332 [400000/697932 (57%)]\tLoss: 170.827063\n",
      "Train Epoch: 332 [500000/697932 (72%)]\tLoss: 175.909516\n",
      "Train Epoch: 332 [600000/697932 (86%)]\tLoss: 170.208438\n",
      "====> Epoch: 332 Average loss: 172.4822\n",
      "====> Test set loss: 173.9038\n",
      "Train Epoch: 333 [0/697932 (0%)]\tLoss: 170.360484\n",
      "Train Epoch: 333 [100000/697932 (14%)]\tLoss: 172.854406\n",
      "Train Epoch: 333 [200000/697932 (29%)]\tLoss: 172.567688\n",
      "Train Epoch: 333 [300000/697932 (43%)]\tLoss: 171.102125\n",
      "Train Epoch: 333 [400000/697932 (57%)]\tLoss: 173.046344\n",
      "Train Epoch: 333 [500000/697932 (72%)]\tLoss: 173.599063\n",
      "Train Epoch: 333 [600000/697932 (86%)]\tLoss: 173.441719\n",
      "====> Epoch: 333 Average loss: 172.1480\n",
      "====> Test set loss: 173.7705\n",
      "Train Epoch: 334 [0/697932 (0%)]\tLoss: 175.356578\n",
      "Train Epoch: 334 [100000/697932 (14%)]\tLoss: 170.510156\n",
      "Train Epoch: 334 [200000/697932 (29%)]\tLoss: 172.444125\n",
      "Train Epoch: 334 [300000/697932 (43%)]\tLoss: 172.119594\n",
      "Train Epoch: 334 [400000/697932 (57%)]\tLoss: 172.493734\n",
      "Train Epoch: 334 [500000/697932 (72%)]\tLoss: 173.870187\n",
      "Train Epoch: 334 [600000/697932 (86%)]\tLoss: 169.716516\n",
      "====> Epoch: 334 Average loss: 171.9633\n",
      "====> Test set loss: 173.7808\n",
      "Train Epoch: 335 [0/697932 (0%)]\tLoss: 173.333391\n",
      "Train Epoch: 335 [100000/697932 (14%)]\tLoss: 171.057766\n",
      "Train Epoch: 335 [200000/697932 (29%)]\tLoss: 173.578359\n",
      "Train Epoch: 335 [300000/697932 (43%)]\tLoss: 174.270547\n",
      "Train Epoch: 335 [400000/697932 (57%)]\tLoss: 172.581969\n",
      "Train Epoch: 335 [500000/697932 (72%)]\tLoss: 173.969875\n",
      "Train Epoch: 335 [600000/697932 (86%)]\tLoss: 172.689266\n",
      "====> Epoch: 335 Average loss: 171.9759\n",
      "====> Test set loss: 173.7310\n",
      "Train Epoch: 336 [0/697932 (0%)]\tLoss: 171.206938\n",
      "Train Epoch: 336 [100000/697932 (14%)]\tLoss: 170.694016\n",
      "Train Epoch: 336 [200000/697932 (29%)]\tLoss: 171.748844\n",
      "Train Epoch: 336 [300000/697932 (43%)]\tLoss: 171.075594\n",
      "Train Epoch: 336 [400000/697932 (57%)]\tLoss: 169.860625\n",
      "Train Epoch: 336 [500000/697932 (72%)]\tLoss: 175.762750\n",
      "Train Epoch: 336 [600000/697932 (86%)]\tLoss: 173.321875\n",
      "====> Epoch: 336 Average loss: 172.0449\n",
      "====> Test set loss: 173.8223\n",
      "Train Epoch: 337 [0/697932 (0%)]\tLoss: 169.720141\n",
      "Train Epoch: 337 [100000/697932 (14%)]\tLoss: 172.380437\n",
      "Train Epoch: 337 [200000/697932 (29%)]\tLoss: 172.491875\n",
      "Train Epoch: 337 [300000/697932 (43%)]\tLoss: 174.628891\n",
      "Train Epoch: 337 [400000/697932 (57%)]\tLoss: 171.341703\n",
      "Train Epoch: 337 [500000/697932 (72%)]\tLoss: 171.921063\n",
      "Train Epoch: 337 [600000/697932 (86%)]\tLoss: 172.544688\n",
      "====> Epoch: 337 Average loss: 172.4388\n",
      "====> Test set loss: 174.1964\n",
      "Train Epoch: 338 [0/697932 (0%)]\tLoss: 172.944266\n",
      "Train Epoch: 338 [100000/697932 (14%)]\tLoss: 174.326469\n",
      "Train Epoch: 338 [200000/697932 (29%)]\tLoss: 173.527453\n",
      "Train Epoch: 338 [300000/697932 (43%)]\tLoss: 169.394609\n",
      "Train Epoch: 338 [400000/697932 (57%)]\tLoss: 172.861656\n",
      "Train Epoch: 338 [500000/697932 (72%)]\tLoss: 171.205906\n",
      "Train Epoch: 338 [600000/697932 (86%)]\tLoss: 174.248031\n",
      "====> Epoch: 338 Average loss: 172.4457\n",
      "====> Test set loss: 174.4649\n",
      "Train Epoch: 339 [0/697932 (0%)]\tLoss: 172.426063\n",
      "Train Epoch: 339 [100000/697932 (14%)]\tLoss: 173.752781\n",
      "Train Epoch: 339 [200000/697932 (29%)]\tLoss: 171.850719\n",
      "Train Epoch: 339 [300000/697932 (43%)]\tLoss: 173.054031\n",
      "Train Epoch: 339 [400000/697932 (57%)]\tLoss: 168.837078\n",
      "Train Epoch: 339 [500000/697932 (72%)]\tLoss: 170.578484\n",
      "Train Epoch: 339 [600000/697932 (86%)]\tLoss: 171.093297\n",
      "====> Epoch: 339 Average loss: 172.3719\n",
      "====> Test set loss: 173.9755\n",
      "Train Epoch: 340 [0/697932 (0%)]\tLoss: 168.534891\n",
      "Train Epoch: 340 [100000/697932 (14%)]\tLoss: 172.643234\n",
      "Train Epoch: 340 [200000/697932 (29%)]\tLoss: 171.254484\n",
      "Train Epoch: 340 [300000/697932 (43%)]\tLoss: 169.179047\n",
      "Train Epoch: 340 [400000/697932 (57%)]\tLoss: 170.852500\n",
      "Train Epoch: 340 [500000/697932 (72%)]\tLoss: 173.115125\n",
      "Train Epoch: 340 [600000/697932 (86%)]\tLoss: 173.392766\n",
      "====> Epoch: 340 Average loss: 172.0440\n",
      "====> Test set loss: 174.2955\n",
      "Train Epoch: 341 [0/697932 (0%)]\tLoss: 171.069312\n",
      "Train Epoch: 341 [100000/697932 (14%)]\tLoss: 171.311656\n",
      "Train Epoch: 341 [200000/697932 (29%)]\tLoss: 173.478750\n",
      "Train Epoch: 341 [300000/697932 (43%)]\tLoss: 169.768656\n",
      "Train Epoch: 341 [400000/697932 (57%)]\tLoss: 171.870016\n",
      "Train Epoch: 341 [500000/697932 (72%)]\tLoss: 174.063859\n",
      "Train Epoch: 341 [600000/697932 (86%)]\tLoss: 173.909047\n",
      "====> Epoch: 341 Average loss: 172.2116\n",
      "====> Test set loss: 174.0047\n",
      "Train Epoch: 342 [0/697932 (0%)]\tLoss: 173.262094\n",
      "Train Epoch: 342 [100000/697932 (14%)]\tLoss: 172.238047\n",
      "Train Epoch: 342 [200000/697932 (29%)]\tLoss: 171.218594\n",
      "Train Epoch: 342 [300000/697932 (43%)]\tLoss: 173.280484\n",
      "Train Epoch: 342 [400000/697932 (57%)]\tLoss: 172.353703\n",
      "Train Epoch: 342 [500000/697932 (72%)]\tLoss: 172.404687\n",
      "Train Epoch: 342 [600000/697932 (86%)]\tLoss: 173.507703\n",
      "====> Epoch: 342 Average loss: 172.0792\n",
      "====> Test set loss: 173.7623\n",
      "Train Epoch: 343 [0/697932 (0%)]\tLoss: 169.446766\n",
      "Train Epoch: 343 [100000/697932 (14%)]\tLoss: 171.463687\n",
      "Train Epoch: 343 [200000/697932 (29%)]\tLoss: 171.795078\n",
      "Train Epoch: 343 [300000/697932 (43%)]\tLoss: 171.935969\n",
      "Train Epoch: 343 [400000/697932 (57%)]\tLoss: 175.684641\n",
      "Train Epoch: 343 [500000/697932 (72%)]\tLoss: 172.219469\n",
      "Train Epoch: 343 [600000/697932 (86%)]\tLoss: 173.559672\n",
      "====> Epoch: 343 Average loss: 172.1199\n",
      "====> Test set loss: 173.7576\n",
      "Train Epoch: 344 [0/697932 (0%)]\tLoss: 169.360187\n",
      "Train Epoch: 344 [100000/697932 (14%)]\tLoss: 175.122406\n",
      "Train Epoch: 344 [200000/697932 (29%)]\tLoss: 171.083922\n",
      "Train Epoch: 344 [300000/697932 (43%)]\tLoss: 174.726188\n",
      "Train Epoch: 344 [400000/697932 (57%)]\tLoss: 176.172969\n",
      "Train Epoch: 344 [500000/697932 (72%)]\tLoss: 170.908156\n",
      "Train Epoch: 344 [600000/697932 (86%)]\tLoss: 171.738547\n",
      "====> Epoch: 344 Average loss: 172.1412\n",
      "====> Test set loss: 173.9067\n",
      "Train Epoch: 345 [0/697932 (0%)]\tLoss: 172.317281\n",
      "Train Epoch: 345 [100000/697932 (14%)]\tLoss: 170.446297\n",
      "Train Epoch: 345 [200000/697932 (29%)]\tLoss: 173.635047\n",
      "Train Epoch: 345 [300000/697932 (43%)]\tLoss: 173.136172\n",
      "Train Epoch: 345 [400000/697932 (57%)]\tLoss: 173.314641\n",
      "Train Epoch: 345 [500000/697932 (72%)]\tLoss: 174.211328\n",
      "Train Epoch: 345 [600000/697932 (86%)]\tLoss: 174.673031\n",
      "====> Epoch: 345 Average loss: 172.2059\n",
      "====> Test set loss: 174.1169\n",
      "Train Epoch: 346 [0/697932 (0%)]\tLoss: 171.834375\n",
      "Train Epoch: 346 [100000/697932 (14%)]\tLoss: 171.527828\n",
      "Train Epoch: 346 [200000/697932 (29%)]\tLoss: 173.236750\n",
      "Train Epoch: 346 [300000/697932 (43%)]\tLoss: 174.171219\n",
      "Train Epoch: 346 [400000/697932 (57%)]\tLoss: 175.402688\n",
      "Train Epoch: 346 [500000/697932 (72%)]\tLoss: 173.565937\n",
      "Train Epoch: 346 [600000/697932 (86%)]\tLoss: 174.240906\n",
      "====> Epoch: 346 Average loss: 172.2781\n",
      "====> Test set loss: 174.0873\n",
      "Train Epoch: 347 [0/697932 (0%)]\tLoss: 171.142188\n",
      "Train Epoch: 347 [100000/697932 (14%)]\tLoss: 172.748094\n",
      "Train Epoch: 347 [200000/697932 (29%)]\tLoss: 175.401563\n",
      "Train Epoch: 347 [300000/697932 (43%)]\tLoss: 168.993594\n",
      "Train Epoch: 347 [400000/697932 (57%)]\tLoss: 171.326766\n",
      "Train Epoch: 347 [500000/697932 (72%)]\tLoss: 172.714453\n",
      "Train Epoch: 347 [600000/697932 (86%)]\tLoss: 170.377406\n",
      "====> Epoch: 347 Average loss: 172.2535\n",
      "====> Test set loss: 174.0809\n",
      "Train Epoch: 348 [0/697932 (0%)]\tLoss: 173.709844\n",
      "Train Epoch: 348 [100000/697932 (14%)]\tLoss: 172.494406\n",
      "Train Epoch: 348 [200000/697932 (29%)]\tLoss: 171.100297\n",
      "Train Epoch: 348 [300000/697932 (43%)]\tLoss: 173.843437\n",
      "Train Epoch: 348 [400000/697932 (57%)]\tLoss: 169.753516\n",
      "Train Epoch: 348 [500000/697932 (72%)]\tLoss: 171.504203\n",
      "Train Epoch: 348 [600000/697932 (86%)]\tLoss: 172.765953\n",
      "====> Epoch: 348 Average loss: 172.1752\n",
      "====> Test set loss: 173.9221\n",
      "Train Epoch: 349 [0/697932 (0%)]\tLoss: 171.393109\n",
      "Train Epoch: 349 [100000/697932 (14%)]\tLoss: 171.977922\n",
      "Train Epoch: 349 [200000/697932 (29%)]\tLoss: 171.027187\n",
      "Train Epoch: 349 [300000/697932 (43%)]\tLoss: 169.127797\n",
      "Train Epoch: 349 [400000/697932 (57%)]\tLoss: 168.888313\n",
      "Train Epoch: 349 [500000/697932 (72%)]\tLoss: 174.500563\n",
      "Train Epoch: 349 [600000/697932 (86%)]\tLoss: 172.003000\n",
      "====> Epoch: 349 Average loss: 172.3728\n",
      "====> Test set loss: 174.2962\n",
      "Train Epoch: 350 [0/697932 (0%)]\tLoss: 173.384391\n",
      "Train Epoch: 350 [100000/697932 (14%)]\tLoss: 171.972187\n",
      "Train Epoch: 350 [200000/697932 (29%)]\tLoss: 171.914469\n",
      "Train Epoch: 350 [300000/697932 (43%)]\tLoss: 171.104406\n",
      "Train Epoch: 350 [400000/697932 (57%)]\tLoss: 171.350734\n",
      "Train Epoch: 350 [500000/697932 (72%)]\tLoss: 171.790922\n",
      "Train Epoch: 350 [600000/697932 (86%)]\tLoss: 174.620438\n",
      "====> Epoch: 350 Average loss: 172.3191\n",
      "====> Test set loss: 173.9521\n",
      "Train Epoch: 351 [0/697932 (0%)]\tLoss: 169.487266\n",
      "Train Epoch: 351 [100000/697932 (14%)]\tLoss: 173.036500\n",
      "Train Epoch: 351 [200000/697932 (29%)]\tLoss: 171.428797\n",
      "Train Epoch: 351 [300000/697932 (43%)]\tLoss: 174.562031\n",
      "Train Epoch: 351 [400000/697932 (57%)]\tLoss: 170.820250\n",
      "Train Epoch: 351 [500000/697932 (72%)]\tLoss: 175.293531\n",
      "Train Epoch: 351 [600000/697932 (86%)]\tLoss: 170.458063\n",
      "====> Epoch: 351 Average loss: 172.3510\n",
      "====> Test set loss: 174.0081\n",
      "Train Epoch: 352 [0/697932 (0%)]\tLoss: 174.207641\n",
      "Train Epoch: 352 [100000/697932 (14%)]\tLoss: 174.271562\n",
      "Train Epoch: 352 [200000/697932 (29%)]\tLoss: 171.629938\n",
      "Train Epoch: 352 [300000/697932 (43%)]\tLoss: 171.394813\n",
      "Train Epoch: 352 [400000/697932 (57%)]\tLoss: 173.010047\n",
      "Train Epoch: 352 [500000/697932 (72%)]\tLoss: 174.197203\n",
      "Train Epoch: 352 [600000/697932 (86%)]\tLoss: 174.922594\n",
      "====> Epoch: 352 Average loss: 172.5261\n",
      "====> Test set loss: 173.9416\n",
      "Train Epoch: 353 [0/697932 (0%)]\tLoss: 169.980531\n",
      "Train Epoch: 353 [100000/697932 (14%)]\tLoss: 175.776719\n",
      "Train Epoch: 353 [200000/697932 (29%)]\tLoss: 171.866547\n",
      "Train Epoch: 353 [300000/697932 (43%)]\tLoss: 170.790500\n",
      "Train Epoch: 353 [400000/697932 (57%)]\tLoss: 173.035953\n",
      "Train Epoch: 353 [500000/697932 (72%)]\tLoss: 170.258312\n",
      "Train Epoch: 353 [600000/697932 (86%)]\tLoss: 171.906922\n",
      "====> Epoch: 353 Average loss: 172.5796\n",
      "====> Test set loss: 174.3981\n",
      "Train Epoch: 354 [0/697932 (0%)]\tLoss: 171.770797\n",
      "Train Epoch: 354 [100000/697932 (14%)]\tLoss: 172.241219\n",
      "Train Epoch: 354 [200000/697932 (29%)]\tLoss: 172.944063\n",
      "Train Epoch: 354 [300000/697932 (43%)]\tLoss: 173.727672\n",
      "Train Epoch: 354 [400000/697932 (57%)]\tLoss: 173.661891\n",
      "Train Epoch: 354 [500000/697932 (72%)]\tLoss: 173.821812\n",
      "Train Epoch: 354 [600000/697932 (86%)]\tLoss: 174.129156\n",
      "====> Epoch: 354 Average loss: 172.9636\n",
      "====> Test set loss: 174.3965\n",
      "Train Epoch: 355 [0/697932 (0%)]\tLoss: 170.132547\n",
      "Train Epoch: 355 [100000/697932 (14%)]\tLoss: 171.832484\n",
      "Train Epoch: 355 [200000/697932 (29%)]\tLoss: 172.470031\n",
      "Train Epoch: 355 [300000/697932 (43%)]\tLoss: 171.726937\n",
      "Train Epoch: 355 [400000/697932 (57%)]\tLoss: 171.714891\n",
      "Train Epoch: 355 [500000/697932 (72%)]\tLoss: 173.710891\n",
      "Train Epoch: 355 [600000/697932 (86%)]\tLoss: 170.337016\n",
      "====> Epoch: 355 Average loss: 172.8259\n",
      "====> Test set loss: 174.1761\n",
      "Train Epoch: 356 [0/697932 (0%)]\tLoss: 174.069922\n",
      "Train Epoch: 356 [100000/697932 (14%)]\tLoss: 171.164375\n",
      "Train Epoch: 356 [200000/697932 (29%)]\tLoss: 174.834547\n",
      "Train Epoch: 356 [300000/697932 (43%)]\tLoss: 171.598281\n",
      "Train Epoch: 356 [400000/697932 (57%)]\tLoss: 174.908297\n",
      "Train Epoch: 356 [500000/697932 (72%)]\tLoss: 171.750047\n",
      "Train Epoch: 356 [600000/697932 (86%)]\tLoss: 174.352516\n",
      "====> Epoch: 356 Average loss: 172.3097\n",
      "====> Test set loss: 174.2414\n",
      "Train Epoch: 357 [0/697932 (0%)]\tLoss: 170.918359\n",
      "Train Epoch: 357 [100000/697932 (14%)]\tLoss: 172.282875\n",
      "Train Epoch: 357 [200000/697932 (29%)]\tLoss: 171.258703\n",
      "Train Epoch: 357 [300000/697932 (43%)]\tLoss: 174.038125\n",
      "Train Epoch: 357 [400000/697932 (57%)]\tLoss: 172.593734\n",
      "Train Epoch: 357 [500000/697932 (72%)]\tLoss: 175.979891\n",
      "Train Epoch: 357 [600000/697932 (86%)]\tLoss: 172.130844\n",
      "====> Epoch: 357 Average loss: 172.4646\n",
      "====> Test set loss: 173.9054\n",
      "Train Epoch: 358 [0/697932 (0%)]\tLoss: 171.608453\n",
      "Train Epoch: 358 [100000/697932 (14%)]\tLoss: 170.829844\n",
      "Train Epoch: 358 [200000/697932 (29%)]\tLoss: 173.699047\n",
      "Train Epoch: 358 [300000/697932 (43%)]\tLoss: 171.752844\n",
      "Train Epoch: 358 [400000/697932 (57%)]\tLoss: 168.309750\n",
      "Train Epoch: 358 [500000/697932 (72%)]\tLoss: 171.418531\n",
      "Train Epoch: 358 [600000/697932 (86%)]\tLoss: 172.473797\n",
      "====> Epoch: 358 Average loss: 172.2742\n",
      "====> Test set loss: 173.9381\n",
      "Train Epoch: 359 [0/697932 (0%)]\tLoss: 172.424766\n",
      "Train Epoch: 359 [100000/697932 (14%)]\tLoss: 174.529984\n",
      "Train Epoch: 359 [200000/697932 (29%)]\tLoss: 170.574641\n",
      "Train Epoch: 359 [300000/697932 (43%)]\tLoss: 174.369922\n",
      "Train Epoch: 359 [400000/697932 (57%)]\tLoss: 175.718750\n",
      "Train Epoch: 359 [500000/697932 (72%)]\tLoss: 170.557875\n",
      "Train Epoch: 359 [600000/697932 (86%)]\tLoss: 170.663563\n",
      "====> Epoch: 359 Average loss: 172.3730\n",
      "====> Test set loss: 174.0709\n",
      "Train Epoch: 360 [0/697932 (0%)]\tLoss: 170.010703\n",
      "Train Epoch: 360 [100000/697932 (14%)]\tLoss: 171.860344\n",
      "Train Epoch: 360 [200000/697932 (29%)]\tLoss: 175.382219\n",
      "Train Epoch: 360 [300000/697932 (43%)]\tLoss: 170.308734\n",
      "Train Epoch: 360 [400000/697932 (57%)]\tLoss: 170.558828\n",
      "Train Epoch: 360 [500000/697932 (72%)]\tLoss: 169.916969\n",
      "Train Epoch: 360 [600000/697932 (86%)]\tLoss: 173.416875\n",
      "====> Epoch: 360 Average loss: 172.4383\n",
      "====> Test set loss: 174.5365\n",
      "Train Epoch: 361 [0/697932 (0%)]\tLoss: 172.428016\n",
      "Train Epoch: 361 [100000/697932 (14%)]\tLoss: 175.195516\n",
      "Train Epoch: 361 [200000/697932 (29%)]\tLoss: 171.819391\n",
      "Train Epoch: 361 [300000/697932 (43%)]\tLoss: 173.230594\n",
      "Train Epoch: 361 [400000/697932 (57%)]\tLoss: 172.473266\n",
      "Train Epoch: 361 [500000/697932 (72%)]\tLoss: 170.790531\n",
      "Train Epoch: 361 [600000/697932 (86%)]\tLoss: 172.900031\n",
      "====> Epoch: 361 Average loss: 172.3644\n",
      "====> Test set loss: 173.9139\n",
      "Train Epoch: 362 [0/697932 (0%)]\tLoss: 172.458687\n",
      "Train Epoch: 362 [100000/697932 (14%)]\tLoss: 172.113063\n",
      "Train Epoch: 362 [200000/697932 (29%)]\tLoss: 177.424859\n",
      "Train Epoch: 362 [300000/697932 (43%)]\tLoss: 170.332250\n",
      "Train Epoch: 362 [400000/697932 (57%)]\tLoss: 171.194156\n",
      "Train Epoch: 362 [500000/697932 (72%)]\tLoss: 172.568250\n",
      "Train Epoch: 362 [600000/697932 (86%)]\tLoss: 170.013313\n",
      "====> Epoch: 362 Average loss: 172.2582\n",
      "====> Test set loss: 173.8432\n",
      "Train Epoch: 363 [0/697932 (0%)]\tLoss: 170.558453\n",
      "Train Epoch: 363 [100000/697932 (14%)]\tLoss: 174.016859\n",
      "Train Epoch: 363 [200000/697932 (29%)]\tLoss: 173.264922\n",
      "Train Epoch: 363 [300000/697932 (43%)]\tLoss: 172.070188\n",
      "Train Epoch: 363 [400000/697932 (57%)]\tLoss: 171.996594\n",
      "Train Epoch: 363 [500000/697932 (72%)]\tLoss: 172.527734\n",
      "Train Epoch: 363 [600000/697932 (86%)]\tLoss: 172.970672\n",
      "====> Epoch: 363 Average loss: 172.0548\n",
      "====> Test set loss: 173.7240\n",
      "Train Epoch: 364 [0/697932 (0%)]\tLoss: 173.197469\n",
      "Train Epoch: 364 [100000/697932 (14%)]\tLoss: 172.724922\n",
      "Train Epoch: 364 [200000/697932 (29%)]\tLoss: 170.034266\n",
      "Train Epoch: 364 [300000/697932 (43%)]\tLoss: 175.566516\n",
      "Train Epoch: 364 [400000/697932 (57%)]\tLoss: 174.400594\n",
      "Train Epoch: 364 [500000/697932 (72%)]\tLoss: 175.279687\n",
      "Train Epoch: 364 [600000/697932 (86%)]\tLoss: 173.814516\n",
      "====> Epoch: 364 Average loss: 172.0834\n",
      "====> Test set loss: 173.8907\n",
      "Train Epoch: 365 [0/697932 (0%)]\tLoss: 170.083469\n",
      "Train Epoch: 365 [100000/697932 (14%)]\tLoss: 171.857141\n",
      "Train Epoch: 365 [200000/697932 (29%)]\tLoss: 175.547313\n",
      "Train Epoch: 365 [300000/697932 (43%)]\tLoss: 171.922031\n",
      "Train Epoch: 365 [400000/697932 (57%)]\tLoss: 173.977453\n",
      "Train Epoch: 365 [500000/697932 (72%)]\tLoss: 174.199828\n",
      "Train Epoch: 365 [600000/697932 (86%)]\tLoss: 172.685344\n",
      "====> Epoch: 365 Average loss: 172.1561\n",
      "====> Test set loss: 174.0339\n",
      "Train Epoch: 366 [0/697932 (0%)]\tLoss: 172.479641\n",
      "Train Epoch: 366 [100000/697932 (14%)]\tLoss: 174.891594\n",
      "Train Epoch: 366 [200000/697932 (29%)]\tLoss: 175.148469\n",
      "Train Epoch: 366 [300000/697932 (43%)]\tLoss: 173.480500\n",
      "Train Epoch: 366 [400000/697932 (57%)]\tLoss: 172.717406\n",
      "Train Epoch: 366 [500000/697932 (72%)]\tLoss: 171.849891\n",
      "Train Epoch: 366 [600000/697932 (86%)]\tLoss: 172.007359\n",
      "====> Epoch: 366 Average loss: 172.1691\n",
      "====> Test set loss: 173.9428\n",
      "Train Epoch: 367 [0/697932 (0%)]\tLoss: 173.202812\n",
      "Train Epoch: 367 [100000/697932 (14%)]\tLoss: 171.063641\n",
      "Train Epoch: 367 [200000/697932 (29%)]\tLoss: 170.538719\n",
      "Train Epoch: 367 [300000/697932 (43%)]\tLoss: 169.530047\n",
      "Train Epoch: 367 [400000/697932 (57%)]\tLoss: 171.006563\n",
      "Train Epoch: 367 [500000/697932 (72%)]\tLoss: 170.804625\n",
      "Train Epoch: 367 [600000/697932 (86%)]\tLoss: 171.135719\n",
      "====> Epoch: 367 Average loss: 172.0771\n",
      "====> Test set loss: 173.8627\n",
      "Train Epoch: 368 [0/697932 (0%)]\tLoss: 167.896578\n",
      "Train Epoch: 368 [100000/697932 (14%)]\tLoss: 170.829672\n",
      "Train Epoch: 368 [200000/697932 (29%)]\tLoss: 170.685313\n",
      "Train Epoch: 368 [300000/697932 (43%)]\tLoss: 170.056656\n",
      "Train Epoch: 368 [400000/697932 (57%)]\tLoss: 172.223984\n",
      "Train Epoch: 368 [500000/697932 (72%)]\tLoss: 175.134938\n",
      "Train Epoch: 368 [600000/697932 (86%)]\tLoss: 171.551109\n",
      "====> Epoch: 368 Average loss: 172.5462\n",
      "====> Test set loss: 175.3822\n",
      "Train Epoch: 369 [0/697932 (0%)]\tLoss: 171.983359\n",
      "Train Epoch: 369 [100000/697932 (14%)]\tLoss: 175.164297\n",
      "Train Epoch: 369 [200000/697932 (29%)]\tLoss: 171.344531\n",
      "Train Epoch: 369 [300000/697932 (43%)]\tLoss: 171.137375\n",
      "Train Epoch: 369 [400000/697932 (57%)]\tLoss: 173.557031\n",
      "Train Epoch: 369 [500000/697932 (72%)]\tLoss: 172.084313\n",
      "Train Epoch: 369 [600000/697932 (86%)]\tLoss: 172.837766\n",
      "====> Epoch: 369 Average loss: 172.6533\n",
      "====> Test set loss: 174.6050\n",
      "Train Epoch: 370 [0/697932 (0%)]\tLoss: 172.234688\n",
      "Train Epoch: 370 [100000/697932 (14%)]\tLoss: 173.634375\n",
      "Train Epoch: 370 [200000/697932 (29%)]\tLoss: 174.327516\n",
      "Train Epoch: 370 [300000/697932 (43%)]\tLoss: 172.499250\n",
      "Train Epoch: 370 [400000/697932 (57%)]\tLoss: 172.355984\n",
      "Train Epoch: 370 [500000/697932 (72%)]\tLoss: 171.494859\n",
      "Train Epoch: 370 [600000/697932 (86%)]\tLoss: 168.209234\n",
      "====> Epoch: 370 Average loss: 172.4707\n",
      "====> Test set loss: 173.8844\n",
      "Train Epoch: 371 [0/697932 (0%)]\tLoss: 173.411953\n",
      "Train Epoch: 371 [100000/697932 (14%)]\tLoss: 169.046453\n",
      "Train Epoch: 371 [200000/697932 (29%)]\tLoss: 172.484359\n",
      "Train Epoch: 371 [300000/697932 (43%)]\tLoss: 172.814063\n",
      "Train Epoch: 371 [400000/697932 (57%)]\tLoss: 171.032625\n",
      "Train Epoch: 371 [500000/697932 (72%)]\tLoss: 173.464141\n",
      "Train Epoch: 371 [600000/697932 (86%)]\tLoss: 175.695938\n",
      "====> Epoch: 371 Average loss: 172.2324\n",
      "====> Test set loss: 174.2873\n",
      "Train Epoch: 372 [0/697932 (0%)]\tLoss: 172.095875\n",
      "Train Epoch: 372 [100000/697932 (14%)]\tLoss: 172.748578\n",
      "Train Epoch: 372 [200000/697932 (29%)]\tLoss: 171.081234\n",
      "Train Epoch: 372 [300000/697932 (43%)]\tLoss: 170.298578\n",
      "Train Epoch: 372 [400000/697932 (57%)]\tLoss: 172.371250\n",
      "Train Epoch: 372 [500000/697932 (72%)]\tLoss: 169.926813\n",
      "Train Epoch: 372 [600000/697932 (86%)]\tLoss: 171.943406\n",
      "====> Epoch: 372 Average loss: 172.2440\n",
      "====> Test set loss: 173.7759\n",
      "Train Epoch: 373 [0/697932 (0%)]\tLoss: 173.506875\n",
      "Train Epoch: 373 [100000/697932 (14%)]\tLoss: 170.240062\n",
      "Train Epoch: 373 [200000/697932 (29%)]\tLoss: 171.065594\n",
      "Train Epoch: 373 [300000/697932 (43%)]\tLoss: 174.021391\n",
      "Train Epoch: 373 [400000/697932 (57%)]\tLoss: 171.131531\n",
      "Train Epoch: 373 [500000/697932 (72%)]\tLoss: 170.819516\n",
      "Train Epoch: 373 [600000/697932 (86%)]\tLoss: 171.458578\n",
      "====> Epoch: 373 Average loss: 172.0899\n",
      "====> Test set loss: 173.8499\n",
      "Train Epoch: 374 [0/697932 (0%)]\tLoss: 170.401672\n",
      "Train Epoch: 374 [100000/697932 (14%)]\tLoss: 170.665750\n",
      "Train Epoch: 374 [200000/697932 (29%)]\tLoss: 171.140000\n",
      "Train Epoch: 374 [300000/697932 (43%)]\tLoss: 170.837672\n",
      "Train Epoch: 374 [400000/697932 (57%)]\tLoss: 173.526844\n",
      "Train Epoch: 374 [500000/697932 (72%)]\tLoss: 171.352469\n",
      "Train Epoch: 374 [600000/697932 (86%)]\tLoss: 170.843391\n",
      "====> Epoch: 374 Average loss: 172.1153\n",
      "====> Test set loss: 174.1129\n",
      "Train Epoch: 375 [0/697932 (0%)]\tLoss: 173.388531\n",
      "Train Epoch: 375 [100000/697932 (14%)]\tLoss: 169.427531\n",
      "Train Epoch: 375 [200000/697932 (29%)]\tLoss: 173.074547\n",
      "Train Epoch: 375 [300000/697932 (43%)]\tLoss: 171.175672\n",
      "Train Epoch: 375 [400000/697932 (57%)]\tLoss: 173.754562\n",
      "Train Epoch: 375 [500000/697932 (72%)]\tLoss: 173.087469\n",
      "Train Epoch: 375 [600000/697932 (86%)]\tLoss: 171.463984\n",
      "====> Epoch: 375 Average loss: 172.3750\n",
      "====> Test set loss: 173.8620\n",
      "Train Epoch: 376 [0/697932 (0%)]\tLoss: 174.800750\n",
      "Train Epoch: 376 [100000/697932 (14%)]\tLoss: 172.810781\n",
      "Train Epoch: 376 [200000/697932 (29%)]\tLoss: 174.302812\n",
      "Train Epoch: 376 [300000/697932 (43%)]\tLoss: 173.580906\n",
      "Train Epoch: 376 [400000/697932 (57%)]\tLoss: 171.690016\n",
      "Train Epoch: 376 [500000/697932 (72%)]\tLoss: 174.212063\n",
      "Train Epoch: 376 [600000/697932 (86%)]\tLoss: 172.363391\n",
      "====> Epoch: 376 Average loss: 172.1981\n",
      "====> Test set loss: 173.8125\n",
      "Train Epoch: 377 [0/697932 (0%)]\tLoss: 172.387937\n",
      "Train Epoch: 377 [100000/697932 (14%)]\tLoss: 173.435297\n",
      "Train Epoch: 377 [200000/697932 (29%)]\tLoss: 172.886594\n",
      "Train Epoch: 377 [300000/697932 (43%)]\tLoss: 172.912641\n",
      "Train Epoch: 377 [400000/697932 (57%)]\tLoss: 173.841609\n",
      "Train Epoch: 377 [500000/697932 (72%)]\tLoss: 170.170859\n",
      "Train Epoch: 377 [600000/697932 (86%)]\tLoss: 174.060688\n",
      "====> Epoch: 377 Average loss: 172.4628\n",
      "====> Test set loss: 174.2055\n",
      "Train Epoch: 378 [0/697932 (0%)]\tLoss: 168.913500\n",
      "Train Epoch: 378 [100000/697932 (14%)]\tLoss: 170.879516\n",
      "Train Epoch: 378 [200000/697932 (29%)]\tLoss: 173.778234\n",
      "Train Epoch: 378 [300000/697932 (43%)]\tLoss: 174.456156\n",
      "Train Epoch: 378 [400000/697932 (57%)]\tLoss: 172.329094\n",
      "Train Epoch: 378 [500000/697932 (72%)]\tLoss: 169.132719\n",
      "Train Epoch: 378 [600000/697932 (86%)]\tLoss: 173.482109\n",
      "====> Epoch: 378 Average loss: 172.5555\n",
      "====> Test set loss: 174.0206\n",
      "Train Epoch: 379 [0/697932 (0%)]\tLoss: 171.993750\n",
      "Train Epoch: 379 [100000/697932 (14%)]\tLoss: 171.051891\n",
      "Train Epoch: 379 [200000/697932 (29%)]\tLoss: 170.928125\n",
      "Train Epoch: 379 [300000/697932 (43%)]\tLoss: 174.368937\n",
      "Train Epoch: 379 [400000/697932 (57%)]\tLoss: 171.939844\n",
      "Train Epoch: 379 [500000/697932 (72%)]\tLoss: 169.862906\n",
      "Train Epoch: 379 [600000/697932 (86%)]\tLoss: 169.637922\n",
      "====> Epoch: 379 Average loss: 172.2409\n",
      "====> Test set loss: 174.1425\n",
      "Train Epoch: 380 [0/697932 (0%)]\tLoss: 172.056813\n",
      "Train Epoch: 380 [100000/697932 (14%)]\tLoss: 174.686906\n",
      "Train Epoch: 380 [200000/697932 (29%)]\tLoss: 171.537250\n",
      "Train Epoch: 380 [300000/697932 (43%)]\tLoss: 172.492203\n",
      "Train Epoch: 380 [400000/697932 (57%)]\tLoss: 173.261875\n",
      "Train Epoch: 380 [500000/697932 (72%)]\tLoss: 170.294422\n",
      "Train Epoch: 380 [600000/697932 (86%)]\tLoss: 173.031438\n",
      "====> Epoch: 380 Average loss: 172.1747\n",
      "====> Test set loss: 173.9152\n",
      "Train Epoch: 381 [0/697932 (0%)]\tLoss: 173.487891\n",
      "Train Epoch: 381 [100000/697932 (14%)]\tLoss: 172.390344\n",
      "Train Epoch: 381 [200000/697932 (29%)]\tLoss: 170.918250\n",
      "Train Epoch: 381 [300000/697932 (43%)]\tLoss: 171.414813\n",
      "Train Epoch: 381 [400000/697932 (57%)]\tLoss: 172.556578\n",
      "Train Epoch: 381 [500000/697932 (72%)]\tLoss: 175.596281\n",
      "Train Epoch: 381 [600000/697932 (86%)]\tLoss: 173.152984\n",
      "====> Epoch: 381 Average loss: 172.2239\n",
      "====> Test set loss: 174.2213\n",
      "Train Epoch: 382 [0/697932 (0%)]\tLoss: 170.436563\n",
      "Train Epoch: 382 [100000/697932 (14%)]\tLoss: 170.726141\n",
      "Train Epoch: 382 [200000/697932 (29%)]\tLoss: 175.274828\n",
      "Train Epoch: 382 [300000/697932 (43%)]\tLoss: 172.739891\n",
      "Train Epoch: 382 [400000/697932 (57%)]\tLoss: 171.749953\n",
      "Train Epoch: 382 [500000/697932 (72%)]\tLoss: 170.843766\n",
      "Train Epoch: 382 [600000/697932 (86%)]\tLoss: 172.348625\n",
      "====> Epoch: 382 Average loss: 172.3580\n",
      "====> Test set loss: 174.3613\n",
      "Train Epoch: 383 [0/697932 (0%)]\tLoss: 172.618109\n",
      "Train Epoch: 383 [100000/697932 (14%)]\tLoss: 174.874125\n",
      "Train Epoch: 383 [200000/697932 (29%)]\tLoss: 172.167281\n",
      "Train Epoch: 383 [300000/697932 (43%)]\tLoss: 172.556750\n",
      "Train Epoch: 383 [400000/697932 (57%)]\tLoss: 170.124437\n",
      "Train Epoch: 383 [500000/697932 (72%)]\tLoss: 174.065156\n",
      "Train Epoch: 383 [600000/697932 (86%)]\tLoss: 172.363469\n",
      "====> Epoch: 383 Average loss: 172.4441\n",
      "====> Test set loss: 173.8668\n",
      "Train Epoch: 384 [0/697932 (0%)]\tLoss: 173.208969\n",
      "Train Epoch: 384 [100000/697932 (14%)]\tLoss: 173.840734\n",
      "Train Epoch: 384 [200000/697932 (29%)]\tLoss: 171.317578\n",
      "Train Epoch: 384 [300000/697932 (43%)]\tLoss: 172.177188\n",
      "Train Epoch: 384 [400000/697932 (57%)]\tLoss: 172.143937\n",
      "Train Epoch: 384 [500000/697932 (72%)]\tLoss: 173.207812\n",
      "Train Epoch: 384 [600000/697932 (86%)]\tLoss: 175.971375\n",
      "====> Epoch: 384 Average loss: 172.3645\n",
      "====> Test set loss: 174.1622\n",
      "Train Epoch: 385 [0/697932 (0%)]\tLoss: 169.914172\n",
      "Train Epoch: 385 [100000/697932 (14%)]\tLoss: 170.669016\n",
      "Train Epoch: 385 [200000/697932 (29%)]\tLoss: 174.858844\n",
      "Train Epoch: 385 [300000/697932 (43%)]\tLoss: 171.970797\n",
      "Train Epoch: 385 [400000/697932 (57%)]\tLoss: 171.421156\n",
      "Train Epoch: 385 [500000/697932 (72%)]\tLoss: 170.810359\n",
      "Train Epoch: 385 [600000/697932 (86%)]\tLoss: 175.099562\n",
      "====> Epoch: 385 Average loss: 172.7350\n",
      "====> Test set loss: 174.3949\n",
      "Train Epoch: 386 [0/697932 (0%)]\tLoss: 172.840500\n",
      "Train Epoch: 386 [100000/697932 (14%)]\tLoss: 170.839094\n",
      "Train Epoch: 386 [200000/697932 (29%)]\tLoss: 170.279438\n",
      "Train Epoch: 386 [300000/697932 (43%)]\tLoss: 173.197969\n",
      "Train Epoch: 386 [400000/697932 (57%)]\tLoss: 174.907578\n",
      "Train Epoch: 386 [500000/697932 (72%)]\tLoss: 174.062250\n",
      "Train Epoch: 386 [600000/697932 (86%)]\tLoss: 174.569125\n",
      "====> Epoch: 386 Average loss: 172.4986\n",
      "====> Test set loss: 174.1248\n",
      "Train Epoch: 387 [0/697932 (0%)]\tLoss: 170.451375\n",
      "Train Epoch: 387 [100000/697932 (14%)]\tLoss: 173.152109\n",
      "Train Epoch: 387 [200000/697932 (29%)]\tLoss: 170.965234\n",
      "Train Epoch: 387 [300000/697932 (43%)]\tLoss: 170.273250\n",
      "Train Epoch: 387 [400000/697932 (57%)]\tLoss: 173.133719\n",
      "Train Epoch: 387 [500000/697932 (72%)]\tLoss: 169.579781\n",
      "Train Epoch: 387 [600000/697932 (86%)]\tLoss: 172.898156\n",
      "====> Epoch: 387 Average loss: 172.4599\n",
      "====> Test set loss: 174.2538\n",
      "Train Epoch: 388 [0/697932 (0%)]\tLoss: 171.085578\n",
      "Train Epoch: 388 [100000/697932 (14%)]\tLoss: 172.396391\n",
      "Train Epoch: 388 [200000/697932 (29%)]\tLoss: 171.053281\n",
      "Train Epoch: 388 [300000/697932 (43%)]\tLoss: 170.773750\n",
      "Train Epoch: 388 [400000/697932 (57%)]\tLoss: 171.670406\n",
      "Train Epoch: 388 [500000/697932 (72%)]\tLoss: 172.576344\n",
      "Train Epoch: 388 [600000/697932 (86%)]\tLoss: 172.119891\n",
      "====> Epoch: 388 Average loss: 172.4249\n",
      "====> Test set loss: 174.1095\n",
      "Train Epoch: 389 [0/697932 (0%)]\tLoss: 170.625391\n",
      "Train Epoch: 389 [100000/697932 (14%)]\tLoss: 176.309594\n",
      "Train Epoch: 389 [200000/697932 (29%)]\tLoss: 173.957906\n",
      "Train Epoch: 389 [300000/697932 (43%)]\tLoss: 174.020906\n",
      "Train Epoch: 389 [400000/697932 (57%)]\tLoss: 172.493812\n",
      "Train Epoch: 389 [500000/697932 (72%)]\tLoss: 171.175187\n",
      "Train Epoch: 389 [600000/697932 (86%)]\tLoss: 170.730406\n",
      "====> Epoch: 389 Average loss: 172.3674\n",
      "====> Test set loss: 174.1511\n",
      "Train Epoch: 390 [0/697932 (0%)]\tLoss: 173.266719\n",
      "Train Epoch: 390 [100000/697932 (14%)]\tLoss: 171.499625\n",
      "Train Epoch: 390 [200000/697932 (29%)]\tLoss: 172.178094\n",
      "Train Epoch: 390 [300000/697932 (43%)]\tLoss: 171.932500\n",
      "Train Epoch: 390 [400000/697932 (57%)]\tLoss: 170.425766\n",
      "Train Epoch: 390 [500000/697932 (72%)]\tLoss: 172.041672\n",
      "Train Epoch: 390 [600000/697932 (86%)]\tLoss: 172.709391\n",
      "====> Epoch: 390 Average loss: 172.5574\n",
      "====> Test set loss: 174.4535\n",
      "Train Epoch: 391 [0/697932 (0%)]\tLoss: 172.716750\n",
      "Train Epoch: 391 [100000/697932 (14%)]\tLoss: 175.047969\n",
      "Train Epoch: 391 [200000/697932 (29%)]\tLoss: 175.556500\n",
      "Train Epoch: 391 [300000/697932 (43%)]\tLoss: 173.062844\n",
      "Train Epoch: 391 [400000/697932 (57%)]\tLoss: 173.656344\n",
      "Train Epoch: 391 [500000/697932 (72%)]\tLoss: 173.262422\n",
      "Train Epoch: 391 [600000/697932 (86%)]\tLoss: 173.475703\n",
      "====> Epoch: 391 Average loss: 172.4531\n",
      "====> Test set loss: 174.0312\n",
      "Train Epoch: 392 [0/697932 (0%)]\tLoss: 169.968562\n",
      "Train Epoch: 392 [100000/697932 (14%)]\tLoss: 172.551187\n",
      "Train Epoch: 392 [200000/697932 (29%)]\tLoss: 172.015078\n",
      "Train Epoch: 392 [300000/697932 (43%)]\tLoss: 173.420547\n",
      "Train Epoch: 392 [400000/697932 (57%)]\tLoss: 171.029891\n",
      "Train Epoch: 392 [500000/697932 (72%)]\tLoss: 171.668875\n",
      "Train Epoch: 392 [600000/697932 (86%)]\tLoss: 175.898094\n",
      "====> Epoch: 392 Average loss: 172.1303\n",
      "====> Test set loss: 173.9863\n",
      "Train Epoch: 393 [0/697932 (0%)]\tLoss: 172.440391\n",
      "Train Epoch: 393 [100000/697932 (14%)]\tLoss: 170.575859\n",
      "Train Epoch: 393 [200000/697932 (29%)]\tLoss: 174.718859\n",
      "Train Epoch: 393 [300000/697932 (43%)]\tLoss: 173.899297\n",
      "Train Epoch: 393 [400000/697932 (57%)]\tLoss: 172.390156\n",
      "Train Epoch: 393 [500000/697932 (72%)]\tLoss: 169.532594\n",
      "Train Epoch: 393 [600000/697932 (86%)]\tLoss: 172.775484\n",
      "====> Epoch: 393 Average loss: 172.1977\n",
      "====> Test set loss: 173.7253\n",
      "Train Epoch: 394 [0/697932 (0%)]\tLoss: 169.322797\n",
      "Train Epoch: 394 [100000/697932 (14%)]\tLoss: 173.902141\n",
      "Train Epoch: 394 [200000/697932 (29%)]\tLoss: 170.266703\n",
      "Train Epoch: 394 [300000/697932 (43%)]\tLoss: 172.404844\n",
      "Train Epoch: 394 [400000/697932 (57%)]\tLoss: 172.029094\n",
      "Train Epoch: 394 [500000/697932 (72%)]\tLoss: 172.224953\n",
      "Train Epoch: 394 [600000/697932 (86%)]\tLoss: 169.917234\n",
      "====> Epoch: 394 Average loss: 172.0467\n",
      "====> Test set loss: 174.1465\n",
      "Train Epoch: 395 [0/697932 (0%)]\tLoss: 172.585187\n",
      "Train Epoch: 395 [100000/697932 (14%)]\tLoss: 173.884609\n",
      "Train Epoch: 395 [200000/697932 (29%)]\tLoss: 173.653875\n",
      "Train Epoch: 395 [300000/697932 (43%)]\tLoss: 173.353391\n",
      "Train Epoch: 395 [400000/697932 (57%)]\tLoss: 170.990938\n",
      "Train Epoch: 395 [500000/697932 (72%)]\tLoss: 169.105125\n",
      "Train Epoch: 395 [600000/697932 (86%)]\tLoss: 171.875500\n",
      "====> Epoch: 395 Average loss: 172.3178\n",
      "====> Test set loss: 173.9884\n",
      "Train Epoch: 396 [0/697932 (0%)]\tLoss: 172.002719\n",
      "Train Epoch: 396 [100000/697932 (14%)]\tLoss: 172.443312\n",
      "Train Epoch: 396 [200000/697932 (29%)]\tLoss: 171.664406\n",
      "Train Epoch: 396 [300000/697932 (43%)]\tLoss: 172.278219\n",
      "Train Epoch: 396 [400000/697932 (57%)]\tLoss: 171.807813\n",
      "Train Epoch: 396 [500000/697932 (72%)]\tLoss: 173.236016\n",
      "Train Epoch: 396 [600000/697932 (86%)]\tLoss: 173.406156\n",
      "====> Epoch: 396 Average loss: 172.2778\n",
      "====> Test set loss: 174.0988\n",
      "Train Epoch: 397 [0/697932 (0%)]\tLoss: 172.142812\n",
      "Train Epoch: 397 [100000/697932 (14%)]\tLoss: 173.663109\n",
      "Train Epoch: 397 [200000/697932 (29%)]\tLoss: 171.575906\n",
      "Train Epoch: 397 [300000/697932 (43%)]\tLoss: 171.229703\n",
      "Train Epoch: 397 [400000/697932 (57%)]\tLoss: 178.512641\n",
      "Train Epoch: 397 [500000/697932 (72%)]\tLoss: 173.305656\n",
      "Train Epoch: 397 [600000/697932 (86%)]\tLoss: 173.559484\n",
      "====> Epoch: 397 Average loss: 172.2246\n",
      "====> Test set loss: 173.9468\n",
      "Train Epoch: 398 [0/697932 (0%)]\tLoss: 178.222813\n",
      "Train Epoch: 398 [100000/697932 (14%)]\tLoss: 170.644203\n",
      "Train Epoch: 398 [200000/697932 (29%)]\tLoss: 173.081891\n",
      "Train Epoch: 398 [300000/697932 (43%)]\tLoss: 175.161031\n",
      "Train Epoch: 398 [400000/697932 (57%)]\tLoss: 174.855187\n",
      "Train Epoch: 398 [500000/697932 (72%)]\tLoss: 172.931437\n",
      "Train Epoch: 398 [600000/697932 (86%)]\tLoss: 169.650469\n",
      "====> Epoch: 398 Average loss: 172.2662\n",
      "====> Test set loss: 174.3418\n",
      "Train Epoch: 399 [0/697932 (0%)]\tLoss: 174.145031\n",
      "Train Epoch: 399 [100000/697932 (14%)]\tLoss: 170.927984\n",
      "Train Epoch: 399 [200000/697932 (29%)]\tLoss: 174.811750\n",
      "Train Epoch: 399 [300000/697932 (43%)]\tLoss: 173.113156\n",
      "Train Epoch: 399 [400000/697932 (57%)]\tLoss: 172.033437\n",
      "Train Epoch: 399 [500000/697932 (72%)]\tLoss: 174.486078\n",
      "Train Epoch: 399 [600000/697932 (86%)]\tLoss: 173.082063\n",
      "====> Epoch: 399 Average loss: 172.2319\n",
      "====> Test set loss: 173.8571\n",
      "Train Epoch: 400 [0/697932 (0%)]\tLoss: 174.265469\n",
      "Train Epoch: 400 [100000/697932 (14%)]\tLoss: 171.873969\n",
      "Train Epoch: 400 [200000/697932 (29%)]\tLoss: 168.902453\n",
      "Train Epoch: 400 [300000/697932 (43%)]\tLoss: 171.374266\n",
      "Train Epoch: 400 [400000/697932 (57%)]\tLoss: 169.924906\n",
      "Train Epoch: 400 [500000/697932 (72%)]\tLoss: 173.207859\n",
      "Train Epoch: 400 [600000/697932 (86%)]\tLoss: 173.568859\n",
      "====> Epoch: 400 Average loss: 172.1102\n",
      "====> Test set loss: 173.7804\n",
      "Train Epoch: 401 [0/697932 (0%)]\tLoss: 173.209156\n",
      "Train Epoch: 401 [100000/697932 (14%)]\tLoss: 172.707938\n",
      "Train Epoch: 401 [200000/697932 (29%)]\tLoss: 172.275781\n",
      "Train Epoch: 401 [300000/697932 (43%)]\tLoss: 171.421750\n",
      "Train Epoch: 401 [400000/697932 (57%)]\tLoss: 174.086797\n",
      "Train Epoch: 401 [500000/697932 (72%)]\tLoss: 171.733875\n",
      "Train Epoch: 401 [600000/697932 (86%)]\tLoss: 171.479625\n",
      "====> Epoch: 401 Average loss: 171.9053\n",
      "====> Test set loss: 173.7398\n",
      "Train Epoch: 402 [0/697932 (0%)]\tLoss: 169.086594\n",
      "Train Epoch: 402 [100000/697932 (14%)]\tLoss: 172.515938\n",
      "Train Epoch: 402 [200000/697932 (29%)]\tLoss: 171.307875\n",
      "Train Epoch: 402 [300000/697932 (43%)]\tLoss: 170.600406\n",
      "Train Epoch: 402 [400000/697932 (57%)]\tLoss: 174.879047\n",
      "Train Epoch: 402 [500000/697932 (72%)]\tLoss: 174.338031\n",
      "Train Epoch: 402 [600000/697932 (86%)]\tLoss: 175.114547\n",
      "====> Epoch: 402 Average loss: 172.0855\n",
      "====> Test set loss: 173.7192\n",
      "Train Epoch: 403 [0/697932 (0%)]\tLoss: 170.841672\n",
      "Train Epoch: 403 [100000/697932 (14%)]\tLoss: 171.623531\n",
      "Train Epoch: 403 [200000/697932 (29%)]\tLoss: 172.702141\n",
      "Train Epoch: 403 [300000/697932 (43%)]\tLoss: 173.517453\n",
      "Train Epoch: 403 [400000/697932 (57%)]\tLoss: 172.653750\n",
      "Train Epoch: 403 [500000/697932 (72%)]\tLoss: 170.426906\n",
      "Train Epoch: 403 [600000/697932 (86%)]\tLoss: 172.819891\n",
      "====> Epoch: 403 Average loss: 172.1729\n",
      "====> Test set loss: 173.8145\n",
      "Train Epoch: 404 [0/697932 (0%)]\tLoss: 166.402172\n",
      "Train Epoch: 404 [100000/697932 (14%)]\tLoss: 171.092328\n",
      "Train Epoch: 404 [200000/697932 (29%)]\tLoss: 172.198438\n",
      "Train Epoch: 404 [300000/697932 (43%)]\tLoss: 167.312734\n",
      "Train Epoch: 404 [400000/697932 (57%)]\tLoss: 172.731547\n",
      "Train Epoch: 404 [500000/697932 (72%)]\tLoss: 172.341813\n",
      "Train Epoch: 404 [600000/697932 (86%)]\tLoss: 170.858734\n",
      "====> Epoch: 404 Average loss: 172.0139\n",
      "====> Test set loss: 173.6973\n",
      "Train Epoch: 405 [0/697932 (0%)]\tLoss: 171.904656\n",
      "Train Epoch: 405 [100000/697932 (14%)]\tLoss: 173.556500\n",
      "Train Epoch: 405 [200000/697932 (29%)]\tLoss: 168.474516\n",
      "Train Epoch: 405 [300000/697932 (43%)]\tLoss: 171.211844\n",
      "Train Epoch: 405 [400000/697932 (57%)]\tLoss: 169.657172\n",
      "Train Epoch: 405 [500000/697932 (72%)]\tLoss: 173.282391\n",
      "Train Epoch: 405 [600000/697932 (86%)]\tLoss: 171.272063\n",
      "====> Epoch: 405 Average loss: 171.9410\n",
      "====> Test set loss: 173.5445\n",
      "Train Epoch: 406 [0/697932 (0%)]\tLoss: 169.073672\n",
      "Train Epoch: 406 [100000/697932 (14%)]\tLoss: 170.794828\n",
      "Train Epoch: 406 [200000/697932 (29%)]\tLoss: 169.925656\n",
      "Train Epoch: 406 [300000/697932 (43%)]\tLoss: 171.933266\n",
      "Train Epoch: 406 [400000/697932 (57%)]\tLoss: 170.911703\n",
      "Train Epoch: 406 [500000/697932 (72%)]\tLoss: 171.428188\n",
      "Train Epoch: 406 [600000/697932 (86%)]\tLoss: 168.855172\n",
      "====> Epoch: 406 Average loss: 171.9956\n",
      "====> Test set loss: 173.8011\n",
      "Train Epoch: 407 [0/697932 (0%)]\tLoss: 166.989484\n",
      "Train Epoch: 407 [100000/697932 (14%)]\tLoss: 171.403219\n",
      "Train Epoch: 407 [200000/697932 (29%)]\tLoss: 170.835906\n",
      "Train Epoch: 407 [300000/697932 (43%)]\tLoss: 171.095016\n",
      "Train Epoch: 407 [400000/697932 (57%)]\tLoss: 173.815078\n",
      "Train Epoch: 407 [500000/697932 (72%)]\tLoss: 171.430156\n",
      "Train Epoch: 407 [600000/697932 (86%)]\tLoss: 169.932484\n",
      "====> Epoch: 407 Average loss: 171.8265\n",
      "====> Test set loss: 173.6654\n",
      "Train Epoch: 408 [0/697932 (0%)]\tLoss: 174.553703\n",
      "Train Epoch: 408 [100000/697932 (14%)]\tLoss: 170.600641\n",
      "Train Epoch: 408 [200000/697932 (29%)]\tLoss: 171.435297\n",
      "Train Epoch: 408 [300000/697932 (43%)]\tLoss: 173.021141\n",
      "Train Epoch: 408 [400000/697932 (57%)]\tLoss: 172.423859\n",
      "Train Epoch: 408 [500000/697932 (72%)]\tLoss: 171.933750\n",
      "Train Epoch: 408 [600000/697932 (86%)]\tLoss: 170.110219\n",
      "====> Epoch: 408 Average loss: 171.9088\n",
      "====> Test set loss: 173.9101\n",
      "Train Epoch: 409 [0/697932 (0%)]\tLoss: 173.996359\n",
      "Train Epoch: 409 [100000/697932 (14%)]\tLoss: 172.869969\n",
      "Train Epoch: 409 [200000/697932 (29%)]\tLoss: 169.650281\n",
      "Train Epoch: 409 [300000/697932 (43%)]\tLoss: 172.903281\n",
      "Train Epoch: 409 [400000/697932 (57%)]\tLoss: 171.008625\n",
      "Train Epoch: 409 [500000/697932 (72%)]\tLoss: 174.563437\n",
      "Train Epoch: 409 [600000/697932 (86%)]\tLoss: 174.256609\n",
      "====> Epoch: 409 Average loss: 171.8894\n",
      "====> Test set loss: 173.6392\n",
      "Train Epoch: 410 [0/697932 (0%)]\tLoss: 168.540406\n",
      "Train Epoch: 410 [100000/697932 (14%)]\tLoss: 169.982844\n",
      "Train Epoch: 410 [200000/697932 (29%)]\tLoss: 172.986906\n",
      "Train Epoch: 410 [300000/697932 (43%)]\tLoss: 169.714922\n",
      "Train Epoch: 410 [400000/697932 (57%)]\tLoss: 170.195312\n",
      "Train Epoch: 410 [500000/697932 (72%)]\tLoss: 173.179422\n",
      "Train Epoch: 410 [600000/697932 (86%)]\tLoss: 173.270813\n",
      "====> Epoch: 410 Average loss: 171.7593\n",
      "====> Test set loss: 173.6794\n",
      "Train Epoch: 411 [0/697932 (0%)]\tLoss: 172.394797\n",
      "Train Epoch: 411 [100000/697932 (14%)]\tLoss: 167.685922\n",
      "Train Epoch: 411 [200000/697932 (29%)]\tLoss: 171.274625\n",
      "Train Epoch: 411 [300000/697932 (43%)]\tLoss: 173.794391\n",
      "Train Epoch: 411 [400000/697932 (57%)]\tLoss: 170.985344\n",
      "Train Epoch: 411 [500000/697932 (72%)]\tLoss: 170.805813\n",
      "Train Epoch: 411 [600000/697932 (86%)]\tLoss: 169.504375\n",
      "====> Epoch: 411 Average loss: 171.7695\n",
      "====> Test set loss: 173.7361\n",
      "Train Epoch: 412 [0/697932 (0%)]\tLoss: 172.355469\n",
      "Train Epoch: 412 [100000/697932 (14%)]\tLoss: 172.555156\n",
      "Train Epoch: 412 [200000/697932 (29%)]\tLoss: 173.904531\n",
      "Train Epoch: 412 [300000/697932 (43%)]\tLoss: 171.400937\n",
      "Train Epoch: 412 [400000/697932 (57%)]\tLoss: 169.459234\n",
      "Train Epoch: 412 [500000/697932 (72%)]\tLoss: 174.569750\n",
      "Train Epoch: 412 [600000/697932 (86%)]\tLoss: 171.041984\n",
      "====> Epoch: 412 Average loss: 171.9889\n",
      "====> Test set loss: 173.6889\n",
      "Train Epoch: 413 [0/697932 (0%)]\tLoss: 172.389453\n",
      "Train Epoch: 413 [100000/697932 (14%)]\tLoss: 173.413125\n",
      "Train Epoch: 413 [200000/697932 (29%)]\tLoss: 172.643781\n",
      "Train Epoch: 413 [300000/697932 (43%)]\tLoss: 172.630812\n",
      "Train Epoch: 413 [400000/697932 (57%)]\tLoss: 173.624297\n",
      "Train Epoch: 413 [500000/697932 (72%)]\tLoss: 172.787891\n",
      "Train Epoch: 413 [600000/697932 (86%)]\tLoss: 171.095344\n",
      "====> Epoch: 413 Average loss: 171.6998\n",
      "====> Test set loss: 173.6180\n",
      "Train Epoch: 414 [0/697932 (0%)]\tLoss: 174.156406\n",
      "Train Epoch: 414 [100000/697932 (14%)]\tLoss: 168.170688\n",
      "Train Epoch: 414 [200000/697932 (29%)]\tLoss: 171.375516\n",
      "Train Epoch: 414 [300000/697932 (43%)]\tLoss: 168.254828\n",
      "Train Epoch: 414 [400000/697932 (57%)]\tLoss: 169.087875\n",
      "Train Epoch: 414 [500000/697932 (72%)]\tLoss: 171.977641\n",
      "Train Epoch: 414 [600000/697932 (86%)]\tLoss: 171.887781\n",
      "====> Epoch: 414 Average loss: 171.7160\n",
      "====> Test set loss: 173.2090\n",
      "Train Epoch: 415 [0/697932 (0%)]\tLoss: 169.287391\n",
      "Train Epoch: 415 [100000/697932 (14%)]\tLoss: 171.322922\n",
      "Train Epoch: 415 [200000/697932 (29%)]\tLoss: 173.015203\n",
      "Train Epoch: 415 [300000/697932 (43%)]\tLoss: 172.674344\n",
      "Train Epoch: 415 [400000/697932 (57%)]\tLoss: 175.222234\n",
      "Train Epoch: 415 [500000/697932 (72%)]\tLoss: 170.958719\n",
      "Train Epoch: 415 [600000/697932 (86%)]\tLoss: 172.159172\n",
      "====> Epoch: 415 Average loss: 171.7181\n",
      "====> Test set loss: 173.4032\n",
      "Train Epoch: 416 [0/697932 (0%)]\tLoss: 170.039906\n",
      "Train Epoch: 416 [100000/697932 (14%)]\tLoss: 173.088859\n",
      "Train Epoch: 416 [200000/697932 (29%)]\tLoss: 172.967922\n",
      "Train Epoch: 416 [300000/697932 (43%)]\tLoss: 172.772922\n",
      "Train Epoch: 416 [400000/697932 (57%)]\tLoss: 168.798281\n",
      "Train Epoch: 416 [500000/697932 (72%)]\tLoss: 172.380844\n",
      "Train Epoch: 416 [600000/697932 (86%)]\tLoss: 170.282688\n",
      "====> Epoch: 416 Average loss: 171.6740\n",
      "====> Test set loss: 173.4540\n",
      "Train Epoch: 417 [0/697932 (0%)]\tLoss: 171.712234\n",
      "Train Epoch: 417 [100000/697932 (14%)]\tLoss: 171.181969\n",
      "Train Epoch: 417 [200000/697932 (29%)]\tLoss: 169.311469\n",
      "Train Epoch: 417 [300000/697932 (43%)]\tLoss: 172.452500\n",
      "Train Epoch: 417 [400000/697932 (57%)]\tLoss: 167.695328\n",
      "Train Epoch: 417 [500000/697932 (72%)]\tLoss: 167.040141\n",
      "Train Epoch: 417 [600000/697932 (86%)]\tLoss: 172.865547\n",
      "====> Epoch: 417 Average loss: 171.6197\n",
      "====> Test set loss: 173.5718\n",
      "Train Epoch: 418 [0/697932 (0%)]\tLoss: 172.092281\n",
      "Train Epoch: 418 [100000/697932 (14%)]\tLoss: 173.348625\n",
      "Train Epoch: 418 [200000/697932 (29%)]\tLoss: 170.249406\n",
      "Train Epoch: 418 [300000/697932 (43%)]\tLoss: 170.991844\n",
      "Train Epoch: 418 [400000/697932 (57%)]\tLoss: 171.183938\n",
      "Train Epoch: 418 [500000/697932 (72%)]\tLoss: 173.263047\n",
      "Train Epoch: 418 [600000/697932 (86%)]\tLoss: 172.714719\n",
      "====> Epoch: 418 Average loss: 171.6474\n",
      "====> Test set loss: 173.3662\n",
      "Train Epoch: 419 [0/697932 (0%)]\tLoss: 170.173266\n",
      "Train Epoch: 419 [100000/697932 (14%)]\tLoss: 172.192969\n",
      "Train Epoch: 419 [200000/697932 (29%)]\tLoss: 172.099578\n",
      "Train Epoch: 419 [300000/697932 (43%)]\tLoss: 171.430828\n",
      "Train Epoch: 419 [400000/697932 (57%)]\tLoss: 173.426516\n",
      "Train Epoch: 419 [500000/697932 (72%)]\tLoss: 169.873906\n",
      "Train Epoch: 419 [600000/697932 (86%)]\tLoss: 172.326234\n",
      "====> Epoch: 419 Average loss: 171.8153\n",
      "====> Test set loss: 173.4533\n",
      "Train Epoch: 420 [0/697932 (0%)]\tLoss: 175.648797\n",
      "Train Epoch: 420 [100000/697932 (14%)]\tLoss: 171.032766\n",
      "Train Epoch: 420 [200000/697932 (29%)]\tLoss: 170.329469\n",
      "Train Epoch: 420 [300000/697932 (43%)]\tLoss: 171.963453\n",
      "Train Epoch: 420 [400000/697932 (57%)]\tLoss: 172.367234\n",
      "Train Epoch: 420 [500000/697932 (72%)]\tLoss: 170.452625\n",
      "Train Epoch: 420 [600000/697932 (86%)]\tLoss: 168.514531\n",
      "====> Epoch: 420 Average loss: 171.6110\n",
      "====> Test set loss: 173.5442\n",
      "Train Epoch: 421 [0/697932 (0%)]\tLoss: 170.708984\n",
      "Train Epoch: 421 [100000/697932 (14%)]\tLoss: 172.158250\n",
      "Train Epoch: 421 [200000/697932 (29%)]\tLoss: 173.428609\n",
      "Train Epoch: 421 [300000/697932 (43%)]\tLoss: 171.276375\n",
      "Train Epoch: 421 [400000/697932 (57%)]\tLoss: 172.228719\n",
      "Train Epoch: 421 [500000/697932 (72%)]\tLoss: 170.969844\n",
      "Train Epoch: 421 [600000/697932 (86%)]\tLoss: 171.068453\n",
      "====> Epoch: 421 Average loss: 171.7179\n",
      "====> Test set loss: 173.3501\n",
      "Train Epoch: 422 [0/697932 (0%)]\tLoss: 170.190078\n",
      "Train Epoch: 422 [100000/697932 (14%)]\tLoss: 173.572531\n",
      "Train Epoch: 422 [200000/697932 (29%)]\tLoss: 169.369469\n",
      "Train Epoch: 422 [300000/697932 (43%)]\tLoss: 172.352844\n",
      "Train Epoch: 422 [400000/697932 (57%)]\tLoss: 169.211781\n",
      "Train Epoch: 422 [500000/697932 (72%)]\tLoss: 169.627453\n",
      "Train Epoch: 422 [600000/697932 (86%)]\tLoss: 172.732281\n",
      "====> Epoch: 422 Average loss: 171.5700\n",
      "====> Test set loss: 173.5473\n",
      "Train Epoch: 423 [0/697932 (0%)]\tLoss: 170.111000\n",
      "Train Epoch: 423 [100000/697932 (14%)]\tLoss: 173.289437\n",
      "Train Epoch: 423 [200000/697932 (29%)]\tLoss: 176.300578\n",
      "Train Epoch: 423 [300000/697932 (43%)]\tLoss: 171.197000\n",
      "Train Epoch: 423 [400000/697932 (57%)]\tLoss: 171.271844\n",
      "Train Epoch: 423 [500000/697932 (72%)]\tLoss: 171.277000\n",
      "Train Epoch: 423 [600000/697932 (86%)]\tLoss: 170.516828\n",
      "====> Epoch: 423 Average loss: 171.6041\n",
      "====> Test set loss: 173.3874\n",
      "Train Epoch: 424 [0/697932 (0%)]\tLoss: 170.924516\n",
      "Train Epoch: 424 [100000/697932 (14%)]\tLoss: 171.744516\n",
      "Train Epoch: 424 [200000/697932 (29%)]\tLoss: 173.868094\n",
      "Train Epoch: 424 [300000/697932 (43%)]\tLoss: 170.261719\n",
      "Train Epoch: 424 [400000/697932 (57%)]\tLoss: 171.331984\n",
      "Train Epoch: 424 [500000/697932 (72%)]\tLoss: 171.969984\n",
      "Train Epoch: 424 [600000/697932 (86%)]\tLoss: 170.699734\n",
      "====> Epoch: 424 Average loss: 171.8271\n",
      "====> Test set loss: 174.0343\n",
      "Train Epoch: 425 [0/697932 (0%)]\tLoss: 171.386484\n",
      "Train Epoch: 425 [100000/697932 (14%)]\tLoss: 171.060000\n",
      "Train Epoch: 425 [200000/697932 (29%)]\tLoss: 173.420453\n",
      "Train Epoch: 425 [300000/697932 (43%)]\tLoss: 170.184250\n",
      "Train Epoch: 425 [400000/697932 (57%)]\tLoss: 171.642703\n",
      "Train Epoch: 425 [500000/697932 (72%)]\tLoss: 171.164078\n",
      "Train Epoch: 425 [600000/697932 (86%)]\tLoss: 170.161859\n",
      "====> Epoch: 425 Average loss: 171.7320\n",
      "====> Test set loss: 173.5599\n",
      "Train Epoch: 426 [0/697932 (0%)]\tLoss: 169.873813\n",
      "Train Epoch: 426 [100000/697932 (14%)]\tLoss: 171.602484\n",
      "Train Epoch: 426 [200000/697932 (29%)]\tLoss: 171.860328\n",
      "Train Epoch: 426 [300000/697932 (43%)]\tLoss: 172.799828\n",
      "Train Epoch: 426 [400000/697932 (57%)]\tLoss: 169.435078\n",
      "Train Epoch: 426 [500000/697932 (72%)]\tLoss: 172.167406\n",
      "Train Epoch: 426 [600000/697932 (86%)]\tLoss: 173.130516\n",
      "====> Epoch: 426 Average loss: 171.7611\n",
      "====> Test set loss: 173.4517\n",
      "Train Epoch: 427 [0/697932 (0%)]\tLoss: 170.509219\n",
      "Train Epoch: 427 [100000/697932 (14%)]\tLoss: 170.434391\n",
      "Train Epoch: 427 [200000/697932 (29%)]\tLoss: 169.454281\n",
      "Train Epoch: 427 [300000/697932 (43%)]\tLoss: 173.474125\n",
      "Train Epoch: 427 [400000/697932 (57%)]\tLoss: 173.988250\n",
      "Train Epoch: 427 [500000/697932 (72%)]\tLoss: 170.945406\n",
      "Train Epoch: 427 [600000/697932 (86%)]\tLoss: 173.600547\n",
      "====> Epoch: 427 Average loss: 171.8559\n",
      "====> Test set loss: 173.4464\n",
      "Train Epoch: 428 [0/697932 (0%)]\tLoss: 172.052531\n",
      "Train Epoch: 428 [100000/697932 (14%)]\tLoss: 171.495406\n",
      "Train Epoch: 428 [200000/697932 (29%)]\tLoss: 168.395109\n",
      "Train Epoch: 428 [300000/697932 (43%)]\tLoss: 175.349906\n",
      "Train Epoch: 428 [400000/697932 (57%)]\tLoss: 170.675187\n",
      "Train Epoch: 428 [500000/697932 (72%)]\tLoss: 175.087484\n",
      "Train Epoch: 428 [600000/697932 (86%)]\tLoss: 170.976031\n",
      "====> Epoch: 428 Average loss: 171.6499\n",
      "====> Test set loss: 173.5363\n",
      "Train Epoch: 429 [0/697932 (0%)]\tLoss: 168.778531\n",
      "Train Epoch: 429 [100000/697932 (14%)]\tLoss: 171.262266\n",
      "Train Epoch: 429 [200000/697932 (29%)]\tLoss: 169.952359\n",
      "Train Epoch: 429 [300000/697932 (43%)]\tLoss: 170.863438\n",
      "Train Epoch: 429 [400000/697932 (57%)]\tLoss: 171.327063\n",
      "Train Epoch: 429 [500000/697932 (72%)]\tLoss: 173.579859\n",
      "Train Epoch: 429 [600000/697932 (86%)]\tLoss: 173.870391\n",
      "====> Epoch: 429 Average loss: 172.0046\n",
      "====> Test set loss: 173.3244\n",
      "Train Epoch: 430 [0/697932 (0%)]\tLoss: 171.287953\n",
      "Train Epoch: 430 [100000/697932 (14%)]\tLoss: 170.483844\n",
      "Train Epoch: 430 [200000/697932 (29%)]\tLoss: 171.829375\n",
      "Train Epoch: 430 [300000/697932 (43%)]\tLoss: 172.329922\n",
      "Train Epoch: 430 [400000/697932 (57%)]\tLoss: 174.363453\n",
      "Train Epoch: 430 [500000/697932 (72%)]\tLoss: 171.583141\n",
      "Train Epoch: 430 [600000/697932 (86%)]\tLoss: 171.877578\n",
      "====> Epoch: 430 Average loss: 171.7006\n",
      "====> Test set loss: 173.4494\n",
      "Train Epoch: 431 [0/697932 (0%)]\tLoss: 173.610000\n",
      "Train Epoch: 431 [100000/697932 (14%)]\tLoss: 175.183406\n",
      "Train Epoch: 431 [200000/697932 (29%)]\tLoss: 176.201219\n",
      "Train Epoch: 431 [300000/697932 (43%)]\tLoss: 169.586094\n",
      "Train Epoch: 431 [400000/697932 (57%)]\tLoss: 166.009125\n",
      "Train Epoch: 431 [500000/697932 (72%)]\tLoss: 171.647031\n",
      "Train Epoch: 431 [600000/697932 (86%)]\tLoss: 170.187047\n",
      "====> Epoch: 431 Average loss: 171.7021\n",
      "====> Test set loss: 173.5171\n",
      "Train Epoch: 432 [0/697932 (0%)]\tLoss: 173.480297\n",
      "Train Epoch: 432 [100000/697932 (14%)]\tLoss: 171.250141\n",
      "Train Epoch: 432 [200000/697932 (29%)]\tLoss: 174.311453\n",
      "Train Epoch: 432 [300000/697932 (43%)]\tLoss: 170.162109\n",
      "Train Epoch: 432 [400000/697932 (57%)]\tLoss: 169.010125\n",
      "Train Epoch: 432 [500000/697932 (72%)]\tLoss: 172.052766\n",
      "Train Epoch: 432 [600000/697932 (86%)]\tLoss: 174.510219\n",
      "====> Epoch: 432 Average loss: 171.9396\n",
      "====> Test set loss: 173.8568\n",
      "Train Epoch: 433 [0/697932 (0%)]\tLoss: 172.663938\n",
      "Train Epoch: 433 [100000/697932 (14%)]\tLoss: 171.615266\n",
      "Train Epoch: 433 [200000/697932 (29%)]\tLoss: 173.121594\n",
      "Train Epoch: 433 [300000/697932 (43%)]\tLoss: 170.788875\n",
      "Train Epoch: 433 [400000/697932 (57%)]\tLoss: 170.897531\n",
      "Train Epoch: 433 [500000/697932 (72%)]\tLoss: 172.799578\n",
      "Train Epoch: 433 [600000/697932 (86%)]\tLoss: 171.625969\n",
      "====> Epoch: 433 Average loss: 171.9116\n",
      "====> Test set loss: 173.5860\n",
      "Train Epoch: 434 [0/697932 (0%)]\tLoss: 170.106406\n",
      "Train Epoch: 434 [100000/697932 (14%)]\tLoss: 174.166984\n",
      "Train Epoch: 434 [200000/697932 (29%)]\tLoss: 171.457078\n",
      "Train Epoch: 434 [300000/697932 (43%)]\tLoss: 171.964766\n",
      "Train Epoch: 434 [400000/697932 (57%)]\tLoss: 175.532125\n",
      "Train Epoch: 434 [500000/697932 (72%)]\tLoss: 172.478172\n",
      "Train Epoch: 434 [600000/697932 (86%)]\tLoss: 172.724187\n",
      "====> Epoch: 434 Average loss: 172.0155\n",
      "====> Test set loss: 173.8790\n",
      "Train Epoch: 435 [0/697932 (0%)]\tLoss: 171.268203\n",
      "Train Epoch: 435 [100000/697932 (14%)]\tLoss: 171.708375\n",
      "Train Epoch: 435 [200000/697932 (29%)]\tLoss: 171.895219\n",
      "Train Epoch: 435 [300000/697932 (43%)]\tLoss: 170.343141\n",
      "Train Epoch: 435 [400000/697932 (57%)]\tLoss: 169.941375\n",
      "Train Epoch: 435 [500000/697932 (72%)]\tLoss: 171.141422\n",
      "Train Epoch: 435 [600000/697932 (86%)]\tLoss: 170.862719\n",
      "====> Epoch: 435 Average loss: 171.8068\n",
      "====> Test set loss: 173.4717\n",
      "Train Epoch: 436 [0/697932 (0%)]\tLoss: 169.124078\n",
      "Train Epoch: 436 [100000/697932 (14%)]\tLoss: 173.288016\n",
      "Train Epoch: 436 [200000/697932 (29%)]\tLoss: 168.242922\n",
      "Train Epoch: 436 [300000/697932 (43%)]\tLoss: 174.943750\n",
      "Train Epoch: 436 [400000/697932 (57%)]\tLoss: 173.098859\n",
      "Train Epoch: 436 [500000/697932 (72%)]\tLoss: 171.201859\n",
      "Train Epoch: 436 [600000/697932 (86%)]\tLoss: 173.548375\n",
      "====> Epoch: 436 Average loss: 171.8384\n",
      "====> Test set loss: 173.8384\n",
      "Train Epoch: 437 [0/697932 (0%)]\tLoss: 171.923516\n",
      "Train Epoch: 437 [100000/697932 (14%)]\tLoss: 171.690812\n",
      "Train Epoch: 437 [200000/697932 (29%)]\tLoss: 169.218687\n",
      "Train Epoch: 437 [300000/697932 (43%)]\tLoss: 169.015844\n",
      "Train Epoch: 437 [400000/697932 (57%)]\tLoss: 172.095219\n",
      "Train Epoch: 437 [500000/697932 (72%)]\tLoss: 168.784531\n",
      "Train Epoch: 437 [600000/697932 (86%)]\tLoss: 170.050437\n",
      "====> Epoch: 437 Average loss: 171.9505\n",
      "====> Test set loss: 173.8465\n",
      "Train Epoch: 438 [0/697932 (0%)]\tLoss: 170.524906\n",
      "Train Epoch: 438 [100000/697932 (14%)]\tLoss: 170.650813\n",
      "Train Epoch: 438 [200000/697932 (29%)]\tLoss: 171.770859\n",
      "Train Epoch: 438 [300000/697932 (43%)]\tLoss: 168.004625\n",
      "Train Epoch: 438 [400000/697932 (57%)]\tLoss: 174.986906\n",
      "Train Epoch: 438 [500000/697932 (72%)]\tLoss: 170.050219\n",
      "Train Epoch: 438 [600000/697932 (86%)]\tLoss: 171.604625\n",
      "====> Epoch: 438 Average loss: 171.8964\n",
      "====> Test set loss: 173.8237\n",
      "Train Epoch: 439 [0/697932 (0%)]\tLoss: 173.159078\n",
      "Train Epoch: 439 [100000/697932 (14%)]\tLoss: 167.189953\n",
      "Train Epoch: 439 [200000/697932 (29%)]\tLoss: 169.061563\n",
      "Train Epoch: 439 [300000/697932 (43%)]\tLoss: 170.482609\n",
      "Train Epoch: 439 [400000/697932 (57%)]\tLoss: 174.162500\n",
      "Train Epoch: 439 [500000/697932 (72%)]\tLoss: 174.039484\n",
      "Train Epoch: 439 [600000/697932 (86%)]\tLoss: 174.592203\n",
      "====> Epoch: 439 Average loss: 171.8855\n",
      "====> Test set loss: 173.8513\n",
      "Train Epoch: 440 [0/697932 (0%)]\tLoss: 172.594922\n",
      "Train Epoch: 440 [100000/697932 (14%)]\tLoss: 170.830187\n",
      "Train Epoch: 440 [200000/697932 (29%)]\tLoss: 172.895172\n",
      "Train Epoch: 440 [300000/697932 (43%)]\tLoss: 171.556719\n",
      "Train Epoch: 440 [400000/697932 (57%)]\tLoss: 170.259156\n",
      "Train Epoch: 440 [500000/697932 (72%)]\tLoss: 173.043016\n",
      "Train Epoch: 440 [600000/697932 (86%)]\tLoss: 171.799172\n",
      "====> Epoch: 440 Average loss: 171.8148\n",
      "====> Test set loss: 173.4838\n",
      "Train Epoch: 441 [0/697932 (0%)]\tLoss: 171.513859\n",
      "Train Epoch: 441 [100000/697932 (14%)]\tLoss: 174.396078\n",
      "Train Epoch: 441 [200000/697932 (29%)]\tLoss: 170.562219\n",
      "Train Epoch: 441 [300000/697932 (43%)]\tLoss: 172.197937\n",
      "Train Epoch: 441 [400000/697932 (57%)]\tLoss: 171.777375\n",
      "Train Epoch: 441 [500000/697932 (72%)]\tLoss: 173.462016\n",
      "Train Epoch: 441 [600000/697932 (86%)]\tLoss: 171.820578\n",
      "====> Epoch: 441 Average loss: 171.8610\n",
      "====> Test set loss: 173.5582\n",
      "Train Epoch: 442 [0/697932 (0%)]\tLoss: 171.617859\n",
      "Train Epoch: 442 [100000/697932 (14%)]\tLoss: 168.369188\n",
      "Train Epoch: 442 [200000/697932 (29%)]\tLoss: 168.408312\n",
      "Train Epoch: 442 [300000/697932 (43%)]\tLoss: 174.941031\n",
      "Train Epoch: 442 [400000/697932 (57%)]\tLoss: 171.326031\n",
      "Train Epoch: 442 [500000/697932 (72%)]\tLoss: 170.722109\n",
      "Train Epoch: 442 [600000/697932 (86%)]\tLoss: 170.368094\n",
      "====> Epoch: 442 Average loss: 171.6992\n",
      "====> Test set loss: 173.4121\n",
      "Train Epoch: 443 [0/697932 (0%)]\tLoss: 171.393500\n",
      "Train Epoch: 443 [100000/697932 (14%)]\tLoss: 171.586016\n",
      "Train Epoch: 443 [200000/697932 (29%)]\tLoss: 170.730828\n",
      "Train Epoch: 443 [300000/697932 (43%)]\tLoss: 170.919047\n",
      "Train Epoch: 443 [400000/697932 (57%)]\tLoss: 172.116250\n",
      "Train Epoch: 443 [500000/697932 (72%)]\tLoss: 172.058484\n",
      "Train Epoch: 443 [600000/697932 (86%)]\tLoss: 170.221875\n",
      "====> Epoch: 443 Average loss: 171.7796\n",
      "====> Test set loss: 173.7014\n",
      "Train Epoch: 444 [0/697932 (0%)]\tLoss: 173.100094\n",
      "Train Epoch: 444 [100000/697932 (14%)]\tLoss: 176.376250\n",
      "Train Epoch: 444 [200000/697932 (29%)]\tLoss: 169.200484\n",
      "Train Epoch: 444 [300000/697932 (43%)]\tLoss: 172.914781\n",
      "Train Epoch: 444 [400000/697932 (57%)]\tLoss: 174.809203\n",
      "Train Epoch: 444 [500000/697932 (72%)]\tLoss: 174.098875\n",
      "Train Epoch: 444 [600000/697932 (86%)]\tLoss: 174.885797\n",
      "====> Epoch: 444 Average loss: 172.0211\n",
      "====> Test set loss: 173.7049\n",
      "Train Epoch: 445 [0/697932 (0%)]\tLoss: 171.222750\n",
      "Train Epoch: 445 [100000/697932 (14%)]\tLoss: 172.115672\n",
      "Train Epoch: 445 [200000/697932 (29%)]\tLoss: 172.041734\n",
      "Train Epoch: 445 [300000/697932 (43%)]\tLoss: 169.159203\n",
      "Train Epoch: 445 [400000/697932 (57%)]\tLoss: 169.569594\n",
      "Train Epoch: 445 [500000/697932 (72%)]\tLoss: 171.790766\n",
      "Train Epoch: 445 [600000/697932 (86%)]\tLoss: 175.107172\n",
      "====> Epoch: 445 Average loss: 172.2340\n",
      "====> Test set loss: 174.1157\n",
      "Train Epoch: 446 [0/697932 (0%)]\tLoss: 172.166578\n",
      "Train Epoch: 446 [100000/697932 (14%)]\tLoss: 173.177188\n",
      "Train Epoch: 446 [200000/697932 (29%)]\tLoss: 174.677156\n",
      "Train Epoch: 446 [300000/697932 (43%)]\tLoss: 173.092844\n",
      "Train Epoch: 446 [400000/697932 (57%)]\tLoss: 170.972016\n",
      "Train Epoch: 446 [500000/697932 (72%)]\tLoss: 169.820266\n",
      "Train Epoch: 446 [600000/697932 (86%)]\tLoss: 174.967750\n",
      "====> Epoch: 446 Average loss: 172.1883\n",
      "====> Test set loss: 174.0802\n",
      "Train Epoch: 447 [0/697932 (0%)]\tLoss: 172.442219\n",
      "Train Epoch: 447 [100000/697932 (14%)]\tLoss: 174.759469\n",
      "Train Epoch: 447 [200000/697932 (29%)]\tLoss: 174.120734\n",
      "Train Epoch: 447 [300000/697932 (43%)]\tLoss: 174.411438\n",
      "Train Epoch: 447 [400000/697932 (57%)]\tLoss: 173.952438\n",
      "Train Epoch: 447 [500000/697932 (72%)]\tLoss: 172.472219\n",
      "Train Epoch: 447 [600000/697932 (86%)]\tLoss: 173.092953\n",
      "====> Epoch: 447 Average loss: 172.0817\n",
      "====> Test set loss: 173.9449\n",
      "Train Epoch: 448 [0/697932 (0%)]\tLoss: 169.784188\n",
      "Train Epoch: 448 [100000/697932 (14%)]\tLoss: 172.711266\n",
      "Train Epoch: 448 [200000/697932 (29%)]\tLoss: 171.718437\n",
      "Train Epoch: 448 [300000/697932 (43%)]\tLoss: 173.575734\n",
      "Train Epoch: 448 [400000/697932 (57%)]\tLoss: 170.740312\n",
      "Train Epoch: 448 [500000/697932 (72%)]\tLoss: 172.398609\n",
      "Train Epoch: 448 [600000/697932 (86%)]\tLoss: 174.743500\n",
      "====> Epoch: 448 Average loss: 172.0661\n",
      "====> Test set loss: 173.8279\n",
      "Train Epoch: 449 [0/697932 (0%)]\tLoss: 169.840250\n",
      "Train Epoch: 449 [100000/697932 (14%)]\tLoss: 171.924266\n",
      "Train Epoch: 449 [200000/697932 (29%)]\tLoss: 172.292500\n",
      "Train Epoch: 449 [300000/697932 (43%)]\tLoss: 169.859766\n",
      "Train Epoch: 449 [400000/697932 (57%)]\tLoss: 171.681062\n",
      "Train Epoch: 449 [500000/697932 (72%)]\tLoss: 173.350125\n",
      "Train Epoch: 449 [600000/697932 (86%)]\tLoss: 173.647844\n",
      "====> Epoch: 449 Average loss: 172.0476\n",
      "====> Test set loss: 173.7641\n",
      "Train Epoch: 450 [0/697932 (0%)]\tLoss: 170.617453\n",
      "Train Epoch: 450 [100000/697932 (14%)]\tLoss: 170.611844\n",
      "Train Epoch: 450 [200000/697932 (29%)]\tLoss: 168.217172\n",
      "Train Epoch: 450 [300000/697932 (43%)]\tLoss: 171.847391\n",
      "Train Epoch: 450 [400000/697932 (57%)]\tLoss: 169.794531\n",
      "Train Epoch: 450 [500000/697932 (72%)]\tLoss: 173.592141\n",
      "Train Epoch: 450 [600000/697932 (86%)]\tLoss: 171.333125\n",
      "====> Epoch: 450 Average loss: 172.0976\n",
      "====> Test set loss: 173.7908\n",
      "Train Epoch: 451 [0/697932 (0%)]\tLoss: 169.686781\n",
      "Train Epoch: 451 [100000/697932 (14%)]\tLoss: 171.551000\n",
      "Train Epoch: 451 [200000/697932 (29%)]\tLoss: 173.572422\n",
      "Train Epoch: 451 [300000/697932 (43%)]\tLoss: 172.446953\n",
      "Train Epoch: 451 [400000/697932 (57%)]\tLoss: 170.113578\n",
      "Train Epoch: 451 [500000/697932 (72%)]\tLoss: 169.871922\n",
      "Train Epoch: 451 [600000/697932 (86%)]\tLoss: 174.887500\n",
      "====> Epoch: 451 Average loss: 172.1903\n",
      "====> Test set loss: 173.9656\n",
      "Train Epoch: 452 [0/697932 (0%)]\tLoss: 176.360594\n",
      "Train Epoch: 452 [100000/697932 (14%)]\tLoss: 171.578797\n",
      "Train Epoch: 452 [200000/697932 (29%)]\tLoss: 169.405328\n",
      "Train Epoch: 452 [300000/697932 (43%)]\tLoss: 171.678234\n",
      "Train Epoch: 452 [400000/697932 (57%)]\tLoss: 175.202438\n",
      "Train Epoch: 452 [500000/697932 (72%)]\tLoss: 172.080906\n",
      "Train Epoch: 452 [600000/697932 (86%)]\tLoss: 173.054609\n",
      "====> Epoch: 452 Average loss: 172.1101\n",
      "====> Test set loss: 173.8027\n",
      "Train Epoch: 453 [0/697932 (0%)]\tLoss: 175.278578\n",
      "Train Epoch: 453 [100000/697932 (14%)]\tLoss: 171.774812\n",
      "Train Epoch: 453 [200000/697932 (29%)]\tLoss: 171.239609\n",
      "Train Epoch: 453 [300000/697932 (43%)]\tLoss: 170.330000\n",
      "Train Epoch: 453 [400000/697932 (57%)]\tLoss: 171.894281\n",
      "Train Epoch: 453 [500000/697932 (72%)]\tLoss: 175.371031\n",
      "Train Epoch: 453 [600000/697932 (86%)]\tLoss: 173.992672\n",
      "====> Epoch: 453 Average loss: 171.9763\n",
      "====> Test set loss: 173.9520\n",
      "Train Epoch: 454 [0/697932 (0%)]\tLoss: 171.696188\n",
      "Train Epoch: 454 [100000/697932 (14%)]\tLoss: 169.942328\n",
      "Train Epoch: 454 [200000/697932 (29%)]\tLoss: 169.777672\n",
      "Train Epoch: 454 [300000/697932 (43%)]\tLoss: 173.128875\n",
      "Train Epoch: 454 [400000/697932 (57%)]\tLoss: 170.464203\n",
      "Train Epoch: 454 [500000/697932 (72%)]\tLoss: 169.183266\n",
      "Train Epoch: 454 [600000/697932 (86%)]\tLoss: 171.251313\n",
      "====> Epoch: 454 Average loss: 172.1376\n",
      "====> Test set loss: 173.7959\n",
      "Train Epoch: 455 [0/697932 (0%)]\tLoss: 172.239344\n",
      "Train Epoch: 455 [100000/697932 (14%)]\tLoss: 171.778516\n",
      "Train Epoch: 455 [200000/697932 (29%)]\tLoss: 170.434000\n",
      "Train Epoch: 455 [300000/697932 (43%)]\tLoss: 170.080016\n",
      "Train Epoch: 455 [400000/697932 (57%)]\tLoss: 170.831469\n",
      "Train Epoch: 455 [500000/697932 (72%)]\tLoss: 172.510000\n",
      "Train Epoch: 455 [600000/697932 (86%)]\tLoss: 172.506063\n",
      "====> Epoch: 455 Average loss: 172.0555\n",
      "====> Test set loss: 173.8214\n",
      "Train Epoch: 456 [0/697932 (0%)]\tLoss: 174.005969\n",
      "Train Epoch: 456 [100000/697932 (14%)]\tLoss: 172.055688\n",
      "Train Epoch: 456 [200000/697932 (29%)]\tLoss: 173.843531\n",
      "Train Epoch: 456 [300000/697932 (43%)]\tLoss: 169.859500\n",
      "Train Epoch: 456 [400000/697932 (57%)]\tLoss: 171.484500\n",
      "Train Epoch: 456 [500000/697932 (72%)]\tLoss: 173.520344\n",
      "Train Epoch: 456 [600000/697932 (86%)]\tLoss: 172.334422\n",
      "====> Epoch: 456 Average loss: 172.0564\n",
      "====> Test set loss: 173.9331\n",
      "Train Epoch: 457 [0/697932 (0%)]\tLoss: 168.885562\n",
      "Train Epoch: 457 [100000/697932 (14%)]\tLoss: 173.696250\n",
      "Train Epoch: 457 [200000/697932 (29%)]\tLoss: 172.219141\n",
      "Train Epoch: 457 [300000/697932 (43%)]\tLoss: 172.087312\n",
      "Train Epoch: 457 [400000/697932 (57%)]\tLoss: 171.296281\n",
      "Train Epoch: 457 [500000/697932 (72%)]\tLoss: 170.433156\n",
      "Train Epoch: 457 [600000/697932 (86%)]\tLoss: 171.143547\n",
      "====> Epoch: 457 Average loss: 172.2989\n",
      "====> Test set loss: 174.0020\n",
      "Train Epoch: 458 [0/697932 (0%)]\tLoss: 173.116656\n",
      "Train Epoch: 458 [100000/697932 (14%)]\tLoss: 173.408797\n",
      "Train Epoch: 458 [200000/697932 (29%)]\tLoss: 172.187766\n",
      "Train Epoch: 458 [300000/697932 (43%)]\tLoss: 171.756141\n",
      "Train Epoch: 458 [400000/697932 (57%)]\tLoss: 170.188281\n",
      "Train Epoch: 458 [500000/697932 (72%)]\tLoss: 171.631563\n",
      "Train Epoch: 458 [600000/697932 (86%)]\tLoss: 172.822984\n",
      "====> Epoch: 458 Average loss: 171.9412\n",
      "====> Test set loss: 173.5807\n",
      "Train Epoch: 459 [0/697932 (0%)]\tLoss: 170.750391\n",
      "Train Epoch: 459 [100000/697932 (14%)]\tLoss: 172.004656\n",
      "Train Epoch: 459 [200000/697932 (29%)]\tLoss: 171.314469\n",
      "Train Epoch: 459 [300000/697932 (43%)]\tLoss: 170.665922\n",
      "Train Epoch: 459 [400000/697932 (57%)]\tLoss: 174.291156\n",
      "Train Epoch: 459 [500000/697932 (72%)]\tLoss: 173.968484\n",
      "Train Epoch: 459 [600000/697932 (86%)]\tLoss: 170.477062\n",
      "====> Epoch: 459 Average loss: 171.8583\n",
      "====> Test set loss: 173.5847\n",
      "Train Epoch: 460 [0/697932 (0%)]\tLoss: 173.537500\n",
      "Train Epoch: 460 [100000/697932 (14%)]\tLoss: 171.540719\n",
      "Train Epoch: 460 [200000/697932 (29%)]\tLoss: 170.967625\n",
      "Train Epoch: 460 [300000/697932 (43%)]\tLoss: 174.088234\n",
      "Train Epoch: 460 [400000/697932 (57%)]\tLoss: 173.033641\n",
      "Train Epoch: 460 [500000/697932 (72%)]\tLoss: 168.481031\n",
      "Train Epoch: 460 [600000/697932 (86%)]\tLoss: 167.366625\n",
      "====> Epoch: 460 Average loss: 171.8844\n",
      "====> Test set loss: 173.9604\n",
      "Train Epoch: 461 [0/697932 (0%)]\tLoss: 170.372094\n",
      "Train Epoch: 461 [100000/697932 (14%)]\tLoss: 172.116688\n",
      "Train Epoch: 461 [200000/697932 (29%)]\tLoss: 171.988063\n",
      "Train Epoch: 461 [300000/697932 (43%)]\tLoss: 172.957141\n",
      "Train Epoch: 461 [400000/697932 (57%)]\tLoss: 177.670547\n",
      "Train Epoch: 461 [500000/697932 (72%)]\tLoss: 173.054750\n",
      "Train Epoch: 461 [600000/697932 (86%)]\tLoss: 172.028156\n",
      "====> Epoch: 461 Average loss: 171.8429\n",
      "====> Test set loss: 173.4950\n",
      "Train Epoch: 462 [0/697932 (0%)]\tLoss: 170.678156\n",
      "Train Epoch: 462 [100000/697932 (14%)]\tLoss: 173.158750\n",
      "Train Epoch: 462 [200000/697932 (29%)]\tLoss: 172.533750\n",
      "Train Epoch: 462 [300000/697932 (43%)]\tLoss: 172.948828\n",
      "Train Epoch: 462 [400000/697932 (57%)]\tLoss: 167.259203\n",
      "Train Epoch: 462 [500000/697932 (72%)]\tLoss: 173.399797\n",
      "Train Epoch: 462 [600000/697932 (86%)]\tLoss: 173.002750\n",
      "====> Epoch: 462 Average loss: 171.7970\n",
      "====> Test set loss: 173.9266\n",
      "Train Epoch: 463 [0/697932 (0%)]\tLoss: 169.238047\n",
      "Train Epoch: 463 [100000/697932 (14%)]\tLoss: 172.687437\n",
      "Train Epoch: 463 [200000/697932 (29%)]\tLoss: 171.548281\n",
      "Train Epoch: 463 [300000/697932 (43%)]\tLoss: 171.391859\n",
      "Train Epoch: 463 [400000/697932 (57%)]\tLoss: 167.734469\n",
      "Train Epoch: 463 [500000/697932 (72%)]\tLoss: 172.359531\n",
      "Train Epoch: 463 [600000/697932 (86%)]\tLoss: 171.210813\n",
      "====> Epoch: 463 Average loss: 171.6912\n",
      "====> Test set loss: 173.6756\n",
      "Train Epoch: 464 [0/697932 (0%)]\tLoss: 173.165875\n",
      "Train Epoch: 464 [100000/697932 (14%)]\tLoss: 175.101797\n",
      "Train Epoch: 464 [200000/697932 (29%)]\tLoss: 172.483000\n",
      "Train Epoch: 464 [300000/697932 (43%)]\tLoss: 171.985562\n",
      "Train Epoch: 464 [400000/697932 (57%)]\tLoss: 171.071422\n",
      "Train Epoch: 464 [500000/697932 (72%)]\tLoss: 171.300750\n",
      "Train Epoch: 464 [600000/697932 (86%)]\tLoss: 172.454906\n",
      "====> Epoch: 464 Average loss: 171.7094\n",
      "====> Test set loss: 173.8138\n",
      "Train Epoch: 465 [0/697932 (0%)]\tLoss: 172.823500\n",
      "Train Epoch: 465 [100000/697932 (14%)]\tLoss: 173.815125\n",
      "Train Epoch: 465 [200000/697932 (29%)]\tLoss: 170.019219\n",
      "Train Epoch: 465 [300000/697932 (43%)]\tLoss: 172.275313\n",
      "Train Epoch: 465 [400000/697932 (57%)]\tLoss: 172.673984\n",
      "Train Epoch: 465 [500000/697932 (72%)]\tLoss: 170.146938\n",
      "Train Epoch: 465 [600000/697932 (86%)]\tLoss: 174.726000\n",
      "====> Epoch: 465 Average loss: 171.5647\n",
      "====> Test set loss: 173.4817\n",
      "Train Epoch: 466 [0/697932 (0%)]\tLoss: 169.645813\n",
      "Train Epoch: 466 [100000/697932 (14%)]\tLoss: 172.658437\n",
      "Train Epoch: 466 [200000/697932 (29%)]\tLoss: 170.748719\n",
      "Train Epoch: 466 [300000/697932 (43%)]\tLoss: 171.777375\n",
      "Train Epoch: 466 [400000/697932 (57%)]\tLoss: 171.826016\n",
      "Train Epoch: 466 [500000/697932 (72%)]\tLoss: 170.913875\n",
      "Train Epoch: 466 [600000/697932 (86%)]\tLoss: 172.348453\n",
      "====> Epoch: 466 Average loss: 171.5363\n",
      "====> Test set loss: 173.4430\n",
      "Train Epoch: 467 [0/697932 (0%)]\tLoss: 170.660469\n",
      "Train Epoch: 467 [100000/697932 (14%)]\tLoss: 167.925984\n",
      "Train Epoch: 467 [200000/697932 (29%)]\tLoss: 171.182875\n",
      "Train Epoch: 467 [300000/697932 (43%)]\tLoss: 170.729594\n",
      "Train Epoch: 467 [400000/697932 (57%)]\tLoss: 173.378453\n",
      "Train Epoch: 467 [500000/697932 (72%)]\tLoss: 173.004844\n",
      "Train Epoch: 467 [600000/697932 (86%)]\tLoss: 172.125937\n",
      "====> Epoch: 467 Average loss: 171.5318\n",
      "====> Test set loss: 173.5747\n",
      "Train Epoch: 468 [0/697932 (0%)]\tLoss: 170.314641\n",
      "Train Epoch: 468 [100000/697932 (14%)]\tLoss: 171.797500\n",
      "Train Epoch: 468 [200000/697932 (29%)]\tLoss: 171.735187\n",
      "Train Epoch: 468 [300000/697932 (43%)]\tLoss: 174.605391\n",
      "Train Epoch: 468 [400000/697932 (57%)]\tLoss: 170.788969\n",
      "Train Epoch: 468 [500000/697932 (72%)]\tLoss: 172.790188\n",
      "Train Epoch: 468 [600000/697932 (86%)]\tLoss: 170.859266\n",
      "====> Epoch: 468 Average loss: 171.6130\n",
      "====> Test set loss: 173.4973\n",
      "Train Epoch: 469 [0/697932 (0%)]\tLoss: 170.707578\n",
      "Train Epoch: 469 [100000/697932 (14%)]\tLoss: 172.713250\n",
      "Train Epoch: 469 [200000/697932 (29%)]\tLoss: 172.271984\n",
      "Train Epoch: 469 [300000/697932 (43%)]\tLoss: 172.706594\n",
      "Train Epoch: 469 [400000/697932 (57%)]\tLoss: 168.961703\n",
      "Train Epoch: 469 [500000/697932 (72%)]\tLoss: 172.425453\n",
      "Train Epoch: 469 [600000/697932 (86%)]\tLoss: 171.033922\n",
      "====> Epoch: 469 Average loss: 171.5847\n",
      "====> Test set loss: 173.7268\n",
      "Train Epoch: 470 [0/697932 (0%)]\tLoss: 172.179812\n",
      "Train Epoch: 470 [100000/697932 (14%)]\tLoss: 173.321906\n",
      "Train Epoch: 470 [200000/697932 (29%)]\tLoss: 171.501922\n",
      "Train Epoch: 470 [300000/697932 (43%)]\tLoss: 172.758453\n",
      "Train Epoch: 470 [400000/697932 (57%)]\tLoss: 171.070078\n",
      "Train Epoch: 470 [500000/697932 (72%)]\tLoss: 171.668937\n",
      "Train Epoch: 470 [600000/697932 (86%)]\tLoss: 170.975219\n",
      "====> Epoch: 470 Average loss: 171.5874\n",
      "====> Test set loss: 173.4907\n",
      "Train Epoch: 471 [0/697932 (0%)]\tLoss: 169.550844\n",
      "Train Epoch: 471 [100000/697932 (14%)]\tLoss: 172.447625\n",
      "Train Epoch: 471 [200000/697932 (29%)]\tLoss: 172.667781\n",
      "Train Epoch: 471 [300000/697932 (43%)]\tLoss: 174.301938\n",
      "Train Epoch: 471 [400000/697932 (57%)]\tLoss: 169.947688\n",
      "Train Epoch: 471 [500000/697932 (72%)]\tLoss: 171.506625\n",
      "Train Epoch: 471 [600000/697932 (86%)]\tLoss: 170.511812\n",
      "====> Epoch: 471 Average loss: 171.7429\n",
      "====> Test set loss: 173.8682\n",
      "Train Epoch: 472 [0/697932 (0%)]\tLoss: 170.962562\n",
      "Train Epoch: 472 [100000/697932 (14%)]\tLoss: 171.532922\n",
      "Train Epoch: 472 [200000/697932 (29%)]\tLoss: 173.100266\n",
      "Train Epoch: 472 [300000/697932 (43%)]\tLoss: 173.083094\n",
      "Train Epoch: 472 [400000/697932 (57%)]\tLoss: 172.741031\n",
      "Train Epoch: 472 [500000/697932 (72%)]\tLoss: 171.095297\n",
      "Train Epoch: 472 [600000/697932 (86%)]\tLoss: 172.821344\n",
      "====> Epoch: 472 Average loss: 171.9130\n",
      "====> Test set loss: 173.8182\n",
      "Train Epoch: 473 [0/697932 (0%)]\tLoss: 171.821188\n",
      "Train Epoch: 473 [100000/697932 (14%)]\tLoss: 170.637656\n",
      "Train Epoch: 473 [200000/697932 (29%)]\tLoss: 171.139922\n",
      "Train Epoch: 473 [300000/697932 (43%)]\tLoss: 172.891125\n",
      "Train Epoch: 473 [400000/697932 (57%)]\tLoss: 172.750375\n",
      "Train Epoch: 473 [500000/697932 (72%)]\tLoss: 169.048937\n",
      "Train Epoch: 473 [600000/697932 (86%)]\tLoss: 173.327906\n",
      "====> Epoch: 473 Average loss: 172.0344\n",
      "====> Test set loss: 174.1911\n",
      "Train Epoch: 474 [0/697932 (0%)]\tLoss: 174.527375\n",
      "Train Epoch: 474 [100000/697932 (14%)]\tLoss: 171.093687\n",
      "Train Epoch: 474 [200000/697932 (29%)]\tLoss: 171.234750\n",
      "Train Epoch: 474 [300000/697932 (43%)]\tLoss: 170.935844\n",
      "Train Epoch: 474 [400000/697932 (57%)]\tLoss: 172.722109\n",
      "Train Epoch: 474 [500000/697932 (72%)]\tLoss: 174.000250\n",
      "Train Epoch: 474 [600000/697932 (86%)]\tLoss: 171.958391\n",
      "====> Epoch: 474 Average loss: 172.2351\n",
      "====> Test set loss: 174.0367\n",
      "Train Epoch: 475 [0/697932 (0%)]\tLoss: 171.335406\n",
      "Train Epoch: 475 [100000/697932 (14%)]\tLoss: 172.019000\n",
      "Train Epoch: 475 [200000/697932 (29%)]\tLoss: 170.874313\n",
      "Train Epoch: 475 [300000/697932 (43%)]\tLoss: 172.800719\n",
      "Train Epoch: 475 [400000/697932 (57%)]\tLoss: 169.622078\n",
      "Train Epoch: 475 [500000/697932 (72%)]\tLoss: 171.414797\n",
      "Train Epoch: 475 [600000/697932 (86%)]\tLoss: 173.803719\n",
      "====> Epoch: 475 Average loss: 172.2763\n",
      "====> Test set loss: 174.0436\n",
      "Train Epoch: 476 [0/697932 (0%)]\tLoss: 170.433000\n",
      "Train Epoch: 476 [100000/697932 (14%)]\tLoss: 168.468000\n",
      "Train Epoch: 476 [200000/697932 (29%)]\tLoss: 172.887172\n",
      "Train Epoch: 476 [300000/697932 (43%)]\tLoss: 173.182094\n",
      "Train Epoch: 476 [400000/697932 (57%)]\tLoss: 172.431281\n",
      "Train Epoch: 476 [500000/697932 (72%)]\tLoss: 172.227156\n",
      "Train Epoch: 476 [600000/697932 (86%)]\tLoss: 170.888828\n",
      "====> Epoch: 476 Average loss: 172.0170\n",
      "====> Test set loss: 173.9626\n",
      "Train Epoch: 477 [0/697932 (0%)]\tLoss: 170.265203\n",
      "Train Epoch: 477 [100000/697932 (14%)]\tLoss: 170.176141\n",
      "Train Epoch: 477 [200000/697932 (29%)]\tLoss: 171.680844\n",
      "Train Epoch: 477 [300000/697932 (43%)]\tLoss: 174.020125\n",
      "Train Epoch: 477 [400000/697932 (57%)]\tLoss: 170.083219\n",
      "Train Epoch: 477 [500000/697932 (72%)]\tLoss: 170.827672\n",
      "Train Epoch: 477 [600000/697932 (86%)]\tLoss: 173.836062\n",
      "====> Epoch: 477 Average loss: 171.9684\n",
      "====> Test set loss: 173.9803\n",
      "Train Epoch: 478 [0/697932 (0%)]\tLoss: 173.152828\n",
      "Train Epoch: 478 [100000/697932 (14%)]\tLoss: 177.034063\n",
      "Train Epoch: 478 [200000/697932 (29%)]\tLoss: 173.434078\n",
      "Train Epoch: 478 [300000/697932 (43%)]\tLoss: 170.766734\n",
      "Train Epoch: 478 [400000/697932 (57%)]\tLoss: 169.918516\n",
      "Train Epoch: 478 [500000/697932 (72%)]\tLoss: 171.877547\n",
      "Train Epoch: 478 [600000/697932 (86%)]\tLoss: 170.691234\n",
      "====> Epoch: 478 Average loss: 172.2684\n",
      "====> Test set loss: 174.0233\n",
      "Train Epoch: 479 [0/697932 (0%)]\tLoss: 174.711234\n",
      "Train Epoch: 479 [100000/697932 (14%)]\tLoss: 169.708391\n",
      "Train Epoch: 479 [200000/697932 (29%)]\tLoss: 168.259281\n",
      "Train Epoch: 479 [300000/697932 (43%)]\tLoss: 173.892578\n",
      "Train Epoch: 479 [400000/697932 (57%)]\tLoss: 172.784219\n",
      "Train Epoch: 479 [500000/697932 (72%)]\tLoss: 172.236469\n",
      "Train Epoch: 479 [600000/697932 (86%)]\tLoss: 172.022812\n",
      "====> Epoch: 479 Average loss: 172.3236\n",
      "====> Test set loss: 173.9735\n",
      "Train Epoch: 480 [0/697932 (0%)]\tLoss: 170.954937\n",
      "Train Epoch: 480 [100000/697932 (14%)]\tLoss: 168.577969\n",
      "Train Epoch: 480 [200000/697932 (29%)]\tLoss: 172.929094\n",
      "Train Epoch: 480 [300000/697932 (43%)]\tLoss: 174.408062\n",
      "Train Epoch: 480 [400000/697932 (57%)]\tLoss: 172.511672\n",
      "Train Epoch: 480 [500000/697932 (72%)]\tLoss: 172.350922\n",
      "Train Epoch: 480 [600000/697932 (86%)]\tLoss: 176.469500\n",
      "====> Epoch: 480 Average loss: 172.1070\n",
      "====> Test set loss: 173.8981\n",
      "Train Epoch: 481 [0/697932 (0%)]\tLoss: 170.361703\n",
      "Train Epoch: 481 [100000/697932 (14%)]\tLoss: 171.516609\n",
      "Train Epoch: 481 [200000/697932 (29%)]\tLoss: 174.221172\n",
      "Train Epoch: 481 [300000/697932 (43%)]\tLoss: 174.384000\n",
      "Train Epoch: 481 [400000/697932 (57%)]\tLoss: 171.877641\n",
      "Train Epoch: 481 [500000/697932 (72%)]\tLoss: 175.332766\n",
      "Train Epoch: 481 [600000/697932 (86%)]\tLoss: 174.295719\n",
      "====> Epoch: 481 Average loss: 172.3301\n",
      "====> Test set loss: 173.9716\n",
      "Train Epoch: 482 [0/697932 (0%)]\tLoss: 170.502578\n",
      "Train Epoch: 482 [100000/697932 (14%)]\tLoss: 176.137844\n",
      "Train Epoch: 482 [200000/697932 (29%)]\tLoss: 174.933281\n",
      "Train Epoch: 482 [300000/697932 (43%)]\tLoss: 175.366813\n",
      "Train Epoch: 482 [400000/697932 (57%)]\tLoss: 173.241641\n",
      "Train Epoch: 482 [500000/697932 (72%)]\tLoss: 172.698875\n",
      "Train Epoch: 482 [600000/697932 (86%)]\tLoss: 173.216000\n",
      "====> Epoch: 482 Average loss: 172.2555\n",
      "====> Test set loss: 174.0068\n",
      "Train Epoch: 483 [0/697932 (0%)]\tLoss: 171.119281\n",
      "Train Epoch: 483 [100000/697932 (14%)]\tLoss: 173.088250\n",
      "Train Epoch: 483 [200000/697932 (29%)]\tLoss: 172.171781\n",
      "Train Epoch: 483 [300000/697932 (43%)]\tLoss: 173.138906\n",
      "Train Epoch: 483 [400000/697932 (57%)]\tLoss: 177.537594\n",
      "Train Epoch: 483 [500000/697932 (72%)]\tLoss: 171.152359\n",
      "Train Epoch: 483 [600000/697932 (86%)]\tLoss: 174.120219\n",
      "====> Epoch: 483 Average loss: 172.1128\n",
      "====> Test set loss: 174.1433\n",
      "Train Epoch: 484 [0/697932 (0%)]\tLoss: 172.187234\n",
      "Train Epoch: 484 [100000/697932 (14%)]\tLoss: 172.148937\n",
      "Train Epoch: 484 [200000/697932 (29%)]\tLoss: 170.921312\n",
      "Train Epoch: 484 [300000/697932 (43%)]\tLoss: 174.093359\n",
      "Train Epoch: 484 [400000/697932 (57%)]\tLoss: 172.704813\n",
      "Train Epoch: 484 [500000/697932 (72%)]\tLoss: 174.488797\n",
      "Train Epoch: 484 [600000/697932 (86%)]\tLoss: 173.920906\n",
      "====> Epoch: 484 Average loss: 172.2202\n",
      "====> Test set loss: 174.2837\n",
      "Train Epoch: 485 [0/697932 (0%)]\tLoss: 172.462844\n",
      "Train Epoch: 485 [100000/697932 (14%)]\tLoss: 169.685094\n",
      "Train Epoch: 485 [200000/697932 (29%)]\tLoss: 174.000500\n",
      "Train Epoch: 485 [300000/697932 (43%)]\tLoss: 170.616984\n",
      "Train Epoch: 485 [400000/697932 (57%)]\tLoss: 175.148656\n",
      "Train Epoch: 485 [500000/697932 (72%)]\tLoss: 171.381625\n",
      "Train Epoch: 485 [600000/697932 (86%)]\tLoss: 170.589719\n",
      "====> Epoch: 485 Average loss: 172.1427\n",
      "====> Test set loss: 174.0630\n",
      "Train Epoch: 486 [0/697932 (0%)]\tLoss: 171.113234\n",
      "Train Epoch: 486 [100000/697932 (14%)]\tLoss: 172.601672\n",
      "Train Epoch: 486 [200000/697932 (29%)]\tLoss: 170.384922\n",
      "Train Epoch: 486 [300000/697932 (43%)]\tLoss: 169.453969\n",
      "Train Epoch: 486 [400000/697932 (57%)]\tLoss: 168.854125\n",
      "Train Epoch: 486 [500000/697932 (72%)]\tLoss: 172.424078\n",
      "Train Epoch: 486 [600000/697932 (86%)]\tLoss: 172.072469\n",
      "====> Epoch: 486 Average loss: 172.2137\n",
      "====> Test set loss: 174.3304\n",
      "Train Epoch: 487 [0/697932 (0%)]\tLoss: 171.738406\n",
      "Train Epoch: 487 [100000/697932 (14%)]\tLoss: 169.568141\n",
      "Train Epoch: 487 [200000/697932 (29%)]\tLoss: 172.758688\n",
      "Train Epoch: 487 [300000/697932 (43%)]\tLoss: 175.587078\n",
      "Train Epoch: 487 [400000/697932 (57%)]\tLoss: 172.229047\n",
      "Train Epoch: 487 [500000/697932 (72%)]\tLoss: 172.223172\n",
      "Train Epoch: 487 [600000/697932 (86%)]\tLoss: 167.304984\n",
      "====> Epoch: 487 Average loss: 172.2940\n",
      "====> Test set loss: 174.0270\n",
      "Train Epoch: 488 [0/697932 (0%)]\tLoss: 171.658000\n",
      "Train Epoch: 488 [100000/697932 (14%)]\tLoss: 173.691578\n",
      "Train Epoch: 488 [200000/697932 (29%)]\tLoss: 171.642469\n",
      "Train Epoch: 488 [300000/697932 (43%)]\tLoss: 174.761828\n",
      "Train Epoch: 488 [400000/697932 (57%)]\tLoss: 172.169031\n",
      "Train Epoch: 488 [500000/697932 (72%)]\tLoss: 173.351625\n",
      "Train Epoch: 488 [600000/697932 (86%)]\tLoss: 169.069969\n",
      "====> Epoch: 488 Average loss: 172.3820\n",
      "====> Test set loss: 174.3928\n",
      "Train Epoch: 489 [0/697932 (0%)]\tLoss: 170.762344\n",
      "Train Epoch: 489 [100000/697932 (14%)]\tLoss: 171.950344\n",
      "Train Epoch: 489 [200000/697932 (29%)]\tLoss: 171.651047\n",
      "Train Epoch: 489 [300000/697932 (43%)]\tLoss: 173.522156\n",
      "Train Epoch: 489 [400000/697932 (57%)]\tLoss: 169.537922\n",
      "Train Epoch: 489 [500000/697932 (72%)]\tLoss: 174.040781\n",
      "Train Epoch: 489 [600000/697932 (86%)]\tLoss: 171.244359\n",
      "====> Epoch: 489 Average loss: 172.3823\n",
      "====> Test set loss: 174.0547\n",
      "Train Epoch: 490 [0/697932 (0%)]\tLoss: 170.665625\n",
      "Train Epoch: 490 [100000/697932 (14%)]\tLoss: 168.748188\n",
      "Train Epoch: 490 [200000/697932 (29%)]\tLoss: 174.908469\n",
      "Train Epoch: 490 [300000/697932 (43%)]\tLoss: 173.270047\n",
      "Train Epoch: 490 [400000/697932 (57%)]\tLoss: 170.387500\n",
      "Train Epoch: 490 [500000/697932 (72%)]\tLoss: 169.697953\n",
      "Train Epoch: 490 [600000/697932 (86%)]\tLoss: 169.186344\n",
      "====> Epoch: 490 Average loss: 172.5029\n",
      "====> Test set loss: 174.1504\n",
      "Train Epoch: 491 [0/697932 (0%)]\tLoss: 171.033641\n",
      "Train Epoch: 491 [100000/697932 (14%)]\tLoss: 172.142844\n",
      "Train Epoch: 491 [200000/697932 (29%)]\tLoss: 173.267734\n",
      "Train Epoch: 491 [300000/697932 (43%)]\tLoss: 171.228156\n",
      "Train Epoch: 491 [400000/697932 (57%)]\tLoss: 173.650656\n",
      "Train Epoch: 491 [500000/697932 (72%)]\tLoss: 171.175594\n",
      "Train Epoch: 491 [600000/697932 (86%)]\tLoss: 173.550453\n",
      "====> Epoch: 491 Average loss: 172.2552\n",
      "====> Test set loss: 174.0013\n",
      "Train Epoch: 492 [0/697932 (0%)]\tLoss: 171.826703\n",
      "Train Epoch: 492 [100000/697932 (14%)]\tLoss: 172.259328\n",
      "Train Epoch: 492 [200000/697932 (29%)]\tLoss: 172.779109\n",
      "Train Epoch: 492 [300000/697932 (43%)]\tLoss: 173.439969\n",
      "Train Epoch: 492 [400000/697932 (57%)]\tLoss: 173.139938\n",
      "Train Epoch: 492 [500000/697932 (72%)]\tLoss: 171.832672\n",
      "Train Epoch: 492 [600000/697932 (86%)]\tLoss: 171.716438\n",
      "====> Epoch: 492 Average loss: 172.1350\n",
      "====> Test set loss: 173.8267\n",
      "Train Epoch: 493 [0/697932 (0%)]\tLoss: 173.811734\n",
      "Train Epoch: 493 [100000/697932 (14%)]\tLoss: 171.618656\n",
      "Train Epoch: 493 [200000/697932 (29%)]\tLoss: 173.507906\n",
      "Train Epoch: 493 [300000/697932 (43%)]\tLoss: 174.198359\n",
      "Train Epoch: 493 [400000/697932 (57%)]\tLoss: 169.596453\n",
      "Train Epoch: 493 [500000/697932 (72%)]\tLoss: 173.817656\n",
      "Train Epoch: 493 [600000/697932 (86%)]\tLoss: 175.007375\n",
      "====> Epoch: 493 Average loss: 172.2242\n",
      "====> Test set loss: 174.2301\n",
      "Train Epoch: 494 [0/697932 (0%)]\tLoss: 173.658734\n",
      "Train Epoch: 494 [100000/697932 (14%)]\tLoss: 173.876937\n",
      "Train Epoch: 494 [200000/697932 (29%)]\tLoss: 172.220000\n",
      "Train Epoch: 494 [300000/697932 (43%)]\tLoss: 175.856531\n",
      "Train Epoch: 494 [400000/697932 (57%)]\tLoss: 173.866328\n",
      "Train Epoch: 494 [500000/697932 (72%)]\tLoss: 173.432594\n",
      "Train Epoch: 494 [600000/697932 (86%)]\tLoss: 174.776016\n",
      "====> Epoch: 494 Average loss: 172.5443\n",
      "====> Test set loss: 174.2332\n",
      "Train Epoch: 495 [0/697932 (0%)]\tLoss: 170.652250\n",
      "Train Epoch: 495 [100000/697932 (14%)]\tLoss: 172.647797\n",
      "Train Epoch: 495 [200000/697932 (29%)]\tLoss: 171.867922\n",
      "Train Epoch: 495 [300000/697932 (43%)]\tLoss: 174.377406\n",
      "Train Epoch: 495 [400000/697932 (57%)]\tLoss: 173.889469\n",
      "Train Epoch: 495 [500000/697932 (72%)]\tLoss: 170.799953\n",
      "Train Epoch: 495 [600000/697932 (86%)]\tLoss: 173.678625\n",
      "====> Epoch: 495 Average loss: 172.0739\n",
      "====> Test set loss: 173.8702\n",
      "Train Epoch: 496 [0/697932 (0%)]\tLoss: 168.829703\n",
      "Train Epoch: 496 [100000/697932 (14%)]\tLoss: 173.656656\n",
      "Train Epoch: 496 [200000/697932 (29%)]\tLoss: 172.021016\n",
      "Train Epoch: 496 [300000/697932 (43%)]\tLoss: 173.373437\n",
      "Train Epoch: 496 [400000/697932 (57%)]\tLoss: 172.475422\n",
      "Train Epoch: 496 [500000/697932 (72%)]\tLoss: 172.534203\n",
      "Train Epoch: 496 [600000/697932 (86%)]\tLoss: 171.404766\n",
      "====> Epoch: 496 Average loss: 172.1296\n",
      "====> Test set loss: 173.8751\n",
      "Train Epoch: 497 [0/697932 (0%)]\tLoss: 170.414156\n",
      "Train Epoch: 497 [100000/697932 (14%)]\tLoss: 172.936297\n",
      "Train Epoch: 497 [200000/697932 (29%)]\tLoss: 174.103844\n",
      "Train Epoch: 497 [300000/697932 (43%)]\tLoss: 169.980297\n",
      "Train Epoch: 497 [400000/697932 (57%)]\tLoss: 171.854984\n",
      "Train Epoch: 497 [500000/697932 (72%)]\tLoss: 172.071313\n",
      "Train Epoch: 497 [600000/697932 (86%)]\tLoss: 171.537547\n",
      "====> Epoch: 497 Average loss: 172.1865\n",
      "====> Test set loss: 173.7906\n",
      "Train Epoch: 498 [0/697932 (0%)]\tLoss: 171.144719\n",
      "Train Epoch: 498 [100000/697932 (14%)]\tLoss: 171.154047\n",
      "Train Epoch: 498 [200000/697932 (29%)]\tLoss: 170.409094\n",
      "Train Epoch: 498 [300000/697932 (43%)]\tLoss: 174.913938\n",
      "Train Epoch: 498 [400000/697932 (57%)]\tLoss: 171.588437\n",
      "Train Epoch: 498 [500000/697932 (72%)]\tLoss: 173.853531\n",
      "Train Epoch: 498 [600000/697932 (86%)]\tLoss: 173.527656\n",
      "====> Epoch: 498 Average loss: 172.1343\n",
      "====> Test set loss: 173.9568\n",
      "Train Epoch: 499 [0/697932 (0%)]\tLoss: 170.603219\n",
      "Train Epoch: 499 [100000/697932 (14%)]\tLoss: 172.638422\n",
      "Train Epoch: 499 [200000/697932 (29%)]\tLoss: 172.722781\n",
      "Train Epoch: 499 [300000/697932 (43%)]\tLoss: 171.405469\n",
      "Train Epoch: 499 [400000/697932 (57%)]\tLoss: 173.583203\n",
      "Train Epoch: 499 [500000/697932 (72%)]\tLoss: 173.451016\n",
      "Train Epoch: 499 [600000/697932 (86%)]\tLoss: 174.565063\n",
      "====> Epoch: 499 Average loss: 172.0535\n",
      "====> Test set loss: 174.0547\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500\n",
    "for epoch in range(0, num_epochs):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    #Make many examples of latent space stuff\n",
    "    for i in range(10):\n",
    "        z = torch.randn( 64, Z_dim).cuda()\n",
    "        sample = vae.decoder(z).cuda()\n",
    "\n",
    "        save_image(sample.view(64, 1, 28, 28), './samples/sample_' +str(i) +'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae,'./trained_models/latest_model_500_epochs_ltsp_3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 40\n",
      "1 32\n",
      "2 4\n",
      "3 8\n",
      "4 2\n",
      "5 1\n",
      "6 6\n",
      "7 39\n",
      "8 9\n",
      "9 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/813719249.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "val = 79\n",
    "num_examples = 10\n",
    "for x in range(num_examples):\n",
    "    test_image_A, test_target_A = test_dataset[val + x]\n",
    "    #(test_image_A.view(1,1,28,28))\n",
    "    print(x, test_target_A)\n",
    "    #save_image(test_image_A.view(1, 1, 28, 28), './examples/example_input_'+str(val)+',' +str(x)+'.png')\n",
    "\n",
    "    #Shape input into what the network wants\n",
    "    z = torch.flatten(test_image_A)#.cuda()\n",
    "    z = torch.tensor(z)\n",
    "    z = z.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    latent_sample =  vae.encoder(z)[0]\n",
    "    sample = vae.forward(z)[0].cuda()\n",
    "    direct_sample = vae.decoder(latent_sample)#[0]\n",
    "\n",
    "    #save_image(sample.view(1, 1, 28, 28), './examples/example_decoded_'+str(val)+',' +str(x)+'.png')\n",
    "\n",
    "    comb_tensor = torch.cat((z, sample[0], direct_sample ) ).cuda()\n",
    "    save_image(comb_tensor.view(3, 1, 28, 28), './examples/example_endecoded_'+str(val)+',' +str(x)+'.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m val%\u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m      9\u001b[39m         \u001b[38;5;28mprint\u001b[39m(val)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mx\u001b[49m, test_target_A)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#save_image(test_image_A.view(1, 1, 28, 28), './examples/example_input_'+str(val)+',' +str(x)+'.png')\u001b[39;00m\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#Shape input into what the network wants\u001b[39;00m\n\u001b[32m     16\u001b[39m z = torch.flatten(test_image_A)\u001b[38;5;66;03m#.cuda()\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "#Capital A label is 10\n",
    "#lower case a label is 36\n",
    "val = 0\n",
    "test_target_A  =1\n",
    "while test_target_A != 36:\n",
    "    test_image_A, test_target_A = test_dataset[val]\n",
    "    val += 1\n",
    "    if val%100 == 0:\n",
    "        print(val)\n",
    "\n",
    "\n",
    "print(x, test_target_A)\n",
    "#save_image(test_image_A.view(1, 1, 28, 28), './examples/example_input_'+str(val)+',' +str(x)+'.png')\n",
    "\n",
    "#Shape input into what the network wants\n",
    "z = torch.flatten(test_image_A)#.cuda()\n",
    "z = torch.tensor(z)\n",
    "z = z.cuda()\n",
    "\n",
    "#Map through the model to the latent space and the whole way through\n",
    "latent_sample =  vae.encoder(z)[0]\n",
    "sample = vae.forward(z)[0].cuda()\n",
    "direct_sample = vae.decoder(latent_sample)#[0]\n",
    "\n",
    "#save_image(sample.view(1, 1, 28, 28), './examples/example_decoded_'+str(val)+',' +str(x)+'.png')\n",
    "\n",
    "comb_tensor = torch.cat((z, sample[0], direct_sample ) ).cuda()\n",
    "save_image(comb_tensor.view(3, 1, 28, 28), './examples/example_ltsp_3_endecoded_'+str(val)+',' +str(x)+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capital A label is 10\n",
    "#lower case a label is 36\n",
    "def all_test_images_indexes_for_label(target_label):\n",
    "    indexes = []\n",
    "    for index in range(len(test_dataset)): \n",
    "        image, label = test_dataset[index]\n",
    "        if label == target_label:\n",
    "            indexes.append(index)\n",
    "    return indexes\n",
    "\n",
    "def master_image_label_map():\n",
    "    master_label = []\n",
    "    for index in range(len(test_dataset)): \n",
    "        image, label = test_dataset[index]\n",
    "        master_label.append(label)\n",
    "    return master_label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 32, 4, 8, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "Label_Map = master_image_label_map()\n",
    "print(Label_Map[79:85])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[195, 303, 497, 738, 791, 798, 862, 880, 950, 1091, 1274, 1332, 1535, 1839, 1928, 1951, 2130, 2393, 2402, 3009, 3153, 3198, 3506, 3763, 4116, 4204, 4231, 4233, 4380, 4381, 4442, 4534, 4574, 4650, 4763, 5035, 5118, 5203, 5556, 5571, 5693, 5803, 5904, 5907, 6048, 6058, 6088, 6128, 6271, 6445, 6519, 6626, 6653, 6655, 6660, 6739, 6833, 6913, 7040, 7092, 7131, 7407, 7435, 7440, 7512, 7879, 7992, 8046, 8087, 8179, 8471, 8491, 8556, 8673, 8818, 8867, 8938, 9022, 9040, 9125, 9194, 9291, 9490, 9546, 9687, 9756, 9844, 9914, 9933, 9955, 10050, 10108, 10149, 10323, 10351, 10428, 10572, 10578, 10671, 10700, 10735, 11015, 11345, 11346, 11347, 11610, 11742, 11874, 11878, 12019, 12029, 12036, 12179, 12294, 12295, 12304, 12621, 12777, 13163, 13194, 13199, 13350, 13635, 13670, 13820, 13846, 13864, 13915, 14055, 14101, 14162, 14350, 14387, 14443, 14696, 14917, 15077, 15136, 15175, 15248, 15287, 15364, 15370, 15495, 15513, 15612, 15701, 15734, 15739, 15750, 15862, 16001, 16198, 16266, 16376, 16406, 16433, 16468, 16621, 16628, 16642, 16951, 17053, 17215, 17237, 17322, 17394, 17477, 17543, 17557, 17603, 17649, 17692, 17909, 17982, 18281, 18299, 18308, 18367, 18502, 18523, 18721, 18799, 18809, 18853, 18864, 18881, 19025, 19203, 19343, 19565, 19641, 19939, 19946, 20033, 20124, 20170, 20199, 20438, 20448, 20479, 20583, 20586, 20755, 21178, 21548, 22236, 22238, 22261, 22304, 22380, 22470, 22669, 22671, 22858, 22918, 23032, 23169, 23526, 23605, 23974, 24350, 24385, 24404, 24452, 24483, 24580, 24943, 24955, 24994, 25113, 25226, 25280, 25335, 25455, 25471, 25497, 25498, 25526, 25636, 25653, 25662, 25664, 25742, 25973, 26000, 26057, 26143, 26156, 26238, 26288, 26320, 26663, 26871, 26910, 27128, 27191, 27328, 27394, 27500, 27526, 27563, 27629, 27676, 27702, 27754, 27758, 28066, 28080, 28561, 28629, 28718, 28775, 28896, 29016, 29144, 29277, 29352, 29392, 29429, 29498, 29600, 29608, 29933, 30161, 30277, 30637, 30726, 30806, 31007, 31231, 31338, 31371, 31442, 31454, 31473, 31866, 31990, 32047, 32071, 32073, 32166, 32210, 32445, 32587, 32599, 32826, 32863, 32935, 32964, 33366, 33636, 33717, 33806, 33890, 34015, 34093, 34276, 34296, 34314, 34383, 34583, 34679, 34789, 34870, 35188, 35305, 35325, 35428, 35595, 35735, 35803, 35875, 35948, 35961, 36001, 36131, 36198, 36411, 36495, 36709, 36725, 37266, 37343, 37520, 37624, 37659, 37663, 37667, 37864, 38276, 38392, 38526, 38712, 38730, 38880, 38962, 39128, 39338, 39396, 39435, 39782, 39855, 39947, 40175, 40221, 40313, 40490, 40667, 40675, 40755, 40897, 41096, 41148, 41421, 41658, 41782, 41879, 41958, 41959, 42010, 42020, 42118, 42122, 42648, 42750, 42821, 42861, 43287, 43374, 43460, 43520, 43557, 43703, 43770, 43975, 44065, 44082, 44084, 44255, 44284, 44399, 44484, 44593, 45138, 45394, 45474, 45701, 45722, 45902, 45931, 46142, 46206, 46248, 46327, 46337, 46438, 46530, 46632, 46649, 46845, 46912, 46982, 46991, 47136, 47195, 47210, 47226, 47271, 47294, 47323, 47553, 48058, 48150, 48153, 48479, 48504, 48543, 48626, 48676, 48824, 48958, 48961, 49082, 49184, 49336, 49391, 49617, 49632, 49647, 49718, 49724, 49728, 49971, 50085, 50164, 50334, 50362, 50401, 50504, 50606, 50731, 50991, 51007, 51200, 51278, 51292, 51411, 51433, 51508, 51763, 51840, 51900, 51948, 51953, 52182, 52317, 52686, 52760, 52786, 52790, 52960, 53028, 53119, 53387, 53415, 53445, 53487, 53525, 53584, 53595, 53597, 53793, 53797, 53851, 53901, 53974, 53979, 54084, 54152, 54373, 54374, 54910, 54929, 54933, 55030, 55046, 55088, 55226, 55236, 55496, 55550, 55798, 55804, 55829, 55865, 55926, 55989, 56036, 56125, 56284, 56340, 56369, 56412, 56441, 56901, 57094, 57197, 57297, 57586, 57609, 57620, 57651, 57699, 57757, 57759, 57828, 57882, 57930, 58025, 58063, 58091, 58354, 58576, 58844, 58925, 59124, 59189, 59292, 59361, 59534, 59589, 59664, 59712, 59750, 60017, 60478, 60504, 60529, 60541, 60610, 60749, 61284, 61407, 61494, 61704, 61771, 61827, 62022, 62089, 62127, 62300, 62388, 62415, 62515, 62596, 62668, 62682, 62833, 62939, 62992, 63049, 63212, 63224, 63382, 63711, 63789, 63815, 63897, 63928, 64037, 64051, 64091, 64164, 64179, 64216, 64342, 64530, 64833, 65175, 65205, 65282, 65632, 65732, 65787, 65855, 66058, 66223, 66234, 66289, 66346, 66479, 66512, 66526, 66535, 66698, 66947, 67024, 67135, 67186, 67360, 67578, 67827, 67906, 68007, 68012, 68130, 68202, 68215, 68422, 68755, 68766, 68818, 68872, 68959, 69034, 69061, 69106, 69166, 69305, 69319, 69320, 69531, 69580, 69649, 69719, 69953, 70066, 70226, 70383, 70417, 70464, 70619, 70668, 70697, 70858, 70960, 70986, 71258, 71279, 71457, 71468, 71648, 71785, 71860, 71905, 71928, 71955, 72213, 72225, 72463, 72548, 72601, 72668, 72824, 72981, 73027, 73147, 73212, 73573, 73821, 73838, 73996, 74174, 74187, 74575, 74659, 74763, 74830, 74847, 75181, 75282, 75385, 75411, 75510, 75580, 75709, 75814, 75842, 75947, 76143, 76315, 76645, 76710, 76750, 76845, 76878, 77099, 77153, 77180, 77653, 77901, 78198, 78328, 78449, 78483, 78830, 78933, 79045, 79059, 79067, 79238, 79306, 79384, 79415, 79649, 79958, 79997, 80050, 80138, 80159, 80163, 80306, 80364, 80449, 80612, 80769, 80831, 80832, 80956, 81114, 81271, 81285, 81314, 81333, 81428, 81519, 81566, 81642, 81915, 82134, 82162, 82439, 82530, 82542, 82670, 82812, 82855, 82970, 83423, 83495, 83772, 83868, 83869, 84034, 84679, 84684, 84922, 84984, 85021, 85416, 85443, 85640, 85918, 86108, 86131, 86356, 86438, 86617, 86698, 86704, 87004, 87118, 87298, 87395, 87434, 87454, 87577, 87581, 87812, 87948, 87977, 88008, 88029, 88239, 88305, 88357, 88362, 88389, 88545, 88744, 88817, 89200, 89482, 89542, 89605, 89606, 89628, 89661, 90057, 90190, 90355, 90443, 90448, 90485, 90634, 90764, 90921, 90974, 91001, 91176, 91278, 91508, 91866, 91890, 91898, 92209, 92248, 92266, 92399, 92458, 92532, 92767, 92919, 93014, 93072, 93247, 93287, 93543, 93831, 93850, 93916, 93966, 94184, 94220, 94240, 94322, 94497, 94533, 94650, 94667, 94672, 94735, 95222, 95362, 95686, 95766, 95777, 95778, 95798, 95808, 95960, 96001, 96026, 96156, 96639, 96741, 96941, 97023, 97143, 97250, 97327, 97330, 97423, 97862, 97901, 97919, 98255, 98288, 98341, 98384, 98395, 98485, 98725, 98748, 98953, 99089, 99142, 99296, 99449, 99482, 99587, 99621, 99641, 99697, 99703, 99792, 99845, 99943, 100070, 100074, 100129, 100204, 100238, 100285, 100350, 100380, 100386, 100415, 100554, 100831, 100888, 100988, 101047, 101195, 101252, 101379, 101384, 101799, 101961, 102025, 102040, 102072, 102439, 102825, 102854, 103021, 103028, 103044, 103341, 103354, 103416, 103622, 103778, 103796, 103981, 104074, 104103, 104128, 104185, 104277, 104370, 104626, 105062, 105095, 105173, 105212, 105214, 105224, 105361, 105458, 105657, 105658, 105738, 105795, 105809, 105906, 106158, 106241, 106324, 106399, 106464, 106524, 106615, 106802, 107037, 107182, 107186, 107264, 107342, 107351, 107520, 107828, 107835, 107870, 107926, 107991, 108103, 108123, 108357, 108376, 108404, 108425, 108513, 108541, 108689, 108811, 108927, 108931, 108986, 109133, 109240, 109446, 109448, 109612, 109617, 109868, 109907, 110127, 110304, 110359, 110429, 110472, 110875, 110995, 111012, 111074, 111115, 111131, 111142, 111398, 111577, 111641, 111697, 111830, 111872, 111951, 112176, 112187, 112307, 112430, 112491, 112504, 112539, 112554, 112684, 112712, 112780, 112810, 112884, 112919, 112937, 113004, 113088, 113442, 113462, 113504, 113536, 113602, 113685, 113902, 114019, 114030, 114306, 114480, 114575, 114679, 114727, 114967, 114988, 115150, 115338, 115346, 115356, 115550, 115666, 115728, 115744, 115756, 115772, 116078, 116181]\n"
     ]
    }
   ],
   "source": [
    "print( all_test_images_indexes_for_label(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "labels_to_indexes = {}\n",
    "for label in range(62):\n",
    "    labels_to_indexes[label] = [index for index in range(len(Label_Map)) if Label_Map[index]==label]\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/2319603254.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "A_indexes = labels_to_indexes[10]\n",
    "\n",
    "for val in A_indexes[:10]:\n",
    "    test_image_A, test_target_A = test_dataset[val]\n",
    "\n",
    "    #Shape input into what the network wants\n",
    "    z = torch.flatten(test_image_A)#.cuda()\n",
    "    z = torch.tensor(z)\n",
    "    z = z.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    latent_sample =  vae.encoder(z)[0]\n",
    "    sample = vae.forward(z)[0].cuda()\n",
    "    direct_sample = vae.decoder(latent_sample)#[0]\n",
    "\n",
    "    #save_image(sample.view(1, 1, 28, 28), './examples/example_decoded_'+str(val)+',' +str(x)+'.png')\n",
    "\n",
    "    comb_tensor = torch.cat((z, sample[0], direct_sample ) ).cuda()\n",
    "    save_image(comb_tensor.view(3, 1, 28, 28), './examples/A/example_endecoded_'+str(val)+',' +str(x)+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_725508/3869809592.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "A_indexes = labels_to_indexes[36]\n",
    "\n",
    "for val in A_indexes[:10]:\n",
    "    test_image_A, test_target_A = test_dataset[val]\n",
    "\n",
    "    #Shape input into what the network wants\n",
    "    z = torch.flatten(test_image_A)#.cuda()\n",
    "    z = torch.tensor(z)\n",
    "    z = z.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    latent_sample =  vae.encoder(z)[0]\n",
    "    sample = vae.forward(z)[0].cuda()\n",
    "    direct_sample = vae.decoder(latent_sample)#[0]\n",
    "\n",
    "    comb_tensor = torch.cat((z, sample[0], direct_sample ) ).cuda()\n",
    "    save_image(comb_tensor.view(3, 1, 28, 28), './examples/a/example_ltsp_3_endecoded_'+str(val)+'.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_latent_space_vector(Indexes):\n",
    "    total_sample = torch.zeros(Z_dim).cuda()\n",
    "    for index in Indexes:\n",
    "        test_image_A, test_target_A = test_dataset[index]\n",
    "\n",
    "        #Shape input into what the network wants\n",
    "        z = torch.flatten(test_image_A)\n",
    "        z = torch.tensor(z)\n",
    "        z = z.cuda()\n",
    "\n",
    "        #Map through the model to the latent space \n",
    "        latent_sample =  vae.encoder(z)[0]\n",
    "        total_sample = torch.add(latent_sample, total_sample)\n",
    "    \n",
    "    return total_sample/len(Indexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_725508/2705388341.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0945,  0.6960, -0.3698], device='cuda:0', grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "A_indexes = labels_to_indexes[10]\n",
    "a_indexes = labels_to_indexes[36]\n",
    "\n",
    "A_mean = mean_latent_space_vector(A_indexes)\n",
    "a_mean = mean_latent_space_vector(a_indexes)\n",
    "\n",
    "lower_to_upper = A_mean-a_mean\n",
    "lower_to_upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_725508/189837734.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "#using lower to upper to change from a to A\n",
    "a_indexes = labels_to_indexes[36]\n",
    "\n",
    "for val in a_indexes[:30]:\n",
    "    test_image_A, test_target_A = test_dataset[val]\n",
    "\n",
    "    #Shape input into what the network wants\n",
    "    z = torch.flatten(test_image_A)#.cuda()\n",
    "    z = torch.tensor(z)\n",
    "    z = z.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    latent_sample =  vae.encoder(z)[0]\n",
    "\n",
    "    sample = vae.forward(z)[0].cuda()\n",
    "\n",
    "    adjusted_latent = torch.add(latent_sample , lower_to_upper)\n",
    "    direct_sample = vae.decoder(adjusted_latent)\n",
    "\n",
    "    comb_tensor = torch.cat((z, sample[0], direct_sample ) ).cuda()\n",
    "    save_image(comb_tensor.view(3, 1, 28, 28), './examples/lower_to_upper/example_ltsp_3_a'+str(val)+'.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_725508/787206124.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "#MIDDLE CASE A to a\n",
    "#using lower to upper to go backwards from A to middle case a\n",
    "A_indexes = labels_to_indexes[10]\n",
    "\n",
    "for val in A_indexes[:30]:\n",
    "    test_image_A, test_target_A = test_dataset[val]\n",
    "\n",
    "    #Shape input into what the network wants\n",
    "    z = torch.flatten(test_image_A)#.cuda(k\n",
    "    z = torch.tensor(z)\n",
    "    z = z.cuda()\n",
    "\n",
    "    #Map through the model to the latent space and the whole way through\n",
    "    latent_sample =  vae.encoder(z)[0]\n",
    "\n",
    "    sample = vae.forward(z)[0].cuda()\n",
    "\n",
    "    adjusted_latent = torch.add(latent_sample , ((-1/2)*lower_to_upper) )\n",
    "    direct_sample = vae.decoder(adjusted_latent)\n",
    "\n",
    "    comb_tensor = torch.cat((z, sample[0], direct_sample ) ).cuda()\n",
    "    save_image(comb_tensor.view(3, 1, 28, 28), './examples/middle_case/example_ltsp_3_A'+str(val)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_725508/2705388341.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "#Generate dict of the mean differences between lowercase and uppercase letters one at a time\n",
    "#uses indexes of the capital letters\n",
    "lower_to_upper_dict = {}\n",
    "Upper_means = {}\n",
    "Lower_means = {}\n",
    "\n",
    "for offset in range(26):\n",
    "    A_indexes = labels_to_indexes[10 + offset]\n",
    "    a_indexes = labels_to_indexes[36 + offset]\n",
    "\n",
    "    A_mean = mean_latent_space_vector(A_indexes)\n",
    "    a_mean = mean_latent_space_vector(a_indexes)\n",
    "\n",
    "    lower_to_upper = A_mean-a_mean\n",
    "    lower_to_upper_dict[10+offset] = lower_to_upper\n",
    "    Upper_means[10+offset] = A_mean\n",
    "    Lower_means[36+offset] = a_mean\n",
    "\n",
    "    print(offset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_uppercase_dict ={\n",
    "10:'A',\n",
    "11:'B',\n",
    "12:'C',\n",
    "13:'D',\n",
    "14:'E',\n",
    "15:'F',\n",
    "16:'G',\n",
    "17:'H',\n",
    "18:'I',\n",
    "19:'J',\n",
    "20:'K',\n",
    "21:'L',\n",
    "22:'M',\n",
    "23:'N',\n",
    "24:'O',\n",
    "25:'P',\n",
    "26:'Q',\n",
    "27:'R',\n",
    "28:'S',\n",
    "29:'T',\n",
    "30:'U',\n",
    "31:'V',\n",
    "32:'W',\n",
    "33:'X',\n",
    "34:'Y',\n",
    "35:'Z',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_tensor_for_image(comb_view):\n",
    "    comb_view = torch.flip(comb_view,dims=[2])\n",
    "    comb_view = torch.rot90(comb_view,3,[2,3])\n",
    "    return comb_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function to make only the Mega image of all fractional case letters and first and mean\n",
    "def generate_fractional_composite(fractional_value, file_name):\n",
    "    Combined_View = torch.empty(0, 1, 28, 28).cuda()\n",
    "    for offset in range(26):\n",
    "        letter_index = 10 +offset\n",
    "        lower_index = 36 + offset\n",
    "\n",
    "        #Map through the model to the latent space and the whole way through\n",
    "        latent_sample = Lower_means[lower_index] \n",
    "        adjusted_latent = torch.add(latent_sample , ((fractional_value)*lower_to_upper_dict[letter_index]) )\n",
    "        direct_sample = vae.decoder(adjusted_latent).cuda()\n",
    "\n",
    "        comb_view = direct_sample.view(1, 1, 28, 28)\n",
    "        comb_view = fix_tensor_for_image(comb_view)\n",
    "        Combined_View = torch.cat((Combined_View, comb_view))\n",
    "\n",
    "    save_image(Combined_View, './examples/average_letter/'+file_name+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mega_image_fractional_composite(fractional_value, file_name):\n",
    "    Combined_View = torch.empty(0, 1, 28, 28).cuda()\n",
    "    for offset in range(26):\n",
    "        letter_index = 10 +offset\n",
    "        lower_index = 36 + offset\n",
    "        A_indexes = labels_to_indexes[letter_index]\n",
    "        test_image_A, test_target_A = test_dataset[A_indexes[0]]\n",
    "\n",
    "        #Shape input into what the network wants\n",
    "        z = torch.flatten(test_image_A)#.cuda(k\n",
    "        z = torch.tensor(z)\n",
    "        z = z.cuda()\n",
    "\n",
    "        #Map through the model to the latent space and the whole way through\n",
    "        latent_sample = Lower_means[lower_index] \n",
    "\n",
    "        sample = vae.decoder(latent_sample)\n",
    "        adjusted_latent = torch.add(latent_sample , ((fractional_value)*lower_to_upper_dict[letter_index]) )\n",
    "        direct_sample = vae.decoder(adjusted_latent).cuda()\n",
    "\n",
    "        comb_tensor = torch.cat( (z, sample, direct_sample) ).cuda()\n",
    "        comb_view = comb_tensor.view(3, 1, 28, 28)\n",
    "        comb_view = fix_tensor_for_image(comb_view)\n",
    "        Combined_View = torch.cat((Combined_View, comb_view))\n",
    "\n",
    "    save_image(Combined_View, './examples/average_letter/'+file_name+'_mega_image.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_fractional_composite(0,'ltsp_3_lower_case')\n",
    "generate_fractional_composite(1/2,'ltsp_3_middle')\n",
    "#generate_fractional_composite(1/4,'one_quarter')\n",
    "#generate_fractional_composite(3/4,'three_quarter')\n",
    "generate_fractional_composite(1,'ltsp_3_back_to_upper_from_lower')\n",
    "#generate_fractional_composite(-1,'double_lower')\n",
    "#generate_fractional_composite(2,'double_upper_from_lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A 1.3487519025802612\n",
      "B 1.4186996221542358\n",
      "D 2.5907042026519775\n",
      "E 1.0275297164916992\n",
      "H 1.4318251609802246\n",
      "J 1.326478362083435\n",
      "L 2.8812243938446045\n",
      "Q 1.9483580589294434\n",
      "R 1.5346614122390747\n",
      "T 1.3105871677398682\n",
      "[1.3487519025802612, 1.4186996221542358, 0.14337532222270966, 2.5907042026519775, 1.0275297164916992, 0.3582804501056671, 0.7960465550422668, 1.4318251609802246, 0.2490057796239853, 1.326478362083435, 0.2991258203983307, 2.8812243938446045, 0.2291758507490158, 0.6736744046211243, 0.09768480062484741, 0.26827070116996765, 1.9483580589294434, 1.5346614122390747, 0.18236872553825378, 1.3105871677398682, 0.37059134244918823, 0.05151326209306717, 0.41919490694999695, 0.20881016552448273, 0.2618374228477478, 0.38266435265541077]\n"
     ]
    }
   ],
   "source": [
    "#Which letters are actually different from upper to lower\n",
    "Norms = []\n",
    "Upper_Lower_distinct_indexes = []\n",
    "for offset in range(26):\n",
    "    letter_index = 10 +offset\n",
    "    captalization_vect = lower_to_upper_dict[letter_index]\n",
    "    vect_norm = torch.norm(captalization_vect).item()\n",
    "    letter = index_to_uppercase_dict[letter_index]\n",
    "    Norms.append(vect_norm)\n",
    "    if vect_norm>1:\n",
    "        print(letter, vect_norm)\n",
    "        Upper_Lower_distinct_indexes.append(10+offset)\n",
    "print(Norms)\n",
    "\n",
    "#J and L are distinct wiht this process but F is not, in the paper they remove J and L and keep F as its own class when compressin in the By_Merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(Norms, bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([norm for norm in Norms if norm >=1 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m mean_distinct_lower_upper_vector =  torch.zeros(\u001b[32m4\u001b[39m).cuda()\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m Upper_Lower_distinct_indexes:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     mean_distinct_lower_upper_vector = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_distinct_lower_upper_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlower_to_upper_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(mean_distinct_lower_upper_vector)\n\u001b[32m      7\u001b[39m mean_distinct_upper_lower_vector = mean_distinct_lower_upper_vector/\u001b[38;5;28mlen\u001b[39m(Upper_Lower_distinct_indexes)\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "#Make average capitalization vector\n",
    "mean_distinct_lower_upper_vector =  torch.zeros(Z_dim).cuda()\n",
    "for index in Upper_Lower_distinct_indexes:\n",
    "    mean_distinct_lower_upper_vector = torch.add( mean_distinct_lower_upper_vector, lower_to_upper_dict[index])\n",
    "\n",
    "print(mean_distinct_lower_upper_vector)\n",
    "mean_distinct_upper_lower_vector = mean_distinct_lower_upper_vector/len(Upper_Lower_distinct_indexes)\n",
    "print(mean_distinct_upper_lower_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n"
     ]
    }
   ],
   "source": [
    "print(1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/2705388341.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "{0: tensor([-0.0486,  1.7586, -0.3592,  0.3226], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 1: tensor([-1.3171, -1.4150, -0.3829, -0.1228], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 2: tensor([ 0.9367,  0.3439,  0.6909, -0.5600], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 3: tensor([-0.5568,  0.3207,  1.0250,  1.5168], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 4: tensor([ 0.0646, -0.7411,  0.1578, -0.2515], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 5: tensor([ 0.8945,  0.4823, -0.7192,  1.3620], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 6: tensor([-1.5097,  0.7385,  0.9170, -0.5531], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 7: tensor([ 1.1069, -1.3633,  1.1155,  0.3314], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 8: tensor([ 0.3109, -0.2116,  0.1776,  1.0464], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>), 9: tensor([ 0.1287, -0.9480,  0.8000,  0.4248], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)}\n"
     ]
    }
   ],
   "source": [
    "#Generate dict of the mean number\n",
    "Number_means = {}\n",
    "\n",
    "for offset in range(10):\n",
    "    num_indexes = labels_to_indexes[offset]\n",
    "    num_mean = mean_latent_space_vector(num_indexes)\n",
    "    Number_means[offset] = num_mean\n",
    "\n",
    "    print(offset)\n",
    "print(Number_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_mega_image_fractional_composite_mean_vector(fractional_value, file_name, include_numbers):\n",
    "    Combined_View = torch.empty(0, 1, 28, 28).cuda()\n",
    "    num_characters = 26\n",
    "    if include_numbers:\n",
    "        num_characters = 36\n",
    "    for offset in range(num_characters):\n",
    "\n",
    "        letter_index = 10 +offset\n",
    "        lower_index  = 36 +offset \n",
    "        if include_numbers:\n",
    "            letter_index = offset\n",
    "            lower_index  = 26 + offset \n",
    "\n",
    "\n",
    "        A_indexes = labels_to_indexes[letter_index]\n",
    "        test_image_A, test_target_A = test_dataset[A_indexes[0]]\n",
    "\n",
    "        #Shape input into what the network wants\n",
    "        z = torch.flatten(test_image_A)#.cuda(k\n",
    "        z = torch.tensor(z)\n",
    "        z = z.cuda()\n",
    "\n",
    "        #Map through the model to the latent space and the whole way through\n",
    "        if letter_index< 10:\n",
    "            latent_sample = Number_means[letter_index]\n",
    "        else:\n",
    "            latent_sample = Lower_means[lower_index] \n",
    "\n",
    "        sample = vae.decoder(latent_sample)\n",
    "        adjusted_latent = torch.add( latent_sample , ( fractional_value * mean_distinct_lower_upper_vector) )\n",
    "        direct_sample = vae.decoder(adjusted_latent).cuda()\n",
    "\n",
    "        comb_tensor = torch.cat( (z, sample, direct_sample) ).cuda()\n",
    "        comb_view = comb_tensor.view(3, 1, 28, 28)\n",
    "\n",
    "        comb_view = fix_tensor_for_image(comb_view)\n",
    "        Combined_View = torch.cat((Combined_View, comb_view))\n",
    "\n",
    "    save_image(Combined_View, './examples/average_figure/'+file_name+'_mega_image_mean_vector.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66026/2532875063.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  z = torch.tensor(z)\n"
     ]
    }
   ],
   "source": [
    "generate_mega_image_fractional_composite_mean_vector(0,'lower_case',True)\n",
    "generate_mega_image_fractional_composite_mean_vector( 0.5 ,'middle',True)\n",
    "generate_mega_image_fractional_composite_mean_vector(1,'back_to_upper_from_lower',True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.5287e+33, 5.5419e+00, 9.0254e-01, 2.2504e+00]], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n",
      "tensor([[2.1072e+32, 4.6183e-01, 7.5212e-02, 1.8753e-01]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(mean_distinct_lower_upper_vector)\n",
    "print(mean_distinct_upper_lower_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
